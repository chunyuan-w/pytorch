tensor([[ 0.0461,  0.4024, -1.0115,  0.2167, -0.6123,  0.5036,  0.2310,  0.6931]])
[DUMP kernel.cpp:1594] TensorExprKernel graph (Before graph optimization):
[DUMP kernel.cpp:1594] graph(%x : Float(1, 8, strides=[8, 1], requires_grad=0, device=cpu)):
[DUMP kernel.cpp:1594]   %1 : NoneType = prim::Constant()
[DUMP kernel.cpp:1594]   %2 : bool = prim::Constant[value=0]()
[DUMP kernel.cpp:1594]   %3 : int[] = prim::Constant[value=[-1]]()
[DUMP kernel.cpp:1594]   %4 : Float(1, strides=[1], requires_grad=0, device=cpu) = aten::sum(%x, %3, %2, %1) # test/test_te_softmax.py:64:0
[DUMP kernel.cpp:1594]   return (%4)
[DUMP kernel.cpp:1622] TensorExprKernel graph (After graph optimization):
[DUMP kernel.cpp:1622] graph(%x : Float(1, 8, strides=[8, 1], requires_grad=0, device=cpu)):
[DUMP kernel.cpp:1622]   %1 : NoneType = prim::Constant()
[DUMP kernel.cpp:1622]   %2 : bool = prim::Constant[value=0]()
[DUMP kernel.cpp:1622]   %3 : int[] = prim::Constant[value=[-1]]()
[DUMP kernel.cpp:1622]   %4 : Float(1, strides=[1], requires_grad=0, device=cpu) = aten::sum(%x, %3, %2, %1) # test/test_te_softmax.py:64:0
[DUMP kernel.cpp:1622]   return (%4)
[DUMP kernel.cpp:1626] TensorExprKernel graph:
[DUMP kernel.cpp:1626] graph(%x : Float(1, 8, strides=[8, 1], requires_grad=0, device=cpu)):
[DUMP kernel.cpp:1626]   %1 : NoneType = prim::Constant()
[DUMP kernel.cpp:1626]   %2 : bool = prim::Constant[value=0]()
[DUMP kernel.cpp:1626]   %3 : int[] = prim::Constant[value=[-1]]()
[DUMP kernel.cpp:1626]   %4 : Float(1, strides=[1], requires_grad=0, device=cpu) = aten::sum(%x, %3, %2, %1) # test/test_te_softmax.py:64:0
[DUMP kernel.cpp:1626]   return (%4)
[DEBUG kernel.cpp:710] Original Stmt:
[DEBUG kernel.cpp:710] {
[DEBUG kernel.cpp:710]   for (int64_t i = 0ll; i < 1ll; i++) {
[DEBUG kernel.cpp:710]     sum[i] = float(0);
[DEBUG kernel.cpp:710]     for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:710]       sum[i] = ReduceOp((sum[i]) + float(tx[0ll, j]), reduce_args={j});
[DEBUG kernel.cpp:710]     }
[DEBUG kernel.cpp:710]   }
[DEBUG kernel.cpp:710] }
[DEBUG kernel.cpp:732] after simplify{
[DEBUG kernel.cpp:732]   sum[0ll] = 0.f;
[DEBUG kernel.cpp:732]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:732]     sum[0ll] = ReduceOp((sum[0ll]) + (tx[0ll, j]), reduce_args={j});
[DEBUG kernel.cpp:732]   }
[DEBUG kernel.cpp:732] }
[DEBUG kernel.cpp:742] after inline{
[DEBUG kernel.cpp:742]   sum[0ll] = 0.f;
[DEBUG kernel.cpp:742]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:742]     sum[0ll] = ReduceOp((sum[0ll]) + (tx[0ll, j]), reduce_args={j});
[DEBUG kernel.cpp:742]   }
[DEBUG kernel.cpp:742] }
[DEBUG kernel.cpp:758] after fuse{
[DEBUG kernel.cpp:758]   sum[0ll] = 0.f;
[DEBUG kernel.cpp:758]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:758]     sum[0ll] = ReduceOp((sum[0ll]) + (tx[0ll, j]), reduce_args={j});
[DEBUG kernel.cpp:758]   }
[DEBUG kernel.cpp:758] }
[DEBUG kernel.cpp:760] after parallelize{
[DEBUG kernel.cpp:760]   sum[0ll] = 0.f;
[DEBUG kernel.cpp:760]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:760]     sum[0ll] = ReduceOp((sum[0ll]) + (tx[0ll, j]), reduce_args={j});
[DEBUG kernel.cpp:760]   }
[DEBUG kernel.cpp:760] }
[DEBUG kernel.cpp:845] after prepareForCodegen{
[DEBUG kernel.cpp:845]   sum[0ll] = 0.f;
[DEBUG kernel.cpp:845]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:845]     sum[0ll] = (sum[0ll]) + (tx[(0ll + 0ll * 8ll) + j * 1ll]);
[DEBUG kernel.cpp:845]   }
[DEBUG kernel.cpp:845] }
[DEBUG kernel.cpp:847] after simplification{
[DEBUG kernel.cpp:847]   sum[0ll] = 0.f;
[DEBUG kernel.cpp:847]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:847]     sum[0ll] = (sum[0ll]) + (tx[j]);
[DEBUG kernel.cpp:847]   }
[DEBUG kernel.cpp:847] }
st: 
sum[0ll] = 0.f;
f: 
for (int64_t j = 0ll; j < 8ll; j++) {
  sum[0ll] = (sum[0ll]) + (tx[j]);
}
worklist size: 1
stmt: sum[0ll] = (sum[0ll]) + (tx[j]);
add_right: tx[j]
loop: 
for (int64_t j = 0ll; j < 8ll; j++) {
  sum[0ll] = (sum[0ll]) + (tx[j]);
}
tmp_add: 
for (int64_t j = 0ll; j < 8ll; j++) {
  buf_tmp[j] = (buf_tmp[j]) + (tx[j]);
}
[DEBUG kernel.cpp:855] after vectorizeReduction{
[DEBUG kernel.cpp:855]   for (int64_t i = 0ll; i < 8ll; i++) {
[DEBUG kernel.cpp:855]     buf_tmp[i] = 0.f;
[DEBUG kernel.cpp:855]   }
[DEBUG kernel.cpp:855]   sum[0ll] = 0.f;
[DEBUG kernel.cpp:855]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:855]     buf_tmp[j] = (buf_tmp[j]) + (tx[j]);
[DEBUG kernel.cpp:855]   }
[DEBUG kernel.cpp:855]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:855]     sum[0ll] = ReduceOp((sum[0ll]) + (buf_tmp[j]), reduce_args={j});
[DEBUG kernel.cpp:855]   }
[DEBUG kernel.cpp:855] }
orig f: 
for (int64_t j_inner = 0ll; j_inner < 8ll; j_inner++) {
  buf_tmp[j_outer * 8ll + j_inner] = (buf_tmp[j_outer * 8ll + j_inner]) + (tx[j_outer * 8ll + j_inner]);
}
before normalize: 
for (int64_t j_inner = 0ll; j_inner < 8ll; j_inner++) {
  buf_tmp[j_outer * 8ll + j_inner] = (buf_tmp[j_outer * 8ll + j_inner]) + (tx[j_outer * 8ll + j_inner]);
}
before flatten: 
for (int64_t j_inner = 0ll; j_inner < 8ll; j_inner++) {
  buf_tmp[j_outer * 8ll + j_inner] = (buf_tmp[j_outer * 8ll + j_inner]) + (tx[j_outer * 8ll + j_inner]);
}
before vec: 
for (int64_t j_inner = 0ll; j_inner < 8ll; j_inner++) {
  buf_tmp[j_outer * 8ll + j_inner] = (buf_tmp[j_outer * 8ll + j_inner]) + (tx[j_outer * 8ll + j_inner]);
}
loop start_: 0
loop lanes_: 8
stmt in Block before mutate: 
buf_tmp[j_outer * 8ll + j_inner] = (buf_tmp[j_outer * 8ll + j_inner]) + (tx[j_outer * 8ll + j_inner]);
old_in: 
j_outer * 8ll + j_inner
old_in: 
j_outer * 8ll
old_in: 
j_outer
new_in: 
j_outer
old_in: 
8ll
new_in: 
8ll
new_in: 
j_outer * 8ll
old_in: 
j_inner
stride value: 1
stride value cstr: 1
new_in: 
Ramp(0ll, 1ll, 8)
insert broadcasts for 0
broadcast make
lanes: 8
insert broadcasts for 1
no broadcast make
new_in: 
(Broadcast(j_outer * 8ll, 8)) + (Ramp(0ll, 1ll, 8))
old_in: 
(buf_tmp[j_outer * 8ll + j_inner]) + (tx[j_outer * 8ll + j_inner])
old_in: 
buf_tmp[j_outer * 8ll + j_inner]
old_in: 
j_outer * 8ll + j_inner
old_in: 
j_outer * 8ll
old_in: 
j_outer
new_in: 
j_outer
old_in: 
8ll
new_in: 
8ll
new_in: 
j_outer * 8ll
old_in: 
j_inner
stride value: 1
stride value cstr: 1
new_in: 
Ramp(0ll, 1ll, 8)
insert broadcasts for 0
broadcast make
lanes: 8
insert broadcasts for 1
no broadcast make
new_in: 
(Broadcast(j_outer * 8ll, 8)) + (Ramp(0ll, 1ll, 8))
insert broadcasts for 0
no broadcast make
new_in: 
buf_tmp[(Broadcast(j_outer * 8ll, 8)) + (Ramp(0ll, 1ll, 8))]
old_in: 
tx[j_outer * 8ll + j_inner]
old_in: 
j_outer * 8ll + j_inner
old_in: 
j_outer * 8ll
old_in: 
j_outer
new_in: 
j_outer
old_in: 
8ll
new_in: 
8ll
new_in: 
j_outer * 8ll
old_in: 
j_inner
stride value: 1
stride value cstr: 1
new_in: 
Ramp(0ll, 1ll, 8)
insert broadcasts for 0
broadcast make
lanes: 8
insert broadcasts for 1
no broadcast make
new_in: 
(Broadcast(j_outer * 8ll, 8)) + (Ramp(0ll, 1ll, 8))
insert broadcasts for 0
no broadcast make
new_in: 
tx[(Broadcast(j_outer * 8ll, 8)) + (Ramp(0ll, 1ll, 8))]
insert broadcasts for 0
no broadcast make
insert broadcasts for 1
no broadcast make
new_in: 
(buf_tmp[(Broadcast(j_outer * 8ll, 8)) + (Ramp(0ll, 1ll, 8))]) + (tx[(Broadcast(j_outer * 8ll, 8)) + (Ramp(0ll, 1ll, 8))])
insert broadcasts for 0
no broadcast make
insert broadcasts for 1
no broadcast make
stmt in Block after mutate: 
buf_tmp[(Broadcast(j_outer * 8ll, 8)) + (Ramp(0ll, 1ll, 8))] = (buf_tmp[(Broadcast(j_outer * 8ll, 8)) + (Ramp(0ll, 1ll, 8))]) + (tx[(Broadcast(j_outer * 8ll, 8)) + (Ramp(0ll, 1ll, 8))]);
after vec: 
{
  buf_tmp[(Broadcast(j_outer * 8ll, 8)) + (Ramp(0ll, 1ll, 8))] = (buf_tmp[(Broadcast(j_outer * 8ll, 8)) + (Ramp(0ll, 1ll, 8))]) + (tx[(Broadcast(j_outer * 8ll, 8)) + (Ramp(0ll, 1ll, 8))]);
}
lanes: 8
stride value cstr: 1
lanes: 8
stride value cstr: 1
lanes: 8
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
orig f: 
for (int64_t i_inner = 0ll; i_inner < 8ll; i_inner++) {
  buf_tmp[i_outer * 8ll + i_inner] = 0.f;
}
before normalize: 
for (int64_t i_inner = 0ll; i_inner < 8ll; i_inner++) {
  buf_tmp[i_outer * 8ll + i_inner] = 0.f;
}
before flatten: 
for (int64_t i_inner = 0ll; i_inner < 8ll; i_inner++) {
  buf_tmp[i_outer * 8ll + i_inner] = 0.f;
}
before vec: 
for (int64_t i_inner = 0ll; i_inner < 8ll; i_inner++) {
  buf_tmp[i_outer * 8ll + i_inner] = 0.f;
}
loop start_: 0
loop lanes_: 8
stmt in Block before mutate: 
buf_tmp[i_outer * 8ll + i_inner] = 0.f;
old_in: 
i_outer * 8ll + i_inner
old_in: 
i_outer * 8ll
old_in: 
i_outer
new_in: 
i_outer
old_in: 
8ll
new_in: 
8ll
new_in: 
i_outer * 8ll
old_in: 
i_inner
stride value: 1
stride value cstr: 1
new_in: 
Ramp(0ll, 1ll, 8)
insert broadcasts for 0
broadcast make
lanes: 8
insert broadcasts for 1
no broadcast make
new_in: 
(Broadcast(i_outer * 8ll, 8)) + (Ramp(0ll, 1ll, 8))
old_in: 
0.f
new_in: 
0.f
insert broadcasts for 0
no broadcast make
insert broadcasts for 1
broadcast make
lanes: 8
stmt in Block after mutate: 
buf_tmp[(Broadcast(i_outer * 8ll, 8)) + (Ramp(0ll, 1ll, 8))] = Broadcast(0.f, 8);
after vec: 
{
  buf_tmp[(Broadcast(i_outer * 8ll, 8)) + (Ramp(0ll, 1ll, 8))] = Broadcast(0.f, 8);
}
lanes: 8
stride value cstr: 1
lanes: 8
stride value cstr: 1
stride value cstr: 1
lanes: 8
stride value cstr: 1
lanes: 8
stride value cstr: 1
lanes: 8
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
lanes: 8
stride value cstr: 1
lanes: 8
stride value cstr: 1
lanes: 8
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
lanes: 8
stride value cstr: 1
lanes: 8
stride value cstr: 1
lanes: 8
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
lanes: 8
stride value cstr: 1
lanes: 8
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
[DEBUG kernel.cpp:881] Final Stmt:
[DEBUG kernel.cpp:881] {
[DEBUG kernel.cpp:881]   buf_tmp[Ramp(0ll, 1ll, 8)] = Broadcast(0.f, 8);
[DEBUG kernel.cpp:881]   sum[0ll] = 0.f;
[DEBUG kernel.cpp:881]   buf_tmp[Ramp(0ll, 1ll, 8)] = (buf_tmp[Ramp(0ll, 1ll, 8)]) + (tx[Ramp(0ll, 1ll, 8)]);
[DEBUG kernel.cpp:881]   for (int64_t j_inner = 0ll; j_inner < 8ll; j_inner++) {
[DEBUG kernel.cpp:881]     sum[0ll] = (sum[0ll]) + (buf_tmp[j_inner]);
[DEBUG kernel.cpp:881]   }
[DEBUG kernel.cpp:881] }
stride value cstr: 1
lanes: 8
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
lanes: 8
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
lanes: 8
stride value cstr: 1
stride value cstr: 1
stride value cstr: 1
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] LLVM module before optimizations
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] ; ModuleID = 'pytorch'
[DEBUG llvm_codegen.cpp:633] source_filename = "pytorch"
[DEBUG llvm_codegen.cpp:633] target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
[DEBUG llvm_codegen.cpp:633] target triple = "x86_64-unknown-linux-gnu"
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] ; Function Attrs: alwaysinline
[DEBUG llvm_codegen.cpp:633] define private i32 @pytorch(float* noalias %0, float* noalias %1) #0 {
[DEBUG llvm_codegen.cpp:633] entry:
[DEBUG llvm_codegen.cpp:633]   %2 = alloca float, i64 32, align 4
[DEBUG llvm_codegen.cpp:633]   %3 = getelementptr float, float* %2, i64 0
[DEBUG llvm_codegen.cpp:633]   %4 = bitcast float* %3 to <8 x float>*
[DEBUG llvm_codegen.cpp:633]   store <8 x float> zeroinitializer, <8 x float>* %4, align 4
[DEBUG llvm_codegen.cpp:633]   %5 = getelementptr float, float* %1, i64 0
[DEBUG llvm_codegen.cpp:633]   store float 0.000000e+00, float* %5, align 4
[DEBUG llvm_codegen.cpp:633]   %6 = getelementptr float, float* %2, i64 0
[DEBUG llvm_codegen.cpp:633]   %7 = bitcast float* %6 to <8 x float>*
[DEBUG llvm_codegen.cpp:633]   %8 = load <8 x float>, <8 x float>* %7, align 4
[DEBUG llvm_codegen.cpp:633]   %9 = getelementptr float, float* %0, i64 0
[DEBUG llvm_codegen.cpp:633]   %10 = bitcast float* %9 to <8 x float>*
[DEBUG llvm_codegen.cpp:633]   %11 = load <8 x float>, <8 x float>* %10, align 4
[DEBUG llvm_codegen.cpp:633]   %12 = fadd <8 x float> %8, %11
[DEBUG llvm_codegen.cpp:633]   %13 = getelementptr float, float* %2, i64 0
[DEBUG llvm_codegen.cpp:633]   %14 = bitcast float* %13 to <8 x float>*
[DEBUG llvm_codegen.cpp:633]   store <8 x float> %12, <8 x float>* %14, align 4
[DEBUG llvm_codegen.cpp:633]   br label %cond
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] cond:                                             ; preds = %body, %entry
[DEBUG llvm_codegen.cpp:633]   %15 = phi i64 [ 0, %entry ], [ %23, %body ]
[DEBUG llvm_codegen.cpp:633]   %16 = icmp slt i64 %15, 8
[DEBUG llvm_codegen.cpp:633]   br i1 %16, label %body, label %exit
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] body:                                             ; preds = %cond
[DEBUG llvm_codegen.cpp:633]   %17 = getelementptr float, float* %1, i64 0
[DEBUG llvm_codegen.cpp:633]   %18 = load float, float* %17, align 4
[DEBUG llvm_codegen.cpp:633]   %19 = getelementptr float, float* %2, i64 %15
[DEBUG llvm_codegen.cpp:633]   %20 = load float, float* %19, align 4
[DEBUG llvm_codegen.cpp:633]   %21 = fadd float %18, %20
[DEBUG llvm_codegen.cpp:633]   %22 = getelementptr float, float* %1, i64 0
[DEBUG llvm_codegen.cpp:633]   store float %21, float* %22, align 4
[DEBUG llvm_codegen.cpp:633]   %23 = add i64 %15, 1
[DEBUG llvm_codegen.cpp:633]   br label %cond
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] exit:                                             ; preds = %cond
[DEBUG llvm_codegen.cpp:633]   %24 = alloca i8*, i32 0, align 8
[DEBUG llvm_codegen.cpp:633]   call void @nnc_aten_free(i64 0, i8** %24)
[DEBUG llvm_codegen.cpp:633]   ret i32 0
[DEBUG llvm_codegen.cpp:633] }
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] define i32 @fused_sum(i8** %0) {
[DEBUG llvm_codegen.cpp:633] wrapBB:
[DEBUG llvm_codegen.cpp:633]   %1 = getelementptr i8*, i8** %0, i32 0
[DEBUG llvm_codegen.cpp:633]   %2 = load i8*, i8** %1, align 8
[DEBUG llvm_codegen.cpp:633]   %3 = bitcast i8* %2 to float*
[DEBUG llvm_codegen.cpp:633]   %4 = getelementptr i8*, i8** %0, i32 1
[DEBUG llvm_codegen.cpp:633]   %5 = load i8*, i8** %4, align 8
[DEBUG llvm_codegen.cpp:633]   %6 = bitcast i8* %5 to float*
[DEBUG llvm_codegen.cpp:633]   %7 = call i32 @pytorch(float* %3, float* %6)
[DEBUG llvm_codegen.cpp:633]   ret i32 %7
[DEBUG llvm_codegen.cpp:633] }
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] ; Function Attrs: nounwind
[DEBUG llvm_codegen.cpp:633] declare void @nnc_aten_free(i64, i8**) #1
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] attributes #0 = { alwaysinline }
[DEBUG llvm_codegen.cpp:633] attributes #1 = { nounwind }
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] LLVM module after optimizations
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] ; ModuleID = 'pytorch'
[DEBUG llvm_codegen.cpp:645] source_filename = "pytorch"
[DEBUG llvm_codegen.cpp:645] target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
[DEBUG llvm_codegen.cpp:645] target triple = "x86_64-unknown-linux-gnu"
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] ; Function Attrs: nounwind
[DEBUG llvm_codegen.cpp:645] define i32 @fused_sum(i8** nocapture readonly %0) local_unnamed_addr #0 {
[DEBUG llvm_codegen.cpp:645] wrapBB:
[DEBUG llvm_codegen.cpp:645]   %1 = alloca [0 x i8*], align 8
[DEBUG llvm_codegen.cpp:645]   %2 = bitcast i8** %0 to float**
[DEBUG llvm_codegen.cpp:645]   %3 = load float*, float** %2, align 8
[DEBUG llvm_codegen.cpp:645]   %4 = getelementptr i8*, i8** %0, i64 1
[DEBUG llvm_codegen.cpp:645]   %5 = bitcast i8** %4 to float**
[DEBUG llvm_codegen.cpp:645]   %6 = load float*, float** %5, align 8
[DEBUG llvm_codegen.cpp:645]   call void @llvm.experimental.noalias.scope.decl(metadata !0)
[DEBUG llvm_codegen.cpp:645]   call void @llvm.experimental.noalias.scope.decl(metadata !3)
[DEBUG llvm_codegen.cpp:645]   %7 = bitcast [0 x i8*]* %1 to i8*
[DEBUG llvm_codegen.cpp:645]   call void @llvm.lifetime.start.p0i8(i64 0, i8* %7)
[DEBUG llvm_codegen.cpp:645]   %8 = bitcast float* %3 to <8 x float>*
[DEBUG llvm_codegen.cpp:645]   %9 = load <8 x float>, <8 x float>* %8, align 4, !alias.scope !0, !noalias !3
[DEBUG llvm_codegen.cpp:645]   %10 = fadd <8 x float> %9, zeroinitializer
[DEBUG llvm_codegen.cpp:645]   %.sroa.0.0.vec.extract.i = extractelement <8 x float> %10, i32 0
[DEBUG llvm_codegen.cpp:645]   %11 = fadd float %.sroa.0.0.vec.extract.i, 0.000000e+00
[DEBUG llvm_codegen.cpp:645]   %.sroa.0.4.vec.extract.i = extractelement <8 x float> %10, i32 1
[DEBUG llvm_codegen.cpp:645]   %12 = fadd float %11, %.sroa.0.4.vec.extract.i
[DEBUG llvm_codegen.cpp:645]   %.sroa.0.8.vec.extract.i = extractelement <8 x float> %10, i32 2
[DEBUG llvm_codegen.cpp:645]   %13 = fadd float %12, %.sroa.0.8.vec.extract.i
[DEBUG llvm_codegen.cpp:645]   %.sroa.0.12.vec.extract.i = extractelement <8 x float> %10, i32 3
[DEBUG llvm_codegen.cpp:645]   %14 = fadd float %13, %.sroa.0.12.vec.extract.i
[DEBUG llvm_codegen.cpp:645]   %.sroa.0.16.vec.extract.i = extractelement <8 x float> %10, i32 4
[DEBUG llvm_codegen.cpp:645]   %15 = fadd float %14, %.sroa.0.16.vec.extract.i
[DEBUG llvm_codegen.cpp:645]   %.sroa.0.20.vec.extract.i = extractelement <8 x float> %10, i32 5
[DEBUG llvm_codegen.cpp:645]   %16 = fadd float %15, %.sroa.0.20.vec.extract.i
[DEBUG llvm_codegen.cpp:645]   %.sroa.0.24.vec.extract.i = extractelement <8 x float> %10, i32 6
[DEBUG llvm_codegen.cpp:645]   %17 = fadd float %16, %.sroa.0.24.vec.extract.i
[DEBUG llvm_codegen.cpp:645]   %.sroa.0.28.vec.extract.i = extractelement <8 x float> %10, i32 7
[DEBUG llvm_codegen.cpp:645]   %18 = fadd float %17, %.sroa.0.28.vec.extract.i
[DEBUG llvm_codegen.cpp:645]   store float %18, float* %6, align 4, !alias.scope !3, !noalias !0
[DEBUG llvm_codegen.cpp:645]   %.sub1.i = getelementptr inbounds [0 x i8*], [0 x i8*]* %1, i64 0, i64 0
[DEBUG llvm_codegen.cpp:645]   call void @nnc_aten_free(i64 0, i8** nonnull %.sub1.i) #0, !noalias !5
[DEBUG llvm_codegen.cpp:645]   call void @llvm.lifetime.end.p0i8(i64 0, i8* %7)
[DEBUG llvm_codegen.cpp:645]   ret i32 0
[DEBUG llvm_codegen.cpp:645] }
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] ; Function Attrs: nounwind
[DEBUG llvm_codegen.cpp:645] declare void @nnc_aten_free(i64, i8**) local_unnamed_addr #0
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] ; Function Attrs: inaccessiblememonly nofree nosync nounwind willreturn
[DEBUG llvm_codegen.cpp:645] declare void @llvm.experimental.noalias.scope.decl(metadata) #1
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] ; Function Attrs: argmemonly nofree nosync nounwind willreturn
[DEBUG llvm_codegen.cpp:645] declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #2
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] ; Function Attrs: argmemonly nofree nosync nounwind willreturn
[DEBUG llvm_codegen.cpp:645] declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #2
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] attributes #0 = { nounwind }
[DEBUG llvm_codegen.cpp:645] attributes #1 = { inaccessiblememonly nofree nosync nounwind willreturn }
[DEBUG llvm_codegen.cpp:645] attributes #2 = { argmemonly nofree nosync nounwind willreturn }
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] !0 = !{!1}
[DEBUG llvm_codegen.cpp:645] !1 = distinct !{!1, !2, !"pytorch: argument 0"}
[DEBUG llvm_codegen.cpp:645] !2 = distinct !{!2, !"pytorch"}
[DEBUG llvm_codegen.cpp:645] !3 = !{!4}
[DEBUG llvm_codegen.cpp:645] !4 = distinct !{!4, !2, !"pytorch: argument 1"}
[DEBUG llvm_codegen.cpp:645] !5 = !{!1, !4}
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:662] 
[DEBUG llvm_codegen.cpp:662] LLVM generated assembly code
[DEBUG llvm_codegen.cpp:662] 
[DEBUG llvm_codegen.cpp:662] 	.text
[DEBUG llvm_codegen.cpp:662] 	.file	"pytorch"
[DEBUG llvm_codegen.cpp:662] 	.globl	fused_sum
[DEBUG llvm_codegen.cpp:662] 	.p2align	4, 0x90
[DEBUG llvm_codegen.cpp:662] 	.type	fused_sum,@function
[DEBUG llvm_codegen.cpp:662] fused_sum:
[DEBUG llvm_codegen.cpp:662] 	pushq	%rax
[DEBUG llvm_codegen.cpp:662] 	movq	(%rdi), %rax
[DEBUG llvm_codegen.cpp:662] 	movq	8(%rdi), %rcx
[DEBUG llvm_codegen.cpp:662] 	vxorps	%xmm0, %xmm0, %xmm0
[DEBUG llvm_codegen.cpp:662] 	vaddps	(%rax), %ymm0, %ymm0
[DEBUG llvm_codegen.cpp:662] 	vxorps	%xmm1, %xmm1, %xmm1
[DEBUG llvm_codegen.cpp:662] 	vaddss	%xmm1, %xmm0, %xmm1
[DEBUG llvm_codegen.cpp:662] 	vmovshdup	%xmm0, %xmm2
[DEBUG llvm_codegen.cpp:662] 	vaddss	%xmm2, %xmm1, %xmm1
[DEBUG llvm_codegen.cpp:662] 	vpermilpd	$1, %xmm0, %xmm2
[DEBUG llvm_codegen.cpp:662] 	vaddss	%xmm2, %xmm1, %xmm1
[DEBUG llvm_codegen.cpp:662] 	vpermilps	$255, %xmm0, %xmm2
[DEBUG llvm_codegen.cpp:662] 	vaddss	%xmm2, %xmm1, %xmm1
[DEBUG llvm_codegen.cpp:662] 	vextractf128	$1, %ymm0, %xmm0
[DEBUG llvm_codegen.cpp:662] 	vaddss	%xmm0, %xmm1, %xmm1
[DEBUG llvm_codegen.cpp:662] 	vmovshdup	%xmm0, %xmm2
[DEBUG llvm_codegen.cpp:662] 	vaddss	%xmm2, %xmm1, %xmm1
[DEBUG llvm_codegen.cpp:662] 	vpermilpd	$1, %xmm0, %xmm2
[DEBUG llvm_codegen.cpp:662] 	vaddss	%xmm2, %xmm1, %xmm1
[DEBUG llvm_codegen.cpp:662] 	vpermilps	$255, %xmm0, %xmm0
[DEBUG llvm_codegen.cpp:662] 	vaddss	%xmm0, %xmm1, %xmm0
[DEBUG llvm_codegen.cpp:662] 	vmovss	%xmm0, (%rcx)
[DEBUG llvm_codegen.cpp:662] 	movabsq	$nnc_aten_free, %rax
[DEBUG llvm_codegen.cpp:662] 	movq	%rsp, %rsi
[DEBUG llvm_codegen.cpp:662] 	xorl	%edi, %edi
[DEBUG llvm_codegen.cpp:662] 	vzeroupper
[DEBUG llvm_codegen.cpp:662] 	callq	*%rax
[DEBUG llvm_codegen.cpp:662] 	xorl	%eax, %eax
[DEBUG llvm_codegen.cpp:662] 	popq	%rcx
[DEBUG llvm_codegen.cpp:662] 	retq
[DEBUG llvm_codegen.cpp:662] .Lfunc_end0:
[DEBUG llvm_codegen.cpp:662] 	.size	fused_sum, .Lfunc_end0-fused_sum
[DEBUG llvm_codegen.cpp:662] 
[DEBUG llvm_codegen.cpp:662] 	.section	".note.GNU-stack","",@progbits
[DEBUG llvm_codegen.cpp:662] 
tensor([0.4691])
tensor([0.4691])
.
----------------------------------------------------------------------
Ran 1 test in 0.162s

OK
