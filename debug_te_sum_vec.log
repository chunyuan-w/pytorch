tensor([[ 0.0461,  0.4024, -1.0115,  0.2167, -0.6123,  0.5036,  0.2310,  0.6931]])
[DUMP kernel.cpp:1592] TensorExprKernel graph (Before graph optimization):
[DUMP kernel.cpp:1592] graph(%x : Float(1, 8, strides=[8, 1], requires_grad=0, device=cpu)):
[DUMP kernel.cpp:1592]   %1 : NoneType = prim::Constant()
[DUMP kernel.cpp:1592]   %2 : bool = prim::Constant[value=0]()
[DUMP kernel.cpp:1592]   %3 : int[] = prim::Constant[value=[-1]]()
[DUMP kernel.cpp:1592]   %4 : Float(1, strides=[1], requires_grad=0, device=cpu) = aten::sum(%x, %3, %2, %1) # test/test_te_softmax.py:64:0
[DUMP kernel.cpp:1592]   return (%4)
[DUMP kernel.cpp:1620] TensorExprKernel graph (After graph optimization):
[DUMP kernel.cpp:1620] graph(%x : Float(1, 8, strides=[8, 1], requires_grad=0, device=cpu)):
[DUMP kernel.cpp:1620]   %1 : NoneType = prim::Constant()
[DUMP kernel.cpp:1620]   %2 : bool = prim::Constant[value=0]()
[DUMP kernel.cpp:1620]   %3 : int[] = prim::Constant[value=[-1]]()
[DUMP kernel.cpp:1620]   %4 : Float(1, strides=[1], requires_grad=0, device=cpu) = aten::sum(%x, %3, %2, %1) # test/test_te_softmax.py:64:0
[DUMP kernel.cpp:1620]   return (%4)
[DUMP kernel.cpp:1624] TensorExprKernel graph:
[DUMP kernel.cpp:1624] graph(%x : Float(1, 8, strides=[8, 1], requires_grad=0, device=cpu)):
[DUMP kernel.cpp:1624]   %1 : NoneType = prim::Constant()
[DUMP kernel.cpp:1624]   %2 : bool = prim::Constant[value=0]()
[DUMP kernel.cpp:1624]   %3 : int[] = prim::Constant[value=[-1]]()
[DUMP kernel.cpp:1624]   %4 : Float(1, strides=[1], requires_grad=0, device=cpu) = aten::sum(%x, %3, %2, %1) # test/test_te_softmax.py:64:0
[DUMP kernel.cpp:1624]   return (%4)
[DEBUG kernel.cpp:710] Original Stmt:
[DEBUG kernel.cpp:710] {
[DEBUG kernel.cpp:710]   for (int64_t i = 0ll; i < 1ll; i++) {
[DEBUG kernel.cpp:710]     sum[i] = float(0);
[DEBUG kernel.cpp:710]     for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:710]       sum[i] = ReduceOp((sum[i]) + float(tx[0ll, j]), reduce_args={j});
[DEBUG kernel.cpp:710]     }
[DEBUG kernel.cpp:710]   }
[DEBUG kernel.cpp:710] }
[DEBUG kernel.cpp:732] after simplify{
[DEBUG kernel.cpp:732]   sum[0ll] = 0.f;
[DEBUG kernel.cpp:732]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:732]     sum[0ll] = ReduceOp((sum[0ll]) + (tx[0ll, j]), reduce_args={j});
[DEBUG kernel.cpp:732]   }
[DEBUG kernel.cpp:732] }
[DEBUG kernel.cpp:742] after inline{
[DEBUG kernel.cpp:742]   sum[0ll] = 0.f;
[DEBUG kernel.cpp:742]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:742]     sum[0ll] = ReduceOp((sum[0ll]) + (tx[0ll, j]), reduce_args={j});
[DEBUG kernel.cpp:742]   }
[DEBUG kernel.cpp:742] }
[DEBUG kernel.cpp:758] after fuse{
[DEBUG kernel.cpp:758]   sum[0ll] = 0.f;
[DEBUG kernel.cpp:758]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:758]     sum[0ll] = ReduceOp((sum[0ll]) + (tx[0ll, j]), reduce_args={j});
[DEBUG kernel.cpp:758]   }
[DEBUG kernel.cpp:758] }
[DEBUG kernel.cpp:760] after parallelize{
[DEBUG kernel.cpp:760]   sum[0ll] = 0.f;
[DEBUG kernel.cpp:760]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:760]     sum[0ll] = ReduceOp((sum[0ll]) + (tx[0ll, j]), reduce_args={j});
[DEBUG kernel.cpp:760]   }
[DEBUG kernel.cpp:760] }
[DEBUG kernel.cpp:845] after prepareForCodegen{
[DEBUG kernel.cpp:845]   sum[0ll] = 0.f;
[DEBUG kernel.cpp:845]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:845]     sum[0ll] = (sum[0ll]) + (tx[(0ll + 0ll * 8ll) + j * 1ll]);
[DEBUG kernel.cpp:845]   }
[DEBUG kernel.cpp:845] }
[DEBUG kernel.cpp:847] after simplification{
[DEBUG kernel.cpp:847]   sum[0ll] = 0.f;
[DEBUG kernel.cpp:847]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:847]     sum[0ll] = (sum[0ll]) + (tx[j]);
[DEBUG kernel.cpp:847]   }
[DEBUG kernel.cpp:847] }
st: 
sum[0ll] = 0.f;
f: 
for (int64_t j = 0ll; j < 8ll; j++) {
  sum[0ll] = (sum[0ll]) + (tx[j]);
}
worklist size: 1
stmt: sum[0ll] = (sum[0ll]) + (tx[j]);
add_right: tx[j]
loop: 
for (int64_t j = 0ll; j < 8ll; j++) {
  sum[0ll] = (sum[0ll]) + (tx[j]);
}
tmp_add: 
for (int64_t j = 0ll; j < 8ll; j++) {
  buf_tmp[j] = (buf_tmp[j]) + (tx[j]);
}
[DEBUG kernel.cpp:855] after vectorizeReduction{
[DEBUG kernel.cpp:855]   for (int64_t i = 0ll; i < 8ll; i++) {
[DEBUG kernel.cpp:855]     buf_tmp[i] = 0.f;
[DEBUG kernel.cpp:855]   }
[DEBUG kernel.cpp:855]   sum[0ll] = 0.f;
[DEBUG kernel.cpp:855]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:855]     buf_tmp[j] = (buf_tmp[j]) + (tx[j]);
[DEBUG kernel.cpp:855]   }
[DEBUG kernel.cpp:855]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:855]     sum[0ll] = ReduceOp((sum[0ll]) + (buf_tmp[j]), reduce_args={j});
[DEBUG kernel.cpp:855]   }
[DEBUG kernel.cpp:855] }
[DEBUG kernel.cpp:879] Final Stmt:
[DEBUG kernel.cpp:879] {
[DEBUG kernel.cpp:879]   for (int64_t i = 0ll; i < 8ll; i++) {
[DEBUG kernel.cpp:879]     buf_tmp[i] = 0.f;
[DEBUG kernel.cpp:879]   }
[DEBUG kernel.cpp:879]   sum[0ll] = 0.f;
[DEBUG kernel.cpp:879]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:879]     buf_tmp[j] = (buf_tmp[j]) + (tx[j]);
[DEBUG kernel.cpp:879]   }
[DEBUG kernel.cpp:879]   for (int64_t j = 0ll; j < 8ll; j++) {
[DEBUG kernel.cpp:879]     sum[0ll] = (sum[0ll]) + (buf_tmp[j]);
[DEBUG kernel.cpp:879]   }
[DEBUG kernel.cpp:879] }
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] LLVM module before optimizations
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] ; ModuleID = 'pytorch'
[DEBUG llvm_codegen.cpp:633] source_filename = "pytorch"
[DEBUG llvm_codegen.cpp:633] target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
[DEBUG llvm_codegen.cpp:633] target triple = "x86_64-unknown-linux-gnu"
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] ; Function Attrs: alwaysinline
[DEBUG llvm_codegen.cpp:633] define private i32 @pytorch(float* noalias %0, float* noalias %1) #0 {
[DEBUG llvm_codegen.cpp:633] entry:
[DEBUG llvm_codegen.cpp:633]   %2 = alloca float, i64 32, align 4
[DEBUG llvm_codegen.cpp:633]   br label %cond
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] cond:                                             ; preds = %body, %entry
[DEBUG llvm_codegen.cpp:633]   %3 = phi i64 [ 0, %entry ], [ %6, %body ]
[DEBUG llvm_codegen.cpp:633]   %4 = icmp slt i64 %3, 8
[DEBUG llvm_codegen.cpp:633]   br i1 %4, label %body, label %exit
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] body:                                             ; preds = %cond
[DEBUG llvm_codegen.cpp:633]   %5 = getelementptr float, float* %2, i64 %3
[DEBUG llvm_codegen.cpp:633]   store float 0.000000e+00, float* %5, align 4
[DEBUG llvm_codegen.cpp:633]   %6 = add i64 %3, 1
[DEBUG llvm_codegen.cpp:633]   br label %cond
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] exit:                                             ; preds = %cond
[DEBUG llvm_codegen.cpp:633]   %7 = getelementptr float, float* %1, i64 0
[DEBUG llvm_codegen.cpp:633]   store float 0.000000e+00, float* %7, align 4
[DEBUG llvm_codegen.cpp:633]   br label %cond1
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] cond1:                                            ; preds = %body2, %exit
[DEBUG llvm_codegen.cpp:633]   %8 = phi i64 [ 0, %exit ], [ %16, %body2 ]
[DEBUG llvm_codegen.cpp:633]   %9 = icmp slt i64 %8, 8
[DEBUG llvm_codegen.cpp:633]   br i1 %9, label %body2, label %exit3
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] body2:                                            ; preds = %cond1
[DEBUG llvm_codegen.cpp:633]   %10 = getelementptr float, float* %2, i64 %8
[DEBUG llvm_codegen.cpp:633]   %11 = load float, float* %10, align 4
[DEBUG llvm_codegen.cpp:633]   %12 = getelementptr float, float* %0, i64 %8
[DEBUG llvm_codegen.cpp:633]   %13 = load float, float* %12, align 4
[DEBUG llvm_codegen.cpp:633]   %14 = fadd float %11, %13
[DEBUG llvm_codegen.cpp:633]   %15 = getelementptr float, float* %2, i64 %8
[DEBUG llvm_codegen.cpp:633]   store float %14, float* %15, align 4
[DEBUG llvm_codegen.cpp:633]   %16 = add i64 %8, 1
[DEBUG llvm_codegen.cpp:633]   br label %cond1
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] exit3:                                            ; preds = %cond1
[DEBUG llvm_codegen.cpp:633]   br label %cond4
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] cond4:                                            ; preds = %body5, %exit3
[DEBUG llvm_codegen.cpp:633]   %17 = phi i64 [ 0, %exit3 ], [ %25, %body5 ]
[DEBUG llvm_codegen.cpp:633]   %18 = icmp slt i64 %17, 8
[DEBUG llvm_codegen.cpp:633]   br i1 %18, label %body5, label %exit6
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] body5:                                            ; preds = %cond4
[DEBUG llvm_codegen.cpp:633]   %19 = getelementptr float, float* %1, i64 0
[DEBUG llvm_codegen.cpp:633]   %20 = load float, float* %19, align 4
[DEBUG llvm_codegen.cpp:633]   %21 = getelementptr float, float* %2, i64 %17
[DEBUG llvm_codegen.cpp:633]   %22 = load float, float* %21, align 4
[DEBUG llvm_codegen.cpp:633]   %23 = fadd float %20, %22
[DEBUG llvm_codegen.cpp:633]   %24 = getelementptr float, float* %1, i64 0
[DEBUG llvm_codegen.cpp:633]   store float %23, float* %24, align 4
[DEBUG llvm_codegen.cpp:633]   %25 = add i64 %17, 1
[DEBUG llvm_codegen.cpp:633]   br label %cond4
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] exit6:                                            ; preds = %cond4
[DEBUG llvm_codegen.cpp:633]   %26 = alloca i8*, i32 0, align 8
[DEBUG llvm_codegen.cpp:633]   call void @nnc_aten_free(i64 0, i8** %26)
[DEBUG llvm_codegen.cpp:633]   ret i32 0
[DEBUG llvm_codegen.cpp:633] }
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] define i32 @fused_sum(i8** %0) {
[DEBUG llvm_codegen.cpp:633] wrapBB:
[DEBUG llvm_codegen.cpp:633]   %1 = getelementptr i8*, i8** %0, i32 0
[DEBUG llvm_codegen.cpp:633]   %2 = load i8*, i8** %1, align 8
[DEBUG llvm_codegen.cpp:633]   %3 = bitcast i8* %2 to float*
[DEBUG llvm_codegen.cpp:633]   %4 = getelementptr i8*, i8** %0, i32 1
[DEBUG llvm_codegen.cpp:633]   %5 = load i8*, i8** %4, align 8
[DEBUG llvm_codegen.cpp:633]   %6 = bitcast i8* %5 to float*
[DEBUG llvm_codegen.cpp:633]   %7 = call i32 @pytorch(float* %3, float* %6)
[DEBUG llvm_codegen.cpp:633]   ret i32 %7
[DEBUG llvm_codegen.cpp:633] }
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] ; Function Attrs: nounwind
[DEBUG llvm_codegen.cpp:633] declare void @nnc_aten_free(i64, i8**) #1
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:633] attributes #0 = { alwaysinline }
[DEBUG llvm_codegen.cpp:633] attributes #1 = { nounwind }
[DEBUG llvm_codegen.cpp:633] 
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] LLVM module after optimizations
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] ; ModuleID = 'pytorch'
[DEBUG llvm_codegen.cpp:645] source_filename = "pytorch"
[DEBUG llvm_codegen.cpp:645] target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
[DEBUG llvm_codegen.cpp:645] target triple = "x86_64-unknown-linux-gnu"
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] ; Function Attrs: nounwind
[DEBUG llvm_codegen.cpp:645] define i32 @fused_sum(i8** nocapture readonly %0) local_unnamed_addr #0 {
[DEBUG llvm_codegen.cpp:645] wrapBB:
[DEBUG llvm_codegen.cpp:645]   %1 = alloca [0 x i8*], align 8
[DEBUG llvm_codegen.cpp:645]   %2 = bitcast i8** %0 to float**
[DEBUG llvm_codegen.cpp:645]   %3 = load float*, float** %2, align 8
[DEBUG llvm_codegen.cpp:645]   %4 = getelementptr i8*, i8** %0, i64 1
[DEBUG llvm_codegen.cpp:645]   %5 = bitcast i8** %4 to float**
[DEBUG llvm_codegen.cpp:645]   %6 = load float*, float** %5, align 8
[DEBUG llvm_codegen.cpp:645]   call void @llvm.experimental.noalias.scope.decl(metadata !0)
[DEBUG llvm_codegen.cpp:645]   call void @llvm.experimental.noalias.scope.decl(metadata !3)
[DEBUG llvm_codegen.cpp:645]   %7 = bitcast [0 x i8*]* %1 to i8*
[DEBUG llvm_codegen.cpp:645]   call void @llvm.lifetime.start.p0i8(i64 0, i8* %7)
[DEBUG llvm_codegen.cpp:645]   %8 = load float, float* %3, align 4, !alias.scope !0, !noalias !3
[DEBUG llvm_codegen.cpp:645]   %9 = fadd float %8, 0.000000e+00
[DEBUG llvm_codegen.cpp:645]   %10 = getelementptr float, float* %3, i64 1
[DEBUG llvm_codegen.cpp:645]   %11 = load float, float* %10, align 4, !alias.scope !0, !noalias !3
[DEBUG llvm_codegen.cpp:645]   %12 = fadd float %11, 0.000000e+00
[DEBUG llvm_codegen.cpp:645]   %13 = getelementptr float, float* %3, i64 2
[DEBUG llvm_codegen.cpp:645]   %14 = load float, float* %13, align 4, !alias.scope !0, !noalias !3
[DEBUG llvm_codegen.cpp:645]   %15 = fadd float %14, 0.000000e+00
[DEBUG llvm_codegen.cpp:645]   %16 = getelementptr float, float* %3, i64 3
[DEBUG llvm_codegen.cpp:645]   %17 = load float, float* %16, align 4, !alias.scope !0, !noalias !3
[DEBUG llvm_codegen.cpp:645]   %18 = fadd float %17, 0.000000e+00
[DEBUG llvm_codegen.cpp:645]   %19 = getelementptr float, float* %3, i64 4
[DEBUG llvm_codegen.cpp:645]   %20 = load float, float* %19, align 4, !alias.scope !0, !noalias !3
[DEBUG llvm_codegen.cpp:645]   %21 = fadd float %20, 0.000000e+00
[DEBUG llvm_codegen.cpp:645]   %22 = getelementptr float, float* %3, i64 5
[DEBUG llvm_codegen.cpp:645]   %23 = load float, float* %22, align 4, !alias.scope !0, !noalias !3
[DEBUG llvm_codegen.cpp:645]   %24 = fadd float %23, 0.000000e+00
[DEBUG llvm_codegen.cpp:645]   %25 = getelementptr float, float* %3, i64 6
[DEBUG llvm_codegen.cpp:645]   %26 = load float, float* %25, align 4, !alias.scope !0, !noalias !3
[DEBUG llvm_codegen.cpp:645]   %27 = fadd float %26, 0.000000e+00
[DEBUG llvm_codegen.cpp:645]   %28 = getelementptr float, float* %3, i64 7
[DEBUG llvm_codegen.cpp:645]   %29 = load float, float* %28, align 4, !alias.scope !0, !noalias !3
[DEBUG llvm_codegen.cpp:645]   %30 = fadd float %29, 0.000000e+00
[DEBUG llvm_codegen.cpp:645]   %31 = fadd float %9, %12
[DEBUG llvm_codegen.cpp:645]   %32 = fadd float %31, %15
[DEBUG llvm_codegen.cpp:645]   %33 = fadd float %32, %18
[DEBUG llvm_codegen.cpp:645]   %34 = fadd float %33, %21
[DEBUG llvm_codegen.cpp:645]   %35 = fadd float %34, %24
[DEBUG llvm_codegen.cpp:645]   %36 = fadd float %35, %27
[DEBUG llvm_codegen.cpp:645]   %37 = fadd float %36, %30
[DEBUG llvm_codegen.cpp:645]   store float %37, float* %6, align 4, !alias.scope !3, !noalias !0
[DEBUG llvm_codegen.cpp:645]   %.sub1.i = getelementptr inbounds [0 x i8*], [0 x i8*]* %1, i64 0, i64 0
[DEBUG llvm_codegen.cpp:645]   call void @nnc_aten_free(i64 0, i8** nonnull %.sub1.i) #0, !noalias !5
[DEBUG llvm_codegen.cpp:645]   call void @llvm.lifetime.end.p0i8(i64 0, i8* %7)
[DEBUG llvm_codegen.cpp:645]   ret i32 0
[DEBUG llvm_codegen.cpp:645] }
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] ; Function Attrs: nounwind
[DEBUG llvm_codegen.cpp:645] declare void @nnc_aten_free(i64, i8**) local_unnamed_addr #0
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] ; Function Attrs: inaccessiblememonly nofree nosync nounwind willreturn
[DEBUG llvm_codegen.cpp:645] declare void @llvm.experimental.noalias.scope.decl(metadata) #1
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] ; Function Attrs: argmemonly nofree nosync nounwind willreturn
[DEBUG llvm_codegen.cpp:645] declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #2
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] ; Function Attrs: argmemonly nofree nosync nounwind willreturn
[DEBUG llvm_codegen.cpp:645] declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #2
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] attributes #0 = { nounwind }
[DEBUG llvm_codegen.cpp:645] attributes #1 = { inaccessiblememonly nofree nosync nounwind willreturn }
[DEBUG llvm_codegen.cpp:645] attributes #2 = { argmemonly nofree nosync nounwind willreturn }
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:645] !0 = !{!1}
[DEBUG llvm_codegen.cpp:645] !1 = distinct !{!1, !2, !"pytorch: argument 0"}
[DEBUG llvm_codegen.cpp:645] !2 = distinct !{!2, !"pytorch"}
[DEBUG llvm_codegen.cpp:645] !3 = !{!4}
[DEBUG llvm_codegen.cpp:645] !4 = distinct !{!4, !2, !"pytorch: argument 1"}
[DEBUG llvm_codegen.cpp:645] !5 = !{!1, !4}
[DEBUG llvm_codegen.cpp:645] 
[DEBUG llvm_codegen.cpp:662] 
[DEBUG llvm_codegen.cpp:662] LLVM generated assembly code
[DEBUG llvm_codegen.cpp:662] 
[DEBUG llvm_codegen.cpp:662] 	.text
[DEBUG llvm_codegen.cpp:662] 	.file	"pytorch"
[DEBUG llvm_codegen.cpp:662] 	.globl	fused_sum
[DEBUG llvm_codegen.cpp:662] 	.p2align	4, 0x90
[DEBUG llvm_codegen.cpp:662] 	.type	fused_sum,@function
[DEBUG llvm_codegen.cpp:662] fused_sum:
[DEBUG llvm_codegen.cpp:662] 	pushq	%rax
[DEBUG llvm_codegen.cpp:662] 	movq	(%rdi), %rax
[DEBUG llvm_codegen.cpp:662] 	vxorps	%xmm0, %xmm0, %xmm0
[DEBUG llvm_codegen.cpp:662] 	vaddss	(%rax), %xmm0, %xmm1
[DEBUG llvm_codegen.cpp:662] 	movq	8(%rdi), %rcx
[DEBUG llvm_codegen.cpp:662] 	vaddss	4(%rax), %xmm0, %xmm2
[DEBUG llvm_codegen.cpp:662] 	vaddss	%xmm2, %xmm1, %xmm1
[DEBUG llvm_codegen.cpp:662] 	vaddss	8(%rax), %xmm0, %xmm2
[DEBUG llvm_codegen.cpp:662] 	vaddss	%xmm2, %xmm1, %xmm1
[DEBUG llvm_codegen.cpp:662] 	vaddss	12(%rax), %xmm0, %xmm2
[DEBUG llvm_codegen.cpp:662] 	vaddss	%xmm2, %xmm1, %xmm1
[DEBUG llvm_codegen.cpp:662] 	vaddss	16(%rax), %xmm0, %xmm2
[DEBUG llvm_codegen.cpp:662] 	vaddss	%xmm2, %xmm1, %xmm1
[DEBUG llvm_codegen.cpp:662] 	vaddss	20(%rax), %xmm0, %xmm2
[DEBUG llvm_codegen.cpp:662] 	vaddss	%xmm2, %xmm1, %xmm1
[DEBUG llvm_codegen.cpp:662] 	vaddss	24(%rax), %xmm0, %xmm2
[DEBUG llvm_codegen.cpp:662] 	vaddss	%xmm2, %xmm1, %xmm1
[DEBUG llvm_codegen.cpp:662] 	vaddss	28(%rax), %xmm0, %xmm0
[DEBUG llvm_codegen.cpp:662] 	vaddss	%xmm0, %xmm1, %xmm0
[DEBUG llvm_codegen.cpp:662] 	vmovss	%xmm0, (%rcx)
[DEBUG llvm_codegen.cpp:662] 	movabsq	$nnc_aten_free, %rax
[DEBUG llvm_codegen.cpp:662] 	movq	%rsp, %rsi
[DEBUG llvm_codegen.cpp:662] 	xorl	%edi, %edi
[DEBUG llvm_codegen.cpp:662] 	callq	*%rax
[DEBUG llvm_codegen.cpp:662] 	xorl	%eax, %eax
[DEBUG llvm_codegen.cpp:662] 	popq	%rcx
[DEBUG llvm_codegen.cpp:662] 	retq
[DEBUG llvm_codegen.cpp:662] .Lfunc_end0:
[DEBUG llvm_codegen.cpp:662] 	.size	fused_sum, .Lfunc_end0-fused_sum
[DEBUG llvm_codegen.cpp:662] 
[DEBUG llvm_codegen.cpp:662] 	.section	".note.GNU-stack","",@progbits
[DEBUG llvm_codegen.cpp:662] 
tensor([0.4691])
tensor([0.4691])
.
----------------------------------------------------------------------
Ran 1 test in 0.157s

OK
