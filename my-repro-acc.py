
# AOT ID: ['0_inference']
from ctypes import c_void_p, c_long
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align

from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()
_frozen_param0 = None  # device(type='cpu') torch.float32 (128,) (1,) 7f41ef7e9580
_frozen_param1 = None  # device(type='cpu') torch.float32 (128,) (1,) 7f41ef7e8d60
_frozen_param2 = None  # device(type='cpu') torch.float32 (128,) (1,) 7f41ef7e9c10
_frozen_param3 = None  # device(type='cpu') torch.float32 (128,) (1,) 7f41ef7ea250
_frozen_param5 = None  # device(type='cpu') torch.float32 (128,) (1,) 7f41ef7ea020
_frozen_param6 = None  # device(type='cpu') torch.float32 (128,) (1,) 7f41ef7e98a0
_frozen_param8 = None  # device(type='cpu') torch.float32 (128,) (1,) 7f41ef7ea200
_frozen_param9 = None  # device(type='cpu') torch.float32 (128,) (1,) 7f41ef7e9620
_frozen_param11 = None  # device(type='cpu') torch.float32 (128,) (1,) 7f41ef7e9ee0
_frozen_param12 = None  # device(type='cpu') torch.float32 (128,) (1,) 7f41ef7e9440
_frozen_param13 = None  # device(type='cpu') torch.float32 (256,) (1,) 7f41ef7eab60
_frozen_param14 = None  # device(type='cpu') torch.float32 (256,) (1,) 7f41ef7ea4d0
_frozen_param16 = None  # device(type='cpu') torch.float32 (256,) (1,) 7f41ef7ea980
_frozen_param17 = None  # device(type='cpu') torch.float32 (256,) (1,) 7f41ef7ea610
_frozen_param19 = None  # device(type='cpu') torch.float32 (256,) (1,) 7f41ef7eab10
_frozen_param20 = None  # device(type='cpu') torch.float32 (256,) (1,) 7f41ef7ea480
_frozen_param22 = None  # device(type='cpu') torch.float32 (256,) (1,) 7f41ef7ea700
_frozen_param23 = None  # device(type='cpu') torch.float32 (256,) (1,) 7f41ef7e9800
_frozen_param24 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7eb060
_frozen_param25 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7eb740
_frozen_param27 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7eafc0
_frozen_param28 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7eb420
_frozen_param30 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7eb380
_frozen_param31 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7eb3d0
_frozen_param33 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e9030
_frozen_param34 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e8770
_frozen_param36 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7eb9c0
_frozen_param37 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e8ef0
_frozen_param39 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e5800
_frozen_param40 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e5cb0
_frozen_param42 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e57b0
_frozen_param43 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e58a0
_frozen_param45 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e5440
_frozen_param46 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e4810
_frozen_param48 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e4ea0
_frozen_param49 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e4d10
_frozen_param51 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e4b30
_frozen_param52 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e4900
_frozen_param54 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e48b0
_frozen_param55 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e4590
_frozen_param57 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e7d80
_frozen_param58 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e4360
_frozen_param60 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e4450
_frozen_param61 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e4090
_frozen_param63 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e4180
_frozen_param64 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e7ce0
_frozen_param66 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e7e20
_frozen_param67 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e77e0
_frozen_param69 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e7a60
_frozen_param70 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e7420
_frozen_param72 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e7470
_frozen_param73 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e5df0
_frozen_param75 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e76a0
_frozen_param76 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e68e0
_frozen_param78 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e72e0
_frozen_param79 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e6520
_frozen_param81 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e6e30
_frozen_param82 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e6f20
_frozen_param84 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e67f0
_frozen_param85 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e62a0
_frozen_param87 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e67a0
_frozen_param88 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e6340
_frozen_param90 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e53a0
_frozen_param91 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e5f80
_frozen_param93 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e5760
_frozen_param94 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e5c60
_frozen_param96 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e5a80
_frozen_param97 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7d98f0
_frozen_param99 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7d9940
_frozen_param100 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7d9440
_frozen_param102 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7d8b30
_frozen_param103 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7dbd80
_frozen_param105 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7e4e00
_frozen_param106 = None  # device(type='cpu') torch.float32 (512,) (1,) 7f41ef7d8d10
_frozen_param107 = None  # device(type='cpu') torch.float32 (1024,) (1,) 7f41ef7d9260
_frozen_param108 = None  # device(type='cpu') torch.float32 (1024,) (1,) 7f41ef7d9030
_frozen_param110 = None  # device(type='cpu') torch.float32 (1024,) (1,) 7f41ef7d90d0
_frozen_param111 = None  # device(type='cpu') torch.float32 (1024,) (1,) 7f41ef7d9080
_frozen_param113 = None  # device(type='cpu') torch.float32 (1024,) (1,) 7f41ef7d92b0
_frozen_param114 = None  # device(type='cpu') torch.float32 (1024,) (1,) 7f41ef7d9f80
_frozen_param116 = None  # device(type='cpu') torch.float32 (1024,) (1,) 7f41ef7e9850
_frozen_param117 = None  # device(type='cpu') torch.float32 (1024,) (1,) 7f41ef7e9120
_frozen_param344 = None  # device(type='cpu') torch.bfloat16 (128,) (1,) 7f419fc02b60
_frozen_param606 = None  # device(type='cpu') torch.bfloat16 (128, 3, 4, 4) (1, 0, 0, 0) 7f419fac1620
_frozen_param346 = None  # device(type='cpu') torch.bfloat16 (128,) (1,) 7f41a03a2070
_frozen_param607 = None  # device(type='cpu') torch.bfloat16 (128, 1, 7, 7) (1, 0, 0, 0) 7f419fa93150
_frozen_param348 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fc1dc60
_frozen_param608 = None  # device(type='cpu') torch.bfloat16 (512, 128) (1, 0) 7f419faf2d40
_frozen_param350 = None  # device(type='cpu') torch.bfloat16 (128,) (1,) 7f419fc313a0
_frozen_param609 = None  # device(type='cpu') torch.bfloat16 (128, 512) (1, 0) 7f419fadbab0
_frozen_param352 = None  # device(type='cpu') torch.float32 (1, 128, 1, 1) (128, 1, 1, 1) 7f419feb8770
_frozen_param353 = None  # device(type='cpu') torch.bfloat16 (128,) (1,) 7f419fc9d7b0
_frozen_param610 = None  # device(type='cpu') torch.bfloat16 (128, 1, 7, 7) (1, 0, 0, 0) 7f419f989df0
constant93 = None  # device(type='cpu') torch.bfloat16 (16, 128, 32) (4096, 32, 1) 7f419f2218a0
constant94 = None  # device(type='cpu') torch.bfloat16 (4, 512, 32) (16384, 32, 1) 7f419f223bf0
_frozen_param355 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fea0fe0
_frozen_param611 = None  # device(type='cpu') torch.bfloat16 (512, 128) (1, 0) 7f419faa4540
_frozen_param357 = None  # device(type='cpu') torch.bfloat16 (128,) (1,) 7f419fa271a0
_frozen_param612 = None  # device(type='cpu') torch.bfloat16 (128, 512) (1, 0) 7f419faae390
_frozen_param359 = None  # device(type='cpu') torch.float32 (1, 128, 1, 1) (128, 1, 1, 1) 7f419fa83c90
_frozen_param360 = None  # device(type='cpu') torch.bfloat16 (128,) (1,) 7f419fed3dd0
_frozen_param613 = None  # device(type='cpu') torch.bfloat16 (128, 1, 7, 7) (1, 0, 0, 0) 7f419f989f80
constant102 = None  # device(type='cpu') torch.bfloat16 (16, 128, 32) (4096, 32, 1) 7f419f246660
constant103 = None  # device(type='cpu') torch.bfloat16 (4, 512, 32) (16384, 32, 1) 7f419f2618f0
_frozen_param362 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa83f10
_frozen_param614 = None  # device(type='cpu') torch.bfloat16 (512, 128) (1, 0) 7f419f989cb0
_frozen_param364 = None  # device(type='cpu') torch.bfloat16 (128,) (1,) 7f419fee4f90
_frozen_param615 = None  # device(type='cpu') torch.bfloat16 (128, 512) (1, 0) 7f419f989c60
_frozen_param366 = None  # device(type='cpu') torch.float32 (1, 128, 1, 1) (128, 1, 1, 1) 7f419fa880e0
constant109 = None  # device(type='cpu') torch.bfloat16 (16, 128, 32) (4096, 32, 1) 7f419f29ade0
constant110 = None  # device(type='cpu') torch.bfloat16 (4, 512, 32) (16384, 32, 1) 7f419f2a0e00
_frozen_param367 = None  # device(type='cpu') torch.bfloat16 (256,) (1,) 7f419fa83e70
_frozen_param616 = None  # device(type='cpu') torch.bfloat16 (256, 128, 2, 2) (1, 0, 0, 0) 7f419f98a110
_frozen_param369 = None  # device(type='cpu') torch.bfloat16 (256,) (1,) 7f419fa883b0
_frozen_param617 = None  # device(type='cpu') torch.bfloat16 (256, 1, 7, 7) (1, 0, 0, 0) 7f419f98a200
_frozen_param371 = None  # device(type='cpu') torch.bfloat16 (1024,) (1,) 7f419fa88540
_frozen_param618 = None  # device(type='cpu') torch.bfloat16 (1024, 256) (1, 0) 7f419f989d50
_frozen_param373 = None  # device(type='cpu') torch.bfloat16 (256,) (1,) 7f419fa88590
_frozen_param619 = None  # device(type='cpu') torch.bfloat16 (256, 1024) (1, 0) 7f419f989e90
_frozen_param375 = None  # device(type='cpu') torch.float32 (1, 256, 1, 1) (256, 1, 1, 1) 7f419fa884a0
_frozen_param376 = None  # device(type='cpu') torch.bfloat16 (256,) (1,) 7f419fa88720
_frozen_param620 = None  # device(type='cpu') torch.bfloat16 (256, 1, 7, 7) (1, 0, 0, 0) 7f419f98a390
constant122 = None  # device(type='cpu') torch.bfloat16 (64, 256, 16) (4096, 16, 1) 7f419f2944a0
constant123 = None  # device(type='cpu') torch.bfloat16 (16, 1024, 16) (16384, 16, 1) 7f419f296fc0
_frozen_param378 = None  # device(type='cpu') torch.bfloat16 (1024,) (1,) 7f419fa88a40
_frozen_param621 = None  # device(type='cpu') torch.bfloat16 (1024, 256) (1, 0) 7f419f98a020
_frozen_param380 = None  # device(type='cpu') torch.bfloat16 (256,) (1,) 7f419fa88a90
_frozen_param622 = None  # device(type='cpu') torch.bfloat16 (256, 1024) (1, 0) 7f419f98a160
_frozen_param382 = None  # device(type='cpu') torch.float32 (1, 256, 1, 1) (256, 1, 1, 1) 7f419fa889a0
_frozen_param383 = None  # device(type='cpu') torch.bfloat16 (256,) (1,) 7f419fed0d10
_frozen_param623 = None  # device(type='cpu') torch.bfloat16 (256, 1, 7, 7) (1, 0, 0, 0) 7f419f98a520
constant131 = None  # device(type='cpu') torch.bfloat16 (64, 256, 16) (4096, 16, 1) 7f419f106200
constant132 = None  # device(type='cpu') torch.bfloat16 (16, 1024, 16) (16384, 16, 1) 7f419f1457b0
_frozen_param385 = None  # device(type='cpu') torch.bfloat16 (1024,) (1,) 7f419fa88ea0
_frozen_param624 = None  # device(type='cpu') torch.bfloat16 (1024, 256) (1, 0) 7f419f98a2a0
_frozen_param387 = None  # device(type='cpu') torch.bfloat16 (256,) (1,) 7f419fa88ef0
_frozen_param625 = None  # device(type='cpu') torch.bfloat16 (256, 1024) (1, 0) 7f419f98a070
_frozen_param389 = None  # device(type='cpu') torch.float32 (1, 256, 1, 1) (256, 1, 1, 1) 7f419fa88e00
constant138 = None  # device(type='cpu') torch.bfloat16 (64, 256, 16) (4096, 16, 1) 7f419f12f290
constant139 = None  # device(type='cpu') torch.bfloat16 (16, 1024, 16) (16384, 16, 1) 7f419f162f20
_frozen_param390 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa89080
_frozen_param626 = None  # device(type='cpu') torch.bfloat16 (512, 256, 2, 2) (1, 0, 0, 0) 7f419f98a6b0
_frozen_param392 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa89350
_frozen_param627 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f98a7a0
_frozen_param394 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa894e0
_frozen_param628 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98a2f0
_frozen_param396 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa89530
_frozen_param629 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98a430
_frozen_param398 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa89440
_frozen_param399 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa896c0
_frozen_param630 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f98a930
constant151 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419f146390
constant152 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419f12d760
_frozen_param401 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa899e0
_frozen_param631 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98a5c0
_frozen_param403 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa89a30
_frozen_param632 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98a700
_frozen_param405 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa89940
_frozen_param406 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa89bc0
_frozen_param633 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f98aac0
constant160 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ef0d440
constant161 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419f1477e0
_frozen_param408 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa89ee0
_frozen_param634 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98a840
_frozen_param410 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa89f30
_frozen_param635 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98a610
_frozen_param412 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa89e40
_frozen_param413 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8a0c0
_frozen_param636 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f98ac50
constant169 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ef2a3e0
constant170 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ef83c90
_frozen_param415 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8a3e0
_frozen_param637 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98a9d0
_frozen_param417 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8a430
_frozen_param638 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98a890
_frozen_param419 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8a340
_frozen_param420 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8a5c0
_frozen_param639 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f98ade0
constant178 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419f12c2c0
constant179 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419f12eed0
_frozen_param422 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8a8e0
_frozen_param640 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98ab60
_frozen_param424 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8a930
_frozen_param641 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98aa20
_frozen_param426 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8a840
_frozen_param427 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8aac0
_frozen_param642 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f98af70
constant187 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ef7cb80
constant188 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ef3b560
_frozen_param429 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8ade0
_frozen_param643 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98acf0
_frozen_param431 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8ae30
_frozen_param644 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98abb0
_frozen_param433 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8ad40
_frozen_param434 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8afc0
_frozen_param645 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f98b100
constant196 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ef3a2a0
constant197 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ef3b060
_frozen_param436 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8b2e0
_frozen_param646 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98ae80
_frozen_param438 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8b330
_frozen_param647 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98ad40
_frozen_param440 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8b240
_frozen_param441 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8b470
_frozen_param648 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f98b290
constant205 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ef82e80
constant206 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ef3ad40
_frozen_param443 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8b790
_frozen_param649 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98b010
_frozen_param445 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8b7e0
_frozen_param650 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98aed0
_frozen_param447 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8b6f0
_frozen_param448 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8b970
_frozen_param651 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f98b420
constant214 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ef0dc10
constant215 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ef7c8b0
_frozen_param450 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8bc90
_frozen_param652 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98b1a0
_frozen_param452 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8bce0
_frozen_param653 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98b060
_frozen_param454 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8bbf0
_frozen_param455 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8be70
_frozen_param654 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f98b5b0
constant223 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ef82980
constant224 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419efd4860
_frozen_param457 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8bfb0
_frozen_param655 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98b330
_frozen_param459 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8c220
_frozen_param656 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98b1f0
_frozen_param461 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8c130
_frozen_param462 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8c3b0
_frozen_param657 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f98b740
constant232 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419eda9f30
constant233 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419eda8130
_frozen_param464 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8c6d0
_frozen_param658 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98b4c0
_frozen_param466 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8c720
_frozen_param659 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98b380
_frozen_param468 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8c630
_frozen_param469 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8c8b0
_frozen_param660 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f98b8d0
constant241 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ef39ee0
constant242 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419efd7ab0
_frozen_param471 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8cbd0
_frozen_param661 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98b650
_frozen_param473 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8cc20
_frozen_param662 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98b510
_frozen_param475 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8cb30
_frozen_param476 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8cdb0
_frozen_param663 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f98ba60
constant250 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ed847c0
constant251 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ef7f240
_frozen_param478 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8d0d0
_frozen_param664 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98b7e0
_frozen_param480 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8d120
_frozen_param665 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98b6a0
_frozen_param482 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8d030
_frozen_param483 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8d2b0
_frozen_param666 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f98bbf0
constant259 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ed86750
constant260 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ee0ad40
_frozen_param485 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8d5d0
_frozen_param667 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98b970
_frozen_param487 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8d620
_frozen_param668 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98b830
_frozen_param489 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8d530
_frozen_param490 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8d7b0
_frozen_param669 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f98bd80
constant268 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ee0a4d0
constant269 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ee0aed0
_frozen_param492 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8dad0
_frozen_param670 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98bb00
_frozen_param494 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8db20
_frozen_param671 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98b9c0
_frozen_param496 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8da30
_frozen_param497 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8dcb0
_frozen_param672 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f98bf10
constant277 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ee0a340
constant278 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419edaa5c0
_frozen_param499 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8dfd0
_frozen_param673 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98bc90
_frozen_param501 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8e020
_frozen_param674 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98bb50
_frozen_param503 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8df30
_frozen_param504 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8e1b0
_frozen_param675 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f9940e0
constant286 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ee1ac50
constant287 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419eda9df0
_frozen_param506 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8e4d0
_frozen_param676 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98bfb0
_frozen_param508 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8e520
_frozen_param677 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f98bce0
_frozen_param510 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8e430
_frozen_param511 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8e6b0
_frozen_param678 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f994270
constant295 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ee19990
constant296 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ecbaac0
_frozen_param513 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8e9d0
_frozen_param679 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f98a480
_frozen_param515 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8ea20
_frozen_param680 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419fafae80
_frozen_param517 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8e930
_frozen_param518 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8ebb0
_frozen_param681 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f994400
constant304 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ecb9d00
constant305 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ee0b380
_frozen_param520 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8eed0
_frozen_param682 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f994180
_frozen_param522 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8ef20
_frozen_param683 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f994130
_frozen_param524 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8ee30
_frozen_param525 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8f0b0
_frozen_param684 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f994590
constant313 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419efd6b10
constant314 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ed86340
_frozen_param527 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8f3d0
_frozen_param685 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f994310
_frozen_param529 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8f420
_frozen_param686 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f9941d0
_frozen_param531 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8f330
_frozen_param532 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8f5b0
_frozen_param687 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f994720
constant322 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ecb9990
constant323 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ecd71f0
_frozen_param534 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8f8d0
_frozen_param688 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f9944a0
_frozen_param536 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8f920
_frozen_param689 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f994360
_frozen_param538 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa8f830
_frozen_param539 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8fab0
_frozen_param690 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f9948b0
constant331 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ed06160
constant332 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ecd4f90
_frozen_param541 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8fdd0
_frozen_param691 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f994630
_frozen_param543 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8fe20
_frozen_param692 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f9944f0
_frozen_param545 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa90090
_frozen_param546 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa8ffb0
_frozen_param693 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f994a40
constant340 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ed63560
constant341 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ecd62f0
_frozen_param548 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa8fd30
_frozen_param694 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f9947c0
_frozen_param550 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa90360
_frozen_param695 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f994680
_frozen_param552 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa90270
_frozen_param553 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa904f0
_frozen_param696 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f994bd0
constant349 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ecbb1a0
constant350 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ecd7510
_frozen_param555 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa90810
_frozen_param697 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f994950
_frozen_param557 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa90860
_frozen_param698 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f994810
_frozen_param559 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa90770
_frozen_param560 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa909f0
_frozen_param699 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f994d60
constant358 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ebb3560
constant359 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ebb0c20
_frozen_param562 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa90d10
_frozen_param700 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f994ae0
_frozen_param564 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa90d60
_frozen_param701 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f9949a0
_frozen_param566 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa90c70
_frozen_param567 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa90ef0
_frozen_param702 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f994ef0
constant367 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ecd4720
constant368 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ebb0d60
_frozen_param569 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa91210
_frozen_param703 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f994c70
_frozen_param571 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa91260
_frozen_param704 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f994b30
_frozen_param573 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa91170
_frozen_param574 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa913f0
_frozen_param705 = None  # device(type='cpu') torch.bfloat16 (512, 1, 7, 7) (1, 0, 0, 0) 7f419f995080
constant376 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ed60680
constant377 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ed856c0
_frozen_param576 = None  # device(type='cpu') torch.bfloat16 (2048,) (1,) 7f419fa91710
_frozen_param706 = None  # device(type='cpu') torch.bfloat16 (2048, 512) (1, 0) 7f419f994e00
_frozen_param578 = None  # device(type='cpu') torch.bfloat16 (512,) (1,) 7f419fa91760
_frozen_param707 = None  # device(type='cpu') torch.bfloat16 (512, 2048) (1, 0) 7f419f994cc0
_frozen_param580 = None  # device(type='cpu') torch.float32 (1, 512, 1, 1) (512, 1, 1, 1) 7f419fa91670
constant383 = None  # device(type='cpu') torch.bfloat16 (64, 512, 32) (16384, 32, 1) 7f419ebe3600
constant384 = None  # device(type='cpu') torch.bfloat16 (16, 2048, 32) (65536, 32, 1) 7f419ebb18a0
_frozen_param581 = None  # device(type='cpu') torch.bfloat16 (1024,) (1,) 7f419fa918f0
_frozen_param708 = None  # device(type='cpu') torch.bfloat16 (1024, 512, 2, 2) (1, 0, 0, 0) 7f419f995210
_frozen_param583 = None  # device(type='cpu') torch.bfloat16 (1024,) (1,) 7f419fa91bc0
_frozen_param709 = None  # device(type='cpu') torch.bfloat16 (1024, 1, 7, 7) (1, 0, 0, 0) 7f419f995300
_frozen_param585 = None  # device(type='cpu') torch.bfloat16 (4096,) (1,) 7f419fa91d50
_frozen_param710 = None  # device(type='cpu') torch.bfloat16 (4096, 1024) (1, 0) 7f419f994e50
_frozen_param587 = None  # device(type='cpu') torch.bfloat16 (1024,) (1,) 7f419fa91da0
_frozen_param711 = None  # device(type='cpu') torch.bfloat16 (1024, 4096) (1, 0) 7f419f994f90
_frozen_param589 = None  # device(type='cpu') torch.float32 (1, 1024, 1, 1) (1024, 1, 1, 1) 7f419fa91cb0
_frozen_param590 = None  # device(type='cpu') torch.bfloat16 (1024,) (1,) 7f419fa91f30
_frozen_param712 = None  # device(type='cpu') torch.bfloat16 (1024, 1, 7, 7) (1, 0, 0, 0) 7f419f995490
constant396 = None  # device(type='cpu') torch.bfloat16 (128, 1024, 32) (32768, 32, 1) 7f419ed05490
constant397 = None  # device(type='cpu') torch.bfloat16 (32, 4096, 32) (131072, 32, 1) 7f419ec4ade0
_frozen_param592 = None  # device(type='cpu') torch.bfloat16 (4096,) (1,) 7f419fa92250
_frozen_param713 = None  # device(type='cpu') torch.bfloat16 (4096, 1024) (1, 0) 7f419f995120
_frozen_param594 = None  # device(type='cpu') torch.bfloat16 (1024,) (1,) 7f419fa922a0
_frozen_param714 = None  # device(type='cpu') torch.bfloat16 (1024, 4096) (1, 0) 7f419f995260
_frozen_param596 = None  # device(type='cpu') torch.float32 (1, 1024, 1, 1) (1024, 1, 1, 1) 7f419fa921b0
_frozen_param597 = None  # device(type='cpu') torch.bfloat16 (1024,) (1,) 7f419fa92430
_frozen_param715 = None  # device(type='cpu') torch.bfloat16 (1024, 1, 7, 7) (1, 0, 0, 0) 7f419f995620
constant405 = None  # device(type='cpu') torch.bfloat16 (128, 1024, 32) (32768, 32, 1) 7f419f3b2ac0
constant406 = None  # device(type='cpu') torch.bfloat16 (32, 4096, 32) (131072, 32, 1) 7f419ebe0a90
_frozen_param599 = None  # device(type='cpu') torch.bfloat16 (4096,) (1,) 7f419fa92750
_frozen_param716 = None  # device(type='cpu') torch.bfloat16 (4096, 1024) (1, 0) 7f419f9953a0
_frozen_param601 = None  # device(type='cpu') torch.bfloat16 (1024,) (1,) 7f419fa927a0
_frozen_param717 = None  # device(type='cpu') torch.bfloat16 (1024, 4096) (1, 0) 7f419f995170
_frozen_param603 = None  # device(type='cpu') torch.float32 (1, 1024, 1, 1) (1024, 1, 1, 1) 7f419fa926b0
constant412 = None  # device(type='cpu') torch.bfloat16 (128, 1024, 32) (32768, 32, 1) 7f419ed06bb0
constant413 = None  # device(type='cpu') torch.bfloat16 (32, 4096, 32) (131072, 32, 1) 7f419ec482c0
_frozen_param604 = None  # device(type='cpu') torch.bfloat16 (1000,) (1,) 7f419fa92930
_frozen_param718 = None  # device(type='cpu') torch.bfloat16 (1000, 1024) (1, 0) 7f419f994fe0
constant416 = None  # device(type='cpu') torch.bfloat16 (1000, 1024, 1) (1024, 1, 1) 7f419ec4b3d0


cpp_fused__to_copy_0 = async_compile.cpp_pybinding(['const float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const float* in_ptr0,
                       bfloat16* out_ptr0)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(3L); x0+=static_cast<long>(1L))
        {
            #pragma GCC ivdep
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(82944L); x1+=static_cast<long>(1L))
            {
                auto tmp0 = in_ptr0[static_cast<long>(x1 + (82944L*x0))];
                auto tmp1 = c10::convert<bfloat16>(tmp0);
                out_ptr0[static_cast<long>(x0 + (3L*x1))] = tmp1;
            }
        }
    }
}
''')


cpp_fused_native_layer_norm_1 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(5184L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(8L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(128L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (128L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(128L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (128L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(128.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (128L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_native_layer_norm_2 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(5184L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(8L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(128L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (128L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(128L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (128L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(128.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (128L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_3 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(128L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(5184L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 162;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 4;
    constexpr int64_t Mc_blocks = 162;
    constexpr int64_t Kc_blocks = 4;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (128L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (4096L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(128L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (128L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (4096L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(128L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_4 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(128L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(5184L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 162;
    constexpr int64_t Nt_blocks = 4;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 162;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((128L*(static_cast<long>((n_start + x1 + x1_inner + (128L*m_start) + (128L*x0))) % static_cast<long>(72L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (128L*m_start) + (128L*x0)), 5184L)) % static_cast<long>(128L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp10 = tmp7 + tmp9;
                            auto tmp11 = at::vec::convert<bfloat16>(tmp10);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (128L*m_start) + (128L*x0)), 16);
                            tmp11.store(Y + static_cast<long>(n_start + x1 + (128L*m_start) + (128L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((128L*(static_cast<long>((n_start + x1 + (128L*m_start) + (128L*x0))) % static_cast<long>(72L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (128L*m_start) + (128L*x0)), 5184L)) % static_cast<long>(128L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp9 = decltype(tmp6)(tmp6 + tmp8);
                            auto tmp10 = c10::convert<bfloat16>(tmp9);
                            Y[static_cast<long>(n_start + x1 + (128L*m_start) + (128L*x0))] = tmp4;
                            Y[static_cast<long>(n_start + x1 + (128L*m_start) + (128L*x0))] = tmp10;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_5 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(5184L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(8L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(128L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (128L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(128L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (128L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(128.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (128L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_6 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(128L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(5184L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 162;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 4;
    constexpr int64_t Mc_blocks = 162;
    constexpr int64_t Kc_blocks = 4;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (128L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (4096L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(128L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (128L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (4096L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(128L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_7 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'bfloat16*', 'float*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, const float* in_ptr5, const bfloat16* in_ptr6, bfloat16* Y, float* out_ptr3)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(128L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(5184L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 162;
    constexpr int64_t Nt_blocks = 4;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 162;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((128L*(static_cast<long>((n_start + x1 + x1_inner + (128L*m_start) + (128L*x0))) % static_cast<long>(72L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (128L*m_start) + (128L*x0)), 5184L)) % static_cast<long>(128L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp10 = in_ptr5[static_cast<long>(0L)];
                            auto tmp13 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr6[static_cast<long>((128L*(static_cast<long>((n_start + x1 + x1_inner + (128L*m_start) + (128L*x0))) % static_cast<long>(72L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (128L*m_start) + (128L*x0)), 5184L)) % static_cast<long>(128L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp11 = at::vec::Vectorized<float>(tmp10);
                            auto tmp12 = tmp9 * tmp11;
                            auto tmp14 = at::vec::convert<float>(tmp13);
                            auto tmp15 = tmp12 + tmp14;
                            auto tmp16 = tmp7 + tmp15;
                            auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                            tmp16.store(out_ptr3 + static_cast<long>(n_start + x1 + (128L*m_start) + (128L*x0)));
                            tmp17.store(Y + static_cast<long>(n_start + x1 + (128L*m_start) + (128L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((128L*(static_cast<long>((n_start + x1 + (128L*m_start) + (128L*x0))) % static_cast<long>(72L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (128L*m_start) + (128L*x0)), 5184L)) % static_cast<long>(128L))))];
                            auto tmp9 = in_ptr5[static_cast<long>(0L)];
                            auto tmp11 = in_ptr6[static_cast<long>((128L*(static_cast<long>((n_start + x1 + (128L*m_start) + (128L*x0))) % static_cast<long>(72L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (128L*m_start) + (128L*x0)), 5184L)) % static_cast<long>(128L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp10 = decltype(tmp8)(tmp8 * tmp9);
                            auto tmp12 = c10::convert<float>(tmp11);
                            auto tmp13 = decltype(tmp10)(tmp10 + tmp12);
                            auto tmp14 = decltype(tmp6)(tmp6 + tmp13);
                            auto tmp15 = c10::convert<bfloat16>(tmp14);
                            out_ptr3[static_cast<long>(n_start + x1 + (128L*m_start) + (128L*x0))] = tmp14;
                            Y[static_cast<long>(n_start + x1 + (128L*m_start) + (128L*x0))] = tmp15;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_8 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(5184L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(8L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(128L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (128L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(128L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (128L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(128.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (128L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_9 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(128L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(5184L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 162;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 4;
    constexpr int64_t Mc_blocks = 162;
    constexpr int64_t Kc_blocks = 4;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (128L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (4096L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(128L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (128L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (4096L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(128L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_view_10 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(128L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(5184L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 162;
    constexpr int64_t Nt_blocks = 4;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 162;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (128L*m_start) + (128L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            Y[static_cast<long>(n_start + x1 + (128L*m_start) + (128L*x0))] = tmp4;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_native_layer_norm_11 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       const float* in_ptr3,
                       const float* in_ptr4,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(5184L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(8L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(128L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (128L*x0)), 16);
                    auto tmp2 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                    auto tmp4 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1 + (128L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    auto tmp3 = tmp1 * tmp2;
                    auto tmp5 = tmp3 + tmp4;
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp5, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(128L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (128L*x0)), 16);
                auto tmp2 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp4 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1 + (128L*x0)), 16);
                auto tmp6 = out_ptr0[static_cast<long>(x0)];
                auto tmp9 = out_ptr1[static_cast<long>(x0)];
                auto tmp17 = at::vec::Vectorized<float>::loadu(in_ptr3 + static_cast<long>(x1), 16);
                auto tmp19 = at::vec::Vectorized<float>::loadu(in_ptr4 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = tmp1 * tmp2;
                auto tmp5 = tmp3 + tmp4;
                auto tmp7 = at::vec::Vectorized<float>(tmp6);
                auto tmp8 = tmp5 - tmp7;
                auto tmp10 = static_cast<float>(128.0);
                auto tmp11 = tmp9 / tmp10;
                auto tmp12 = static_cast<float>(1e-06);
                auto tmp13 = decltype(tmp11)(tmp11 + tmp12);
                auto tmp14 = 1 / std::sqrt(tmp13);
                auto tmp15 = at::vec::Vectorized<float>(tmp14);
                auto tmp16 = tmp8 * tmp15;
                auto tmp18 = tmp16 * tmp17;
                auto tmp20 = tmp18 + tmp19;
                auto tmp21 = at::vec::convert<bfloat16>(tmp20);
                tmp21.store(out_ptr2 + static_cast<long>(x1 + (128L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_native_layer_norm_12 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(1296L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(16L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(256L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (256L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(256L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (256L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(256.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (256L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_13 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_48_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 48 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 48 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(2, C + 32 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(3, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 3, 6);
        _tile_stream_loadd(4, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 6);
        _tile_stream_loadd(5, A + 32 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(2, C + 32 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 48 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 16 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(4, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 4);
        _tile_stream_loadd(3, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(1, 3, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 16 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(1, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(2, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 1, 2);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 16 == 0, "N dimension must be multiple of 16");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 48) {
        int64_t block_m = std::min<int64_t>(M - m, 48);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 16) {
            if (block_m >= 48) {
                kernel_micro_gemm_amx_kernel_48_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 48;
                m_tail += 48;
            }
            else
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_1<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(1024L);
    constexpr int64_t K = static_cast<long>(256L);
    constexpr int64_t M0 = 48;
    constexpr int64_t N0 = 16;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(1296L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 27;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 8;
    constexpr int64_t Mc_blocks = 27;
    constexpr int64_t Kc_blocks = 8;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (256L*m_start))]),
                            &(W[static_cast<long>((16L*k_start) + (4096L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(256L),
                            static_cast<long>(16L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (256L*m_start))]),
                            &(W[static_cast<long>((16L*k_start) + (4096L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(256L),
                            static_cast<long>(16L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (1024L*m_start) + (1024L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (1024L*m_start) + (1024L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_14 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_48_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 48 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 48 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(2, C + 32 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(3, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 3, 6);
        _tile_stream_loadd(4, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 6);
        _tile_stream_loadd(5, A + 32 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(2, C + 32 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 48 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 16 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(4, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 4);
        _tile_stream_loadd(3, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(1, 3, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 16 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(1, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(2, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 1, 2);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 16 == 0, "N dimension must be multiple of 16");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 48) {
        int64_t block_m = std::min<int64_t>(M - m, 48);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 16) {
            if (block_m >= 48) {
                kernel_micro_gemm_amx_kernel_48_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 48;
                m_tail += 48;
            }
            else
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_1<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(256L);
    constexpr int64_t K = static_cast<long>(1024L);
    constexpr int64_t M0 = 48;
    constexpr int64_t N0 = 16;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(1296L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 27;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 32;
    constexpr int64_t Mc_blocks = 27;
    constexpr int64_t Kc_blocks = 32;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (1024L*m_start))]),
                            &(W[static_cast<long>((16L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(1024L),
                            static_cast<long>(16L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (1024L*m_start))]),
                            &(W[static_cast<long>((16L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(1024L),
                            static_cast<long>(16L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((256L*(static_cast<long>((n_start + x1 + x1_inner + (256L*m_start) + (256L*x0))) % static_cast<long>(36L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (256L*m_start) + (256L*x0)), 1296L)) % static_cast<long>(256L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp10 = tmp7 + tmp9;
                            auto tmp11 = at::vec::convert<bfloat16>(tmp10);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (256L*m_start) + (256L*x0)), 16);
                            tmp11.store(Y + static_cast<long>(n_start + x1 + (256L*m_start) + (256L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((256L*(static_cast<long>((n_start + x1 + (256L*m_start) + (256L*x0))) % static_cast<long>(36L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (256L*m_start) + (256L*x0)), 1296L)) % static_cast<long>(256L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp9 = decltype(tmp6)(tmp6 + tmp8);
                            auto tmp10 = c10::convert<bfloat16>(tmp9);
                            Y[static_cast<long>(n_start + x1 + (256L*m_start) + (256L*x0))] = tmp4;
                            Y[static_cast<long>(n_start + x1 + (256L*m_start) + (256L*x0))] = tmp10;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_15 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(1296L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(16L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(256L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (256L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(256L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (256L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(256.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (256L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_16 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_48_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 48 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 48 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(2, C + 32 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(3, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 3, 6);
        _tile_stream_loadd(4, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 6);
        _tile_stream_loadd(5, A + 32 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(2, C + 32 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 48 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 16 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(4, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 4);
        _tile_stream_loadd(3, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(1, 3, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 16 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(1, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(2, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 1, 2);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 16 == 0, "N dimension must be multiple of 16");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 48) {
        int64_t block_m = std::min<int64_t>(M - m, 48);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 16) {
            if (block_m >= 48) {
                kernel_micro_gemm_amx_kernel_48_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 48;
                m_tail += 48;
            }
            else
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_1<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(1024L);
    constexpr int64_t K = static_cast<long>(256L);
    constexpr int64_t M0 = 48;
    constexpr int64_t N0 = 16;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(1296L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 27;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 8;
    constexpr int64_t Mc_blocks = 27;
    constexpr int64_t Kc_blocks = 8;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (256L*m_start))]),
                            &(W[static_cast<long>((16L*k_start) + (4096L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(256L),
                            static_cast<long>(16L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (256L*m_start))]),
                            &(W[static_cast<long>((16L*k_start) + (4096L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(256L),
                            static_cast<long>(16L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (1024L*m_start) + (1024L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (1024L*m_start) + (1024L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_17 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'bfloat16*', 'float*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_48_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 48 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 48 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(2, C + 32 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(3, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 3, 6);
        _tile_stream_loadd(4, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 6);
        _tile_stream_loadd(5, A + 32 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(2, C + 32 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 48 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 16 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(4, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 4);
        _tile_stream_loadd(3, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(1, 3, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 16 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(1, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(2, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 1, 2);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 16 == 0, "N dimension must be multiple of 16");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 48) {
        int64_t block_m = std::min<int64_t>(M - m, 48);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 16) {
            if (block_m >= 48) {
                kernel_micro_gemm_amx_kernel_48_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 48;
                m_tail += 48;
            }
            else
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_1<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, const float* in_ptr5, const bfloat16* in_ptr6, bfloat16* Y, float* out_ptr3)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(256L);
    constexpr int64_t K = static_cast<long>(1024L);
    constexpr int64_t M0 = 48;
    constexpr int64_t N0 = 16;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(1296L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 27;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 32;
    constexpr int64_t Mc_blocks = 27;
    constexpr int64_t Kc_blocks = 32;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (1024L*m_start))]),
                            &(W[static_cast<long>((16L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(1024L),
                            static_cast<long>(16L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (1024L*m_start))]),
                            &(W[static_cast<long>((16L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(1024L),
                            static_cast<long>(16L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((256L*(static_cast<long>((n_start + x1 + x1_inner + (256L*m_start) + (256L*x0))) % static_cast<long>(36L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (256L*m_start) + (256L*x0)), 1296L)) % static_cast<long>(256L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp10 = in_ptr5[static_cast<long>(0L)];
                            auto tmp13 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr6[static_cast<long>((256L*(static_cast<long>((n_start + x1 + x1_inner + (256L*m_start) + (256L*x0))) % static_cast<long>(36L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (256L*m_start) + (256L*x0)), 1296L)) % static_cast<long>(256L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp11 = at::vec::Vectorized<float>(tmp10);
                            auto tmp12 = tmp9 * tmp11;
                            auto tmp14 = at::vec::convert<float>(tmp13);
                            auto tmp15 = tmp12 + tmp14;
                            auto tmp16 = tmp7 + tmp15;
                            auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                            tmp16.store(out_ptr3 + static_cast<long>(n_start + x1 + (256L*m_start) + (256L*x0)));
                            tmp17.store(Y + static_cast<long>(n_start + x1 + (256L*m_start) + (256L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((256L*(static_cast<long>((n_start + x1 + (256L*m_start) + (256L*x0))) % static_cast<long>(36L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (256L*m_start) + (256L*x0)), 1296L)) % static_cast<long>(256L))))];
                            auto tmp9 = in_ptr5[static_cast<long>(0L)];
                            auto tmp11 = in_ptr6[static_cast<long>((256L*(static_cast<long>((n_start + x1 + (256L*m_start) + (256L*x0))) % static_cast<long>(36L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (256L*m_start) + (256L*x0)), 1296L)) % static_cast<long>(256L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp10 = decltype(tmp8)(tmp8 * tmp9);
                            auto tmp12 = c10::convert<float>(tmp11);
                            auto tmp13 = decltype(tmp10)(tmp10 + tmp12);
                            auto tmp14 = decltype(tmp6)(tmp6 + tmp13);
                            auto tmp15 = c10::convert<bfloat16>(tmp14);
                            out_ptr3[static_cast<long>(n_start + x1 + (256L*m_start) + (256L*x0))] = tmp14;
                            Y[static_cast<long>(n_start + x1 + (256L*m_start) + (256L*x0))] = tmp15;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_18 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(1296L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(16L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(256L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (256L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(256L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (256L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(256.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (256L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_19 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_48_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 48 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 48 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(2, C + 32 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(3, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 3, 6);
        _tile_stream_loadd(4, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 6);
        _tile_stream_loadd(5, A + 32 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(2, C + 32 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 48 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 16 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(4, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 4);
        _tile_stream_loadd(3, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(1, 3, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 16 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(1, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(2, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 1, 2);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 16 == 0, "N dimension must be multiple of 16");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 48) {
        int64_t block_m = std::min<int64_t>(M - m, 48);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 16) {
            if (block_m >= 48) {
                kernel_micro_gemm_amx_kernel_48_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 48;
                m_tail += 48;
            }
            else
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_1<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(1024L);
    constexpr int64_t K = static_cast<long>(256L);
    constexpr int64_t M0 = 48;
    constexpr int64_t N0 = 16;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(1296L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 27;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 8;
    constexpr int64_t Mc_blocks = 27;
    constexpr int64_t Kc_blocks = 8;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (256L*m_start))]),
                            &(W[static_cast<long>((16L*k_start) + (4096L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(256L),
                            static_cast<long>(16L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (256L*m_start))]),
                            &(W[static_cast<long>((16L*k_start) + (4096L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(256L),
                            static_cast<long>(16L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (1024L*m_start) + (1024L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (1024L*m_start) + (1024L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_view_20 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_48_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 48 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 48 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(2, C + 32 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(3, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 3, 6);
        _tile_stream_loadd(4, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 6);
        _tile_stream_loadd(5, A + 32 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(2, C + 32 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 48 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 16 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(4, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 4);
        _tile_stream_loadd(3, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(1, 3, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 16 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_1(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 1, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 1, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(1, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(2, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 1, 2);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 1, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 16 == 0, "N dimension must be multiple of 16");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 48) {
        int64_t block_m = std::min<int64_t>(M - m, 48);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 16) {
            if (block_m >= 48) {
                kernel_micro_gemm_amx_kernel_48_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 48;
                m_tail += 48;
            }
            else
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_1<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_1<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(256L);
    constexpr int64_t K = static_cast<long>(1024L);
    constexpr int64_t M0 = 48;
    constexpr int64_t N0 = 16;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(1296L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 27;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 32;
    constexpr int64_t Mc_blocks = 27;
    constexpr int64_t Kc_blocks = 32;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (1024L*m_start))]),
                            &(W[static_cast<long>((16L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(1024L),
                            static_cast<long>(16L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (1024L*m_start))]),
                            &(W[static_cast<long>((16L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(1024L),
                            static_cast<long>(16L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (256L*m_start) + (256L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            Y[static_cast<long>(n_start + x1 + (256L*m_start) + (256L*x0))] = tmp4;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_native_layer_norm_21 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       const float* in_ptr3,
                       const float* in_ptr4,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(1296L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(16L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(256L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (256L*x0)), 16);
                    auto tmp2 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                    auto tmp4 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1 + (256L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    auto tmp3 = tmp1 * tmp2;
                    auto tmp5 = tmp3 + tmp4;
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp5, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(256L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (256L*x0)), 16);
                auto tmp2 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp4 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1 + (256L*x0)), 16);
                auto tmp6 = out_ptr0[static_cast<long>(x0)];
                auto tmp9 = out_ptr1[static_cast<long>(x0)];
                auto tmp17 = at::vec::Vectorized<float>::loadu(in_ptr3 + static_cast<long>(x1), 16);
                auto tmp19 = at::vec::Vectorized<float>::loadu(in_ptr4 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = tmp1 * tmp2;
                auto tmp5 = tmp3 + tmp4;
                auto tmp7 = at::vec::Vectorized<float>(tmp6);
                auto tmp8 = tmp5 - tmp7;
                auto tmp10 = static_cast<float>(256.0);
                auto tmp11 = tmp9 / tmp10;
                auto tmp12 = static_cast<float>(1e-06);
                auto tmp13 = decltype(tmp11)(tmp11 + tmp12);
                auto tmp14 = 1 / std::sqrt(tmp13);
                auto tmp15 = at::vec::Vectorized<float>(tmp14);
                auto tmp16 = tmp8 * tmp15;
                auto tmp18 = tmp16 * tmp17;
                auto tmp20 = tmp18 + tmp19;
                auto tmp21 = at::vec::convert<bfloat16>(tmp20);
                tmp21.store(out_ptr2 + static_cast<long>(x1 + (256L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_native_layer_norm_22 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_23 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_24 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp10 = tmp7 + tmp9;
                            auto tmp11 = at::vec::convert<bfloat16>(tmp10);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                            tmp11.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp9 = decltype(tmp6)(tmp6 + tmp8);
                            auto tmp10 = c10::convert<bfloat16>(tmp9);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp4;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp10;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_25 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_26 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_27 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'bfloat16*', 'float*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, const float* in_ptr5, const bfloat16* in_ptr6, bfloat16* Y, float* out_ptr3)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp10 = in_ptr5[static_cast<long>(0L)];
                            auto tmp13 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp11 = at::vec::Vectorized<float>(tmp10);
                            auto tmp12 = tmp9 * tmp11;
                            auto tmp14 = at::vec::convert<float>(tmp13);
                            auto tmp15 = tmp12 + tmp14;
                            auto tmp16 = tmp7 + tmp15;
                            auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                            tmp16.store(out_ptr3 + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)));
                            tmp17.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp9 = in_ptr5[static_cast<long>(0L)];
                            auto tmp11 = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp10 = decltype(tmp8)(tmp8 * tmp9);
                            auto tmp12 = c10::convert<float>(tmp11);
                            auto tmp13 = decltype(tmp10)(tmp10 + tmp12);
                            auto tmp14 = decltype(tmp6)(tmp6 + tmp13);
                            auto tmp15 = c10::convert<bfloat16>(tmp14);
                            out_ptr3[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp14;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp15;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_28 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_29 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_30 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const float* in_ptr4, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = tmp7 + tmp8;
                            auto tmp10 = at::vec::convert<bfloat16>(tmp9);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                            tmp10.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = decltype(tmp6)(tmp6 + tmp7);
                            auto tmp9 = c10::convert<bfloat16>(tmp8);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp4;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp9;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_31 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_32 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_33 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*', 'float*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, const float* in_ptr5, const float* in_ptr6, bfloat16* Y, float* out_ptr3)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp10 = in_ptr5[static_cast<long>(0L)];
                            auto tmp13 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp11 = at::vec::Vectorized<float>(tmp10);
                            auto tmp12 = tmp9 * tmp11;
                            auto tmp14 = tmp12 + tmp13;
                            auto tmp15 = tmp7 + tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp15.store(out_ptr3 + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)));
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp9 = in_ptr5[static_cast<long>(0L)];
                            auto tmp11 = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp10 = decltype(tmp8)(tmp8 * tmp9);
                            auto tmp12 = decltype(tmp10)(tmp10 + tmp11);
                            auto tmp13 = decltype(tmp6)(tmp6 + tmp12);
                            auto tmp14 = c10::convert<bfloat16>(tmp13);
                            out_ptr3[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp13;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp14;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_34 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_35 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_36 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const float* in_ptr4, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = tmp7 + tmp8;
                            auto tmp10 = at::vec::convert<bfloat16>(tmp9);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                            tmp10.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = decltype(tmp6)(tmp6 + tmp7);
                            auto tmp9 = c10::convert<bfloat16>(tmp8);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp4;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp9;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_37 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_38 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_39 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*', 'float*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, const float* in_ptr5, const float* in_ptr6, bfloat16* Y, float* out_ptr3)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp10 = in_ptr5[static_cast<long>(0L)];
                            auto tmp13 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp11 = at::vec::Vectorized<float>(tmp10);
                            auto tmp12 = tmp9 * tmp11;
                            auto tmp14 = tmp12 + tmp13;
                            auto tmp15 = tmp7 + tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp15.store(out_ptr3 + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)));
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp9 = in_ptr5[static_cast<long>(0L)];
                            auto tmp11 = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp10 = decltype(tmp8)(tmp8 * tmp9);
                            auto tmp12 = decltype(tmp10)(tmp10 + tmp11);
                            auto tmp13 = decltype(tmp6)(tmp6 + tmp12);
                            auto tmp14 = c10::convert<bfloat16>(tmp13);
                            out_ptr3[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp13;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp14;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_40 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_41 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_42 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const float* in_ptr4, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = tmp7 + tmp8;
                            auto tmp10 = at::vec::convert<bfloat16>(tmp9);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                            tmp10.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = decltype(tmp6)(tmp6 + tmp7);
                            auto tmp9 = c10::convert<bfloat16>(tmp8);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp4;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp9;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_43 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_44 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_45 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*', 'float*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, const float* in_ptr5, const float* in_ptr6, bfloat16* Y, float* out_ptr3)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp10 = in_ptr5[static_cast<long>(0L)];
                            auto tmp13 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp11 = at::vec::Vectorized<float>(tmp10);
                            auto tmp12 = tmp9 * tmp11;
                            auto tmp14 = tmp12 + tmp13;
                            auto tmp15 = tmp7 + tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp15.store(out_ptr3 + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)));
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp9 = in_ptr5[static_cast<long>(0L)];
                            auto tmp11 = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp10 = decltype(tmp8)(tmp8 * tmp9);
                            auto tmp12 = decltype(tmp10)(tmp10 + tmp11);
                            auto tmp13 = decltype(tmp6)(tmp6 + tmp12);
                            auto tmp14 = c10::convert<bfloat16>(tmp13);
                            out_ptr3[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp13;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp14;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_46 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_47 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_48 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const float* in_ptr4, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = tmp7 + tmp8;
                            auto tmp10 = at::vec::convert<bfloat16>(tmp9);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                            tmp10.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = decltype(tmp6)(tmp6 + tmp7);
                            auto tmp9 = c10::convert<bfloat16>(tmp8);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp4;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp9;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_49 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_50 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_51 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*', 'float*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, const float* in_ptr5, const float* in_ptr6, bfloat16* Y, float* out_ptr3)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp10 = in_ptr5[static_cast<long>(0L)];
                            auto tmp13 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp11 = at::vec::Vectorized<float>(tmp10);
                            auto tmp12 = tmp9 * tmp11;
                            auto tmp14 = tmp12 + tmp13;
                            auto tmp15 = tmp7 + tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp15.store(out_ptr3 + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)));
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp9 = in_ptr5[static_cast<long>(0L)];
                            auto tmp11 = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp10 = decltype(tmp8)(tmp8 * tmp9);
                            auto tmp12 = decltype(tmp10)(tmp10 + tmp11);
                            auto tmp13 = decltype(tmp6)(tmp6 + tmp12);
                            auto tmp14 = c10::convert<bfloat16>(tmp13);
                            out_ptr3[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp13;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp14;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_52 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_53 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_54 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const float* in_ptr4, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = tmp7 + tmp8;
                            auto tmp10 = at::vec::convert<bfloat16>(tmp9);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                            tmp10.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = decltype(tmp6)(tmp6 + tmp7);
                            auto tmp9 = c10::convert<bfloat16>(tmp8);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp4;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp9;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_55 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_56 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_57 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*', 'float*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, const float* in_ptr5, const float* in_ptr6, bfloat16* Y, float* out_ptr3)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp10 = in_ptr5[static_cast<long>(0L)];
                            auto tmp13 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp11 = at::vec::Vectorized<float>(tmp10);
                            auto tmp12 = tmp9 * tmp11;
                            auto tmp14 = tmp12 + tmp13;
                            auto tmp15 = tmp7 + tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp15.store(out_ptr3 + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)));
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp9 = in_ptr5[static_cast<long>(0L)];
                            auto tmp11 = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp10 = decltype(tmp8)(tmp8 * tmp9);
                            auto tmp12 = decltype(tmp10)(tmp10 + tmp11);
                            auto tmp13 = decltype(tmp6)(tmp6 + tmp12);
                            auto tmp14 = c10::convert<bfloat16>(tmp13);
                            out_ptr3[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp13;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp14;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_58 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_59 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_60 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const float* in_ptr4, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = tmp7 + tmp8;
                            auto tmp10 = at::vec::convert<bfloat16>(tmp9);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                            tmp10.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = decltype(tmp6)(tmp6 + tmp7);
                            auto tmp9 = c10::convert<bfloat16>(tmp8);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp4;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp9;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_61 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_62 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_63 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*', 'float*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, const float* in_ptr5, const float* in_ptr6, bfloat16* Y, float* out_ptr3)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp10 = in_ptr5[static_cast<long>(0L)];
                            auto tmp13 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp11 = at::vec::Vectorized<float>(tmp10);
                            auto tmp12 = tmp9 * tmp11;
                            auto tmp14 = tmp12 + tmp13;
                            auto tmp15 = tmp7 + tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp15.store(out_ptr3 + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)));
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp9 = in_ptr5[static_cast<long>(0L)];
                            auto tmp11 = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp10 = decltype(tmp8)(tmp8 * tmp9);
                            auto tmp12 = decltype(tmp10)(tmp10 + tmp11);
                            auto tmp13 = decltype(tmp6)(tmp6 + tmp12);
                            auto tmp14 = c10::convert<bfloat16>(tmp13);
                            out_ptr3[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp13;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp14;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_64 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_65 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_66 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const float* in_ptr4, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = tmp7 + tmp8;
                            auto tmp10 = at::vec::convert<bfloat16>(tmp9);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                            tmp10.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = decltype(tmp6)(tmp6 + tmp7);
                            auto tmp9 = c10::convert<bfloat16>(tmp8);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp4;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp9;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_67 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_68 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_69 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*', 'float*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, const float* in_ptr5, const float* in_ptr6, bfloat16* Y, float* out_ptr3)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp10 = in_ptr5[static_cast<long>(0L)];
                            auto tmp13 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp11 = at::vec::Vectorized<float>(tmp10);
                            auto tmp12 = tmp9 * tmp11;
                            auto tmp14 = tmp12 + tmp13;
                            auto tmp15 = tmp7 + tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp15.store(out_ptr3 + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)));
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp9 = in_ptr5[static_cast<long>(0L)];
                            auto tmp11 = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp10 = decltype(tmp8)(tmp8 * tmp9);
                            auto tmp12 = decltype(tmp10)(tmp10 + tmp11);
                            auto tmp13 = decltype(tmp6)(tmp6 + tmp12);
                            auto tmp14 = c10::convert<bfloat16>(tmp13);
                            out_ptr3[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp13;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp14;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_70 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_71 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_72 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const float* in_ptr4, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = tmp7 + tmp8;
                            auto tmp10 = at::vec::convert<bfloat16>(tmp9);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                            tmp10.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = decltype(tmp6)(tmp6 + tmp7);
                            auto tmp9 = c10::convert<bfloat16>(tmp8);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp4;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp9;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_73 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_74 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_75 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*', 'float*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, const float* in_ptr5, const float* in_ptr6, bfloat16* Y, float* out_ptr3)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp10 = in_ptr5[static_cast<long>(0L)];
                            auto tmp13 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp11 = at::vec::Vectorized<float>(tmp10);
                            auto tmp12 = tmp9 * tmp11;
                            auto tmp14 = tmp12 + tmp13;
                            auto tmp15 = tmp7 + tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp15.store(out_ptr3 + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)));
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp9 = in_ptr5[static_cast<long>(0L)];
                            auto tmp11 = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp10 = decltype(tmp8)(tmp8 * tmp9);
                            auto tmp12 = decltype(tmp10)(tmp10 + tmp11);
                            auto tmp13 = decltype(tmp6)(tmp6 + tmp12);
                            auto tmp14 = c10::convert<bfloat16>(tmp13);
                            out_ptr3[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp13;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp14;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_76 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_77 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_78 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const float* in_ptr4, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = tmp7 + tmp8;
                            auto tmp10 = at::vec::convert<bfloat16>(tmp9);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                            tmp10.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = decltype(tmp6)(tmp6 + tmp7);
                            auto tmp9 = c10::convert<bfloat16>(tmp8);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp4;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp9;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_79 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_80 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_81 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*', 'float*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, const float* in_ptr5, const float* in_ptr6, bfloat16* Y, float* out_ptr3)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp10 = in_ptr5[static_cast<long>(0L)];
                            auto tmp13 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp11 = at::vec::Vectorized<float>(tmp10);
                            auto tmp12 = tmp9 * tmp11;
                            auto tmp14 = tmp12 + tmp13;
                            auto tmp15 = tmp7 + tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp15.store(out_ptr3 + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)));
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp9 = in_ptr5[static_cast<long>(0L)];
                            auto tmp11 = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp10 = decltype(tmp8)(tmp8 * tmp9);
                            auto tmp12 = decltype(tmp10)(tmp10 + tmp11);
                            auto tmp13 = decltype(tmp6)(tmp6 + tmp12);
                            auto tmp14 = c10::convert<bfloat16>(tmp13);
                            out_ptr3[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp13;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp14;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_82 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_83 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_84 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const float* in_ptr4, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = tmp7 + tmp8;
                            auto tmp10 = at::vec::convert<bfloat16>(tmp9);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                            tmp10.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = decltype(tmp6)(tmp6 + tmp7);
                            auto tmp9 = c10::convert<bfloat16>(tmp8);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp4;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp9;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_85 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_86 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_87 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*', 'float*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, const float* in_ptr5, const float* in_ptr6, bfloat16* Y, float* out_ptr3)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp10 = in_ptr5[static_cast<long>(0L)];
                            auto tmp13 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp11 = at::vec::Vectorized<float>(tmp10);
                            auto tmp12 = tmp9 * tmp11;
                            auto tmp14 = tmp12 + tmp13;
                            auto tmp15 = tmp7 + tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp15.store(out_ptr3 + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)));
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp9 = in_ptr5[static_cast<long>(0L)];
                            auto tmp11 = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp10 = decltype(tmp8)(tmp8 * tmp9);
                            auto tmp12 = decltype(tmp10)(tmp10 + tmp11);
                            auto tmp13 = decltype(tmp6)(tmp6 + tmp12);
                            auto tmp14 = c10::convert<bfloat16>(tmp13);
                            out_ptr3[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp13;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp14;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_88 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_89 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_90 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const float* in_ptr4, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = tmp7 + tmp8;
                            auto tmp10 = at::vec::convert<bfloat16>(tmp9);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                            tmp10.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = decltype(tmp6)(tmp6 + tmp7);
                            auto tmp9 = c10::convert<bfloat16>(tmp8);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp4;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp9;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_91 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_92 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_93 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*', 'float*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, const float* in_ptr5, const float* in_ptr6, bfloat16* Y, float* out_ptr3)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp10 = in_ptr5[static_cast<long>(0L)];
                            auto tmp13 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp11 = at::vec::Vectorized<float>(tmp10);
                            auto tmp12 = tmp9 * tmp11;
                            auto tmp14 = tmp12 + tmp13;
                            auto tmp15 = tmp7 + tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp15.store(out_ptr3 + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)));
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp9 = in_ptr5[static_cast<long>(0L)];
                            auto tmp11 = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp10 = decltype(tmp8)(tmp8 * tmp9);
                            auto tmp12 = decltype(tmp10)(tmp10 + tmp11);
                            auto tmp13 = decltype(tmp6)(tmp6 + tmp12);
                            auto tmp14 = c10::convert<bfloat16>(tmp13);
                            out_ptr3[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp13;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp14;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_94 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_95 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_96 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const float* in_ptr4, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = tmp7 + tmp8;
                            auto tmp10 = at::vec::convert<bfloat16>(tmp9);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                            tmp10.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = decltype(tmp6)(tmp6 + tmp7);
                            auto tmp9 = c10::convert<bfloat16>(tmp8);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp4;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp9;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_97 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_98 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_99 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'const float*', 'const float*', 'bfloat16*', 'float*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, const float* in_ptr5, const float* in_ptr6, bfloat16* Y, float* out_ptr3)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp10 = in_ptr5[static_cast<long>(0L)];
                            auto tmp13 =
                            [&]
                            {
                                __at_align__ std::array<float, 16> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                                }
                                return at::vec::Vectorized<float>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp11 = at::vec::Vectorized<float>(tmp10);
                            auto tmp12 = tmp9 * tmp11;
                            auto tmp14 = tmp12 + tmp13;
                            auto tmp15 = tmp7 + tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp15.store(out_ptr3 + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)));
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp9 = in_ptr5[static_cast<long>(0L)];
                            auto tmp11 = in_ptr6[static_cast<long>((512L*(static_cast<long>((n_start + x1 + (512L*m_start) + (512L*x0))) % static_cast<long>(18L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (512L*m_start) + (512L*x0)), 324L)) % static_cast<long>(512L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp10 = decltype(tmp8)(tmp8 * tmp9);
                            auto tmp12 = decltype(tmp10)(tmp10 + tmp11);
                            auto tmp13 = decltype(tmp6)(tmp6 + tmp12);
                            auto tmp14 = c10::convert<bfloat16>(tmp13);
                            out_ptr3[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp13;
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp14;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_100 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(512.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_101 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(2048L);
    constexpr int64_t K = static_cast<long>(512L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 64;
    constexpr int64_t Kt_blocks = 16;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 16;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (512L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (16384L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(512L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (2048L*m_start) + (2048L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_view_102 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(512L);
    constexpr int64_t K = static_cast<long>(2048L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(324L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 11;
    constexpr int64_t Nt_blocks = 16;
    constexpr int64_t Kt_blocks = 64;
    constexpr int64_t Mc_blocks = 11;
    constexpr int64_t Kc_blocks = 64;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (2048L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (65536L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(2048L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            Y[static_cast<long>(n_start + x1 + (512L*m_start) + (512L*x0))] = tmp4;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_native_layer_norm_103 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       const float* in_ptr3,
                       const float* in_ptr4,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(324L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(32L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp2 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                    auto tmp4 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    auto tmp3 = tmp1 * tmp2;
                    auto tmp5 = tmp3 + tmp4;
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp5, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(512L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp2 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp4 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
                auto tmp6 = out_ptr0[static_cast<long>(x0)];
                auto tmp9 = out_ptr1[static_cast<long>(x0)];
                auto tmp17 = at::vec::Vectorized<float>::loadu(in_ptr3 + static_cast<long>(x1), 16);
                auto tmp19 = at::vec::Vectorized<float>::loadu(in_ptr4 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = tmp1 * tmp2;
                auto tmp5 = tmp3 + tmp4;
                auto tmp7 = at::vec::Vectorized<float>(tmp6);
                auto tmp8 = tmp5 - tmp7;
                auto tmp10 = static_cast<float>(512.0);
                auto tmp11 = tmp9 / tmp10;
                auto tmp12 = static_cast<float>(1e-06);
                auto tmp13 = decltype(tmp11)(tmp11 + tmp12);
                auto tmp14 = 1 / std::sqrt(tmp13);
                auto tmp15 = at::vec::Vectorized<float>(tmp14);
                auto tmp16 = tmp8 * tmp15;
                auto tmp18 = tmp16 * tmp17;
                auto tmp20 = tmp18 + tmp19;
                auto tmp21 = at::vec::convert<bfloat16>(tmp20);
                tmp21.store(out_ptr2 + static_cast<long>(x1 + (512L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_native_layer_norm_104 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(81L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(64L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(1024L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (1024L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(1024L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (1024L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(1024.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (1024L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_105 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(4096L);
    constexpr int64_t K = static_cast<long>(1024L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(81L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 3;
    constexpr int64_t Nt_blocks = 128;
    constexpr int64_t Kt_blocks = 32;
    constexpr int64_t Mc_blocks = 3;
    constexpr int64_t Kc_blocks = 32;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (1024L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (32768L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(1024L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (1024L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (32768L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(1024L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (4096L*m_start) + (4096L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (4096L*m_start) + (4096L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_106 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(1024L);
    constexpr int64_t K = static_cast<long>(4096L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(81L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 3;
    constexpr int64_t Nt_blocks = 32;
    constexpr int64_t Kt_blocks = 128;
    constexpr int64_t Mc_blocks = 3;
    constexpr int64_t Kc_blocks = 128;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (4096L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (131072L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(4096L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (4096L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (131072L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(4096L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((1024L*(static_cast<long>((n_start + x1 + x1_inner + (1024L*m_start) + (1024L*x0))) % static_cast<long>(9L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (1024L*m_start) + (1024L*x0)), 81L)) % static_cast<long>(1024L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp10 = tmp7 + tmp9;
                            auto tmp11 = at::vec::convert<bfloat16>(tmp10);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (1024L*m_start) + (1024L*x0)), 16);
                            tmp11.store(Y + static_cast<long>(n_start + x1 + (1024L*m_start) + (1024L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((1024L*(static_cast<long>((n_start + x1 + (1024L*m_start) + (1024L*x0))) % static_cast<long>(9L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (1024L*m_start) + (1024L*x0)), 81L)) % static_cast<long>(1024L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp9 = decltype(tmp6)(tmp6 + tmp8);
                            auto tmp10 = c10::convert<bfloat16>(tmp9);
                            Y[static_cast<long>(n_start + x1 + (1024L*m_start) + (1024L*x0))] = tmp4;
                            Y[static_cast<long>(n_start + x1 + (1024L*m_start) + (1024L*x0))] = tmp10;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_107 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(81L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(64L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(1024L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (1024L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(1024L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (1024L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(1024.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (1024L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_108 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(4096L);
    constexpr int64_t K = static_cast<long>(1024L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(81L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 3;
    constexpr int64_t Nt_blocks = 128;
    constexpr int64_t Kt_blocks = 32;
    constexpr int64_t Mc_blocks = 3;
    constexpr int64_t Kc_blocks = 32;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (1024L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (32768L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(1024L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (1024L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (32768L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(1024L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (4096L*m_start) + (4096L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (4096L*m_start) + (4096L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mul_view_109 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'const float*', 'const bfloat16*', 'bfloat16*', 'float*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, const float* in_ptr3, const bfloat16* in_ptr4, const float* in_ptr5, const bfloat16* in_ptr6, bfloat16* Y, float* out_ptr3)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(1024L);
    constexpr int64_t K = static_cast<long>(4096L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(81L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 3;
    constexpr int64_t Nt_blocks = 32;
    constexpr int64_t Kt_blocks = 128;
    constexpr int64_t Mc_blocks = 3;
    constexpr int64_t Kc_blocks = 128;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (4096L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (131072L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(4096L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (4096L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (131072L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(4096L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp8 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr4[static_cast<long>((1024L*(static_cast<long>((n_start + x1 + x1_inner + (1024L*m_start) + (1024L*x0))) % static_cast<long>(9L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (1024L*m_start) + (1024L*x0)), 81L)) % static_cast<long>(1024L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp10 = in_ptr5[static_cast<long>(0L)];
                            auto tmp13 =
                            [&]
                            {
                                __at_align__ std::array<bfloat16, 32> tmpbuf;
                                #pragma GCC unroll 16
                                for (long x1_inner = 0; x1_inner < 16; x1_inner++)
                                {
                                    tmpbuf[x1_inner] = in_ptr6[static_cast<long>((1024L*(static_cast<long>((n_start + x1 + x1_inner + (1024L*m_start) + (1024L*x0))) % static_cast<long>(9L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + x1_inner + (1024L*m_start) + (1024L*x0)), 81L)) % static_cast<long>(1024L))))];
                                }
                                return at::vec::Vectorized<bfloat16>::loadu(tmpbuf.data(), 16);
                            }
                            ()
                            ;
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp9 = at::vec::convert<float>(tmp8);
                            auto tmp11 = at::vec::Vectorized<float>(tmp10);
                            auto tmp12 = tmp9 * tmp11;
                            auto tmp14 = at::vec::convert<float>(tmp13);
                            auto tmp15 = tmp12 + tmp14;
                            auto tmp16 = tmp7 + tmp15;
                            auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                            tmp16.store(out_ptr3 + static_cast<long>(n_start + x1 + (1024L*m_start) + (1024L*x0)));
                            tmp17.store(Y + static_cast<long>(n_start + x1 + (1024L*m_start) + (1024L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp5 = in_ptr3[static_cast<long>(0L)];
                            auto tmp7 = in_ptr4[static_cast<long>((1024L*(static_cast<long>((n_start + x1 + (1024L*m_start) + (1024L*x0))) % static_cast<long>(9L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (1024L*m_start) + (1024L*x0)), 81L)) % static_cast<long>(1024L))))];
                            auto tmp9 = in_ptr5[static_cast<long>(0L)];
                            auto tmp11 = in_ptr6[static_cast<long>((1024L*(static_cast<long>((n_start + x1 + (1024L*m_start) + (1024L*x0))) % static_cast<long>(9L))) + (9216L*(static_cast<long>(c10::div_floor_integer((n_start + x1 + (1024L*m_start) + (1024L*x0)), 81L)) % static_cast<long>(1024L))))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp8 = c10::convert<float>(tmp7);
                            auto tmp10 = decltype(tmp8)(tmp8 * tmp9);
                            auto tmp12 = c10::convert<float>(tmp11);
                            auto tmp13 = decltype(tmp10)(tmp10 + tmp12);
                            auto tmp14 = decltype(tmp6)(tmp6 + tmp13);
                            auto tmp15 = c10::convert<bfloat16>(tmp14);
                            out_ptr3[static_cast<long>(n_start + x1 + (1024L*m_start) + (1024L*x0))] = tmp14;
                            Y[static_cast<long>(n_start + x1 + (1024L*m_start) + (1024L*x0))] = tmp15;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_native_layer_norm_110 = async_compile.cpp_pybinding(['const bfloat16*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr0,
                       float* out_ptr1,
                       bfloat16* out_ptr2)
{
    {
        #pragma GCC ivdep
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(81L); x0+=static_cast<long>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(64L));
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(1024L); x1+=static_cast<long>(16L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (1024L*x0)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp1, &weight_recps);
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.mean);
                out_ptr1[static_cast<long>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
            for(long x1=static_cast<long>(0L); x1<static_cast<long>(1024L); x1+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x1 + (1024L*x0)), 16);
                auto tmp2 = out_ptr0[static_cast<long>(x0)];
                auto tmp5 = out_ptr1[static_cast<long>(x0)];
                auto tmp13 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x1), 16);
                auto tmp15 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x1), 16);
                auto tmp1 = at::vec::convert<float>(tmp0);
                auto tmp3 = at::vec::Vectorized<float>(tmp2);
                auto tmp4 = tmp1 - tmp3;
                auto tmp6 = static_cast<float>(1024.0);
                auto tmp7 = tmp5 / tmp6;
                auto tmp8 = static_cast<float>(1e-06);
                auto tmp9 = decltype(tmp7)(tmp7 + tmp8);
                auto tmp10 = 1 / std::sqrt(tmp9);
                auto tmp11 = at::vec::Vectorized<float>(tmp10);
                auto tmp12 = tmp4 * tmp11;
                auto tmp14 = tmp12 * tmp13;
                auto tmp16 = tmp14 + tmp15;
                auto tmp17 = at::vec::convert<bfloat16>(tmp16);
                tmp17.store(out_ptr2 + static_cast<long>(x1 + (1024L*x0)), 16);
            }
        }
    }
}
''')


cpp_fused_gelu_native_layer_norm_111 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(4096L);
    constexpr int64_t K = static_cast<long>(1024L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(81L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 3;
    constexpr int64_t Nt_blocks = 128;
    constexpr int64_t Kt_blocks = 32;
    constexpr int64_t Mc_blocks = 3;
    constexpr int64_t Kc_blocks = 32;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (1024L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (32768L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(1024L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (1024L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (32768L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(1024L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = at::vec::Vectorized<float>(tmp5);
                            auto tmp7 = tmp3 * tmp6;
                            auto tmp8 = static_cast<float>(0.7071067811865476);
                            auto tmp9 = at::vec::Vectorized<float>(tmp8);
                            auto tmp10 = tmp3 * tmp9;
                            auto tmp11 = tmp10.erf();
                            auto tmp12 = static_cast<float>(1.0);
                            auto tmp13 = at::vec::Vectorized<float>(tmp12);
                            auto tmp14 = tmp11 + tmp13;
                            auto tmp15 = tmp7 * tmp14;
                            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
                            tmp16.store(Y + static_cast<long>(n_start + x1 + (4096L*m_start) + (4096L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            auto tmp5 = static_cast<float>(0.5);
                            auto tmp6 = decltype(tmp3)(tmp3 * tmp5);
                            auto tmp7 = static_cast<float>(0.7071067811865476);
                            auto tmp8 = decltype(tmp3)(tmp3 * tmp7);
                            auto tmp9 = std::erf(tmp8);
                            auto tmp10 = static_cast<float>(1.0);
                            auto tmp11 = decltype(tmp9)(tmp9 + tmp10);
                            auto tmp12 = decltype(tmp6)(tmp6 * tmp11);
                            auto tmp13 = c10::convert<bfloat16>(tmp12);
                            Y[static_cast<long>(n_start + x1 + (4096L*m_start) + (4096L*x0))] = tmp13;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused_view_112 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"



template <bool accum>
inline void kernel_micro_gemm_amx_kernel_32_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 32 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_loadd(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
        _tile_zero(2);
        _tile_zero(3);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(4, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(6, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 4, 6);
        _tile_loadd(7, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 4, 7);
        _tile_stream_loadd(5, A + 16 * lda + k, lda * sizeof(bfloat16));
        _tile_dpbf16ps(2, 5, 6);
        _tile_dpbf16ps(3, 5, 7);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
        _tile_stored(2, C + 16 * ldc + 0, ldc * sizeof(float));
        _tile_stored(3, C + 16 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 32 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}
template <bool accum>
inline void kernel_micro_gemm_amx_kernel_16_2(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc,
    uint8_t tilecfg_rows
) {
    // TODO(jgong5): add prefetch hint for A, B, C
    auto loadconfig = [](const amx_tilecfg& cfg) {
        _tile_loadconfig(&cfg);
    };
    const auto last_k_offset = K / 32 * 32;
    const auto tail_k_size = K - last_k_offset;
    if C10_LIKELY (last_k_offset > 0) {
        amx_state.configure(tilecfg_rows, 64, 16 / 16, 2, loadconfig);
    } else {
        amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
    }
    auto load_c = [&]() {
        _tile_loadd(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_loadd(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };
    auto zero_c = [&]() {
        _tile_zero(0);
        _tile_zero(1);
    };

    if constexpr (accum) {
        load_c();
    } else {
        zero_c();
    }

    auto compute = [&](int k) {
        _tile_stream_loadd(2, A + 0 * lda + k, lda * sizeof(bfloat16));
        _tile_loadd(3, B + k * ldb + 0, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(0, 2, 3);
        _tile_loadd(4, B + k * ldb + 32, ldb * 2 * sizeof(bfloat16));
        _tile_dpbf16ps(1, 2, 4);
    };

    #pragma GCC unroll 4
    for (int k = 0; k < last_k_offset; k += 32) {
        compute(k);
    }

    auto store_c = [&]() {
    // store to C
        _tile_stored(0, C + 0 * ldc + 0, ldc * sizeof(float));
        _tile_stored(1, C + 0 * ldc + 16, ldc * sizeof(float));
    };

    // TODO(jgong5): move tail k computation to separate loopnest to save tile configuration overhead
    if C10_UNLIKELY (tail_k_size > 0) {
        if C10_LIKELY (last_k_offset > 0) {
            store_c();
            amx_state.configure(tilecfg_rows, tail_k_size * sizeof(bfloat16), 16 / 16, 2, loadconfig);
            load_c();
        }
        compute(last_k_offset);
    }

    store_c();
}

template <bool accum>
inline void kernel_micro_gemm(
    AMXState& amx_state,
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    TORCH_CHECK(N % 32 == 0, "N dimension must be multiple of 32");
    TORCH_CHECK(K % 2 == 0, "K dimension must be multiple of 2");
    // TODO(jgong5): loop unroll for M and N
    for (int64_t m = 0; m < M; m += 32) {
        int64_t block_m = std::min<int64_t>(M - m, 32);
        int64_t m_tail = m;
        for (int64_t n = 0; n < N; n += 32) {
            if (block_m >= 32) {
                kernel_micro_gemm_amx_kernel_32_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 32;
                m_tail += 32;
            }
            else
            if (block_m >= 16) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m * lda,
                    B + n,
                    C + m * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    16
                );
                block_m -= 16;
                m_tail += 16;
            }
            if (block_m > 0) {
                kernel_micro_gemm_amx_kernel_16_2<accum>(
                    amx_state,
                    A + m_tail * lda,
                    B + n,
                    C + m_tail * ldc + n,
                    K,
                    lda,
                    ldb,
                    ldc,
                    block_m
                );
            }
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(1024L);
    constexpr int64_t K = static_cast<long>(4096L);
    constexpr int64_t M0 = 32;
    constexpr int64_t N0 = 32;
    constexpr int64_t K0 = 32;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(81L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 3;
    constexpr int64_t Nt_blocks = 32;
    constexpr int64_t Kt_blocks = 128;
    constexpr int64_t Mc_blocks = 3;
    constexpr int64_t Kc_blocks = 128;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;
        AMXState amx_state;
        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (4096L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (131072L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(4096L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            amx_state,
                            &(X[static_cast<long>(k_start + (4096L*m_start))]),
                            &(W[static_cast<long>((32L*k_start) + (131072L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(4096L),
                            static_cast<long>(32L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1 + (N0*x0)), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (1024L*m_start) + (1024L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1 + (N0*x0))];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            Y[static_cast<long>(n_start + x1 + (1024L*m_start) + (1024L*x0))] = tmp4;
                        }
                    }
                }

            }
        }
        amx_state.release([]() { _tile_release(); });
    }
}
''')


cpp_fused__to_copy_add_mean_mul_native_layer_norm_113 = async_compile.cpp_pybinding(['float*', 'const bfloat16*', 'const float*', 'const float*', 'const float*', 'const float*', 'float*', 'float*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"
extern "C"  void kernel(float* in_out_ptr0,
                       const bfloat16* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       const float* in_ptr3,
                       const float* in_ptr4,
                       float* out_ptr1,
                       float* out_ptr2,
                       bfloat16* out_ptr3)
{
    auto out_ptr0 = in_out_ptr0;
    {
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(1024L); x0+=static_cast<long>(16L))
        {
            {
                float tmp_acc0 = 0;
                at::vec::Vectorized<float> tmp_acc0_vec = at::vec::Vectorized<float>(0);
                for(long x1=static_cast<long>(0L); x1<static_cast<long>(81L); x1+=static_cast<long>(1L))
                {
                    auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(in_ptr0 + static_cast<long>(x0 + (1024L*x1)), 16);
                    auto tmp2 = at::vec::Vectorized<float>::loadu(in_ptr1 + static_cast<long>(x0), 16);
                    auto tmp4 = at::vec::Vectorized<float>::loadu(in_ptr2 + static_cast<long>(x0 + (1024L*x1)), 16);
                    auto tmp1 = at::vec::convert<float>(tmp0);
                    auto tmp3 = tmp1 * tmp2;
                    auto tmp5 = tmp3 + tmp4;
                    tmp_acc0_vec = tmp_acc0_vec + tmp5;
                }
                tmp_acc0_vec.store(out_ptr0 + static_cast<long>(x0));
            }
        }
    }
    {
        {
            Welford<float> tmp_acc0 = Welford<float>();
            Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
            static WeightRecp<at::vec::Vectorized<float>> weight_recps(static_cast<long>(64L));
            for(long x0=static_cast<long>(0L); x0<static_cast<long>(1024L); x0+=static_cast<long>(16L))
            {
                auto tmp0 = at::vec::Vectorized<float>::loadu(out_ptr0 + static_cast<long>(x0), 16);
                auto tmp1 = static_cast<float>(81.0);
                auto tmp2 = at::vec::Vectorized<float>(tmp1);
                auto tmp3 = tmp0 / tmp2;
                tmp3.store(in_out_ptr0 + static_cast<long>(x0));
                tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp3, &weight_recps);
            }
            tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
            out_ptr1[static_cast<long>(0L)] = static_cast<float>(tmp_acc0.mean);
            out_ptr2[static_cast<long>(0L)] = static_cast<float>(tmp_acc0.m2);
        }
    }
    {
        for(long x0=static_cast<long>(0L); x0<static_cast<long>(1024L); x0+=static_cast<long>(16L))
        {
            auto tmp0 = at::vec::Vectorized<float>::loadu(in_out_ptr0 + static_cast<long>(x0), 16);
            auto tmp1 = out_ptr1[static_cast<long>(0L)];
            auto tmp4 = out_ptr2[static_cast<long>(0L)];
            auto tmp12 = at::vec::Vectorized<float>::loadu(in_ptr3 + static_cast<long>(x0), 16);
            auto tmp14 = at::vec::Vectorized<float>::loadu(in_ptr4 + static_cast<long>(x0), 16);
            auto tmp2 = at::vec::Vectorized<float>(tmp1);
            auto tmp3 = tmp0 - tmp2;
            auto tmp5 = static_cast<float>(1024.0);
            auto tmp6 = tmp4 / tmp5;
            auto tmp7 = static_cast<float>(1e-06);
            auto tmp8 = decltype(tmp6)(tmp6 + tmp7);
            auto tmp9 = 1 / std::sqrt(tmp8);
            auto tmp10 = at::vec::Vectorized<float>(tmp9);
            auto tmp11 = tmp3 * tmp10;
            auto tmp13 = tmp11 * tmp12;
            auto tmp15 = tmp13 + tmp14;
            auto tmp16 = at::vec::convert<bfloat16>(tmp15);
            tmp16.store(out_ptr3 + static_cast<long>(x0), 16);
        }
    }
}
''')


cpp_fused__to_copy_addmm_114 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''
#include "/tmp/torchinductor_chunyuan/ky/cky2bufythacofebk7ujv36e4pxyqcqbpsy5r4vojoprjiwcwfxf.h"

#include "c10/util/Unroll.h"




template <bool accum>
inline void kernel_micro_gemm(
    const bfloat16* __restrict__ A,
    const bfloat16* __restrict__ B,
    float* __restrict__ C,
    int64_t M,
    int64_t N,
    int64_t K,
    int64_t lda,
    int64_t ldb,
    int64_t ldc
) {
    for (int64_t m = 0; m < M; ++m) {
        for (int64_t n = 0; n < N; ++n) {
            float result = accum ? C[m * ldc + n] : 0;
            for (int64_t k = 0; k < K; ++k) {
                result += (float)A[m * lda + k] * (float)B[k * ldb + n] * 1;
            }
            C[m * ldc + n] = result;
        }
    }
}

extern "C"
void kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)
{

    constexpr int64_t num_threads = 1;
    constexpr int64_t N = static_cast<long>(1000L);
    constexpr int64_t K = static_cast<long>(1024L);
    constexpr int64_t M0 = 1;
    constexpr int64_t N0 = 1;
    constexpr int64_t K0 = 1;
    constexpr int64_t N0_blocks = (N + N0 - 1) / N0;
    constexpr int64_t K0_blocks = (K + K0 - 1) / K0;

    static_assert(N % N0 == 0, "N dimension must be multiple of N0");

    // TODO(jgong5): improve cache blocking with CPU info (Mc, Kc)
    constexpr int64_t M = static_cast<long>(1L);
    constexpr int64_t M0_blocks = (M + M0 - 1) / M0;
    constexpr int64_t Mt_blocks = 1;
    constexpr int64_t Nt_blocks = 1000;
    constexpr int64_t Kt_blocks = 1024;
    constexpr int64_t Mc_blocks = 1;
    constexpr int64_t Kc_blocks = 1024;

    // TODO(jgong5): support k-slicing
    TORCH_CHECK(Kt_blocks == K0_blocks, "Do not support k slicing yet.");
    // make sure all partitions are assigned
    TORCH_CHECK(
        Mt_blocks * Nt_blocks * Kt_blocks * 1 >= M0_blocks * N0_blocks * K0_blocks,
        "Not all partitions are assigned."
    );
    {
        int64_t m_block_start = 0;
        int64_t m_block_end = M0_blocks;
        int64_t n_block_start = 0;
        int64_t n_block_end = N0_blocks;
        int64_t k_block_start = 0;
        int64_t k_block_end = K0_blocks;

        for (int64_t mc = m_block_start; mc < m_block_end; mc += Mc_blocks) {
            const int64_t m_start = mc * M0;
            const int64_t m_end = std::min((mc + Mc_blocks) * M0, M);
            const int64_t m_size = m_end - m_start;
            auto _local_acc_buf = std::make_unique<float[]>(static_cast<long>((N0*m_end) + ((-1L)*N0*m_start))); auto local_acc_buf = _local_acc_buf.get();
            for (int64_t nc = n_block_start; nc < n_block_end; ++nc) {
                const int64_t n_start = nc * N0;
                const int64_t n_size = N0;
                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {
                    int64_t k_start = kc * K0;
                    int64_t k_end = std::min((kc + Kc_blocks) * K0, K);
                    if (kc == k_block_start) {
                        kernel_micro_gemm<static_cast<bool>(false)>(
                            &(X[static_cast<long>(k_start + (1024L*m_start))]),
                            &(W[static_cast<long>(k_start + (1024L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(1024L),
                            static_cast<long>(1L),
                            static_cast<long>(N0)
                        );

                    } else {
                        kernel_micro_gemm<static_cast<bool>(true)>(
                            &(X[static_cast<long>(k_start + (1024L*m_start))]),
                            &(W[static_cast<long>(k_start + (1024L*nc))]),
                            &(local_acc_buf[static_cast<long>(0L)]),
                            static_cast<long>(m_end + ((-1L)*m_start)),
                            static_cast<long>(N0),
                            static_cast<long>(k_end + ((-1L)*k_start)),
                            static_cast<long>(1024L),
                            static_cast<long>(1L),
                            static_cast<long>(N0)
                        );

                    }
                }
                {
                    #pragma GCC ivdep
                    for(long x0=static_cast<long>(0L); x0<static_cast<long>(m_end + ((-1L)*m_start)); x0+=static_cast<long>(1L))
                    {
                        for(long x1=static_cast<long>(0L); x1<static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1+=static_cast<long>(16L))
                        {
                            auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<long>(n_start + x1), 16);
                            auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<long>(x1), 16);
                            auto tmp1 = at::vec::convert<float>(tmp0);
                            auto tmp3 = tmp1 + tmp2;
                            auto tmp4 = at::vec::convert<bfloat16>(tmp3);
                            tmp4.store(Y + static_cast<long>(n_start + x1 + (1000L*m_start) + (1000L*x0)), 16);
                        }
                        #pragma omp simd simdlen(8) 
                        for(long x1=static_cast<long>(16L*(c10::div_floor_integer(N0, 16L))); x1<static_cast<long>(N0); x1+=static_cast<long>(1L))
                        {
                            auto tmp0 = inp[static_cast<long>(n_start + x1)];
                            auto tmp2 = local_acc_buf[static_cast<long>(x1)];
                            auto tmp1 = c10::convert<float>(tmp0);
                            auto tmp3 = decltype(tmp1)(tmp1 + tmp2);
                            auto tmp4 = c10::convert<bfloat16>(tmp3);
                            Y[static_cast<long>(n_start + x1 + (1000L*m_start) + (1000L*x0))] = tmp4;
                        }
                    }
                }

            }
        }

    }
}
''')


async_compile.wait(globals())
del async_compile

def call(args):
    arg344_1, = args
    args.clear()
    assert_size_stride(arg344_1, (1, 3, 288, 288), (248832, 82944, 288, 1))
    buf0 = empty_strided_cpu((1, 3, 288, 288), (248832, 1, 864, 3), torch.bfloat16)
    print("enter cpp_fused__to_copy_0")
    cpp_fused__to_copy_0(arg344_1, buf0)
    print("done cpp_fused__to_copy_0")
    del arg344_1
    buf1 = torch.ops.mkldnn._convolution_pointwise(buf0, _frozen_param606, _frozen_param344, [0, 0], [4, 4], [1, 1], 1, 'none', [None], '')
    assert_size_stride(buf1, (1, 128, 72, 72), (663552, 1, 9216, 128))
    del buf0
    buf2 = empty_strided_cpu((1, 72, 72, 1), (5184, 72, 1, 5184), torch.float32)
    buf3 = empty_strided_cpu((1, 72, 72, 1), (5184, 72, 1, 5184), torch.float32)
    buf5 = empty_strided_cpu((1, 72, 72, 128), (663552, 9216, 128, 1), torch.bfloat16)
    print("enter cpp_fused_native_layer_norm_1")
    cpp_fused_native_layer_norm_1(buf1, _frozen_param0, _frozen_param1, buf2, buf3, buf5)
    print("done cpp_fused_native_layer_norm_1")
    #buf6 = torch.ops.mkldnn._convolution_pointwise(reinterpret_tensor(buf5, (1, 128, 72, 72), (0, 1, 9216, 128), 0), _frozen_param607, _frozen_param346, [3, 3], [1, 1], [1, 1], 128, 'none', [None], '')
    buf6 = torch.ops.mkldnn._convolution_pointwise(reinterpret_tensor(buf5, (1, 128, 72, 72), (663552, 1, 9216, 128), 0), _frozen_param607, _frozen_param346, [3, 3], [1, 1], [1, 1], 128, 'none', [None], '')
    assert_size_stride(buf6, (1, 128, 72, 72), (663552, 1, 9216, 128))
    buf7 = buf3; del buf3  # reuse
    buf8 = buf2; del buf2  # reuse
    buf10 = reinterpret_tensor(buf1, (1, 72, 72, 128), (663552, 9216, 128, 1), 0); del buf1  # reuse
    print("enter cpp_fused_native_layer_norm_2")
    cpp_fused_native_layer_norm_2(buf6, _frozen_param2, _frozen_param3, buf7, buf8, buf10)
    print("done cpp_fused_native_layer_norm_2")
    buf12 = empty_strided_cpu((5184, 512), (512, 1), torch.bfloat16)
    print("enter cpp_fused_gelu_native_layer_norm_3")
    cpp_fused_gelu_native_layer_norm_3(buf10, constant93, _frozen_param348, buf12)
    print("done cpp_fused_gelu_native_layer_norm_3")
    buf14 = reinterpret_tensor(buf10, (5184, 128), (128, 1), 0); del buf10  # reuse
    buf15 = buf6; del buf6  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_4")
    cpp_fused__to_copy_add_mul_view_4(buf12, constant94, _frozen_param350, _frozen_param352, buf5, buf15)
    print("done cpp_fused__to_copy_add_mul_view_4")
    buf16 = torch.ops.mkldnn._convolution_pointwise(buf15, _frozen_param610, _frozen_param353, [3, 3], [1, 1], [1, 1], 128, 'none', [None], '')
    assert_size_stride(buf16, (1, 128, 72, 72), (663552, 1, 9216, 128))
    buf17 = buf8; del buf8  # reuse
    buf18 = buf7; del buf7  # reuse
    buf20 = reinterpret_tensor(buf15, (1, 72, 72, 128), (663552, 9216, 128, 1), 0); del buf15  # reuse
    print("enter cpp_fused_native_layer_norm_5")
    cpp_fused_native_layer_norm_5(buf16, _frozen_param5, _frozen_param6, buf17, buf18, buf20)
    print("done cpp_fused_native_layer_norm_5")
    del buf16
    buf22 = buf12; del buf12  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_6")
    cpp_fused_gelu_native_layer_norm_6(buf20, constant102, _frozen_param355, buf22)
    print("done cpp_fused_gelu_native_layer_norm_6")
    buf25 = empty_strided_cpu((1, 128, 72, 72), (663552, 1, 9216, 128), torch.float32)
    buf26 = reinterpret_tensor(buf20, (1, 128, 72, 72), (663552, 1, 9216, 128), 0); del buf20  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_7")
    cpp_fused__to_copy_add_mul_view_7(buf22, constant103, _frozen_param357, _frozen_param359, buf14, _frozen_param352, buf5, buf26, buf25)
    print("done cpp_fused__to_copy_add_mul_view_7")
    del buf14
    del buf5
    buf27 = torch.ops.mkldnn._convolution_pointwise(buf26, _frozen_param613, _frozen_param360, [3, 3], [1, 1], [1, 1], 128, 'none', [None], '')
    assert_size_stride(buf27, (1, 128, 72, 72), (663552, 1, 9216, 128))
    buf28 = buf18; del buf18  # reuse
    buf29 = buf17; del buf17  # reuse
    buf31 = reinterpret_tensor(buf26, (1, 72, 72, 128), (663552, 9216, 128, 1), 0); del buf26  # reuse
    print("enter cpp_fused_native_layer_norm_8")
    cpp_fused_native_layer_norm_8(buf27, _frozen_param8, _frozen_param9, buf28, buf29, buf31)
    print("done cpp_fused_native_layer_norm_8")
    buf33 = buf22; del buf22  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_9")
    cpp_fused_gelu_native_layer_norm_9(buf31, constant109, _frozen_param362, buf33)
    print("done cpp_fused_gelu_native_layer_norm_9")
    buf35 = reinterpret_tensor(buf31, (5184, 128), (128, 1), 0); del buf31  # reuse
    print("enter cpp_fused_view_10")
    cpp_fused_view_10(buf33, constant110, _frozen_param364, buf35)
    print("done cpp_fused_view_10")
    del buf33
    buf36 = buf29; del buf29  # reuse
    buf37 = buf28; del buf28  # reuse
    buf39 = buf27; del buf27  # reuse
    print("enter cpp_fused__to_copy_native_layer_norm_11")
    cpp_fused__to_copy_native_layer_norm_11(buf35, _frozen_param366, buf25, _frozen_param11, _frozen_param12, buf36, buf37, buf39)
    print("done cpp_fused__to_copy_native_layer_norm_11")
    del buf25
    del buf35
    del buf36
    del buf37
    buf40 = torch.ops.mkldnn._convolution_pointwise(buf39, _frozen_param616, _frozen_param367, [0, 0], [2, 2], [1, 1], 1, 'none', [None], '')
    assert_size_stride(buf40, (1, 256, 36, 36), (331776, 1, 9216, 256))
    buf41 = torch.ops.mkldnn._convolution_pointwise(buf40, _frozen_param617, _frozen_param369, [3, 3], [1, 1], [1, 1], 256, 'none', [None], '')
    assert_size_stride(buf41, (1, 256, 36, 36), (331776, 1, 9216, 256))
    buf42 = empty_strided_cpu((1, 36, 36, 1), (1296, 36, 1, 1296), torch.float32)
    buf43 = empty_strided_cpu((1, 36, 36, 1), (1296, 36, 1, 1296), torch.float32)
    buf45 = empty_strided_cpu((1, 36, 36, 256), (331776, 9216, 256, 1), torch.bfloat16)
    print("enter cpp_fused_native_layer_norm_12")
    cpp_fused_native_layer_norm_12(buf41, _frozen_param13, _frozen_param14, buf42, buf43, buf45)
    print("done cpp_fused_native_layer_norm_12")
    buf47 = empty_strided_cpu((1296, 1024), (1024, 1), torch.bfloat16)
    print("enter cpp_fused_gelu_native_layer_norm_13")
    cpp_fused_gelu_native_layer_norm_13(buf45, constant122, _frozen_param371, buf47)
    print("done cpp_fused_gelu_native_layer_norm_13")
    buf49 = reinterpret_tensor(buf45, (1296, 256), (256, 1), 0); del buf45  # reuse
    buf50 = buf41; del buf41  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_14")
    cpp_fused__to_copy_add_mul_view_14(buf47, constant123, _frozen_param373, _frozen_param375, buf40, buf50)
    print("done cpp_fused__to_copy_add_mul_view_14")
    buf51 = torch.ops.mkldnn._convolution_pointwise(buf50, _frozen_param620, _frozen_param376, [3, 3], [1, 1], [1, 1], 256, 'none', [None], '')
    assert_size_stride(buf51, (1, 256, 36, 36), (331776, 1, 9216, 256))
    buf52 = buf43; del buf43  # reuse
    buf53 = buf42; del buf42  # reuse
    buf55 = reinterpret_tensor(buf50, (1, 36, 36, 256), (331776, 9216, 256, 1), 0); del buf50  # reuse
    print("enter cpp_fused_native_layer_norm_15")
    cpp_fused_native_layer_norm_15(buf51, _frozen_param16, _frozen_param17, buf52, buf53, buf55)
    print("done cpp_fused_native_layer_norm_15")
    del buf51
    buf57 = buf47; del buf47  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_16")
    cpp_fused_gelu_native_layer_norm_16(buf55, constant131, _frozen_param378, buf57)
    print("done cpp_fused_gelu_native_layer_norm_16")
    buf60 = empty_strided_cpu((1, 256, 36, 36), (331776, 1, 9216, 256), torch.float32)
    buf61 = reinterpret_tensor(buf55, (1, 256, 36, 36), (331776, 1, 9216, 256), 0); del buf55  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_17")
    cpp_fused__to_copy_add_mul_view_17(buf57, constant132, _frozen_param380, _frozen_param382, buf49, _frozen_param375, buf40, buf61, buf60)
    print("done cpp_fused__to_copy_add_mul_view_17")
    del buf40
    del buf49
    buf62 = torch.ops.mkldnn._convolution_pointwise(buf61, _frozen_param623, _frozen_param383, [3, 3], [1, 1], [1, 1], 256, 'none', [None], '')
    assert_size_stride(buf62, (1, 256, 36, 36), (331776, 1, 9216, 256))
    buf63 = buf53; del buf53  # reuse
    buf64 = buf52; del buf52  # reuse
    buf66 = reinterpret_tensor(buf61, (1, 36, 36, 256), (331776, 9216, 256, 1), 0); del buf61  # reuse
    print("enter cpp_fused_native_layer_norm_18")
    cpp_fused_native_layer_norm_18(buf62, _frozen_param19, _frozen_param20, buf63, buf64, buf66)
    print("done cpp_fused_native_layer_norm_18")
    buf68 = buf57; del buf57  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_19")
    cpp_fused_gelu_native_layer_norm_19(buf66, constant138, _frozen_param385, buf68)
    print("done cpp_fused_gelu_native_layer_norm_19")
    buf70 = reinterpret_tensor(buf66, (1296, 256), (256, 1), 0); del buf66  # reuse
    print("enter cpp_fused_view_20")
    cpp_fused_view_20(buf68, constant139, _frozen_param387, buf70)
    print("done cpp_fused_view_20")
    del buf68
    buf71 = buf64; del buf64  # reuse
    buf72 = buf63; del buf63  # reuse
    buf74 = buf62; del buf62  # reuse
    print("enter cpp_fused__to_copy_native_layer_norm_21")
    cpp_fused__to_copy_native_layer_norm_21(buf70, _frozen_param389, buf60, _frozen_param22, _frozen_param23, buf71, buf72, buf74)
    print("done cpp_fused__to_copy_native_layer_norm_21")
    del buf60
    del buf70
    del buf71
    del buf72
    buf75 = torch.ops.mkldnn._convolution_pointwise(buf74, _frozen_param626, _frozen_param390, [0, 0], [2, 2], [1, 1], 1, 'none', [None], '')
    assert_size_stride(buf75, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf76 = torch.ops.mkldnn._convolution_pointwise(buf75, _frozen_param627, _frozen_param392, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf76, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf77 = empty_strided_cpu((1, 18, 18, 1), (324, 18, 1, 324), torch.float32)
    buf78 = empty_strided_cpu((1, 18, 18, 1), (324, 18, 1, 324), torch.float32)
    buf80 = empty_strided_cpu((1, 18, 18, 512), (165888, 9216, 512, 1), torch.bfloat16)
    print("enter cpp_fused_native_layer_norm_22")
    cpp_fused_native_layer_norm_22(buf76, _frozen_param24, _frozen_param25, buf77, buf78, buf80)
    print("done cpp_fused_native_layer_norm_22")
    buf82 = reinterpret_tensor(buf39, (324, 2048), (2048, 1), 0); del buf39  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_23")
    cpp_fused_gelu_native_layer_norm_23(buf80, constant151, _frozen_param394, buf82)
    print("done cpp_fused_gelu_native_layer_norm_23")
    buf84 = reinterpret_tensor(buf80, (324, 512), (512, 1), 0); del buf80  # reuse
    buf85 = buf76; del buf76  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_24")
    cpp_fused__to_copy_add_mul_view_24(buf82, constant152, _frozen_param396, _frozen_param398, buf75, buf85)
    print("done cpp_fused__to_copy_add_mul_view_24")
    buf86 = torch.ops.mkldnn._convolution_pointwise(buf85, _frozen_param630, _frozen_param399, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf86, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf87 = buf78; del buf78  # reuse
    buf88 = buf77; del buf77  # reuse
    buf90 = reinterpret_tensor(buf85, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf85  # reuse
    print("enter cpp_fused_native_layer_norm_25")
    cpp_fused_native_layer_norm_25(buf86, _frozen_param27, _frozen_param28, buf87, buf88, buf90)
    print("done cpp_fused_native_layer_norm_25")
    del buf86
    buf92 = buf82; del buf82  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_26")
    cpp_fused_gelu_native_layer_norm_26(buf90, constant160, _frozen_param401, buf92)
    print("done cpp_fused_gelu_native_layer_norm_26")
    buf95 = empty_strided_cpu((1, 512, 18, 18), (165888, 1, 9216, 512), torch.float32)
    buf96 = reinterpret_tensor(buf90, (1, 512, 18, 18), (165888, 1, 9216, 512), 0); del buf90  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_27")
    cpp_fused__to_copy_add_mul_view_27(buf92, constant161, _frozen_param403, _frozen_param405, buf84, _frozen_param398, buf75, buf96, buf95)
    print("done cpp_fused__to_copy_add_mul_view_27")
    del buf75
    del buf84
    buf97 = torch.ops.mkldnn._convolution_pointwise(buf96, _frozen_param633, _frozen_param406, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf97, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf98 = buf88; del buf88  # reuse
    buf99 = buf87; del buf87  # reuse
    buf101 = reinterpret_tensor(buf96, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf96  # reuse
    print("enter cpp_fused_native_layer_norm_28")
    cpp_fused_native_layer_norm_28(buf97, _frozen_param30, _frozen_param31, buf98, buf99, buf101)
    print("done cpp_fused_native_layer_norm_28")
    buf103 = buf92; del buf92  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_29")
    cpp_fused_gelu_native_layer_norm_29(buf101, constant169, _frozen_param408, buf103)
    print("done cpp_fused_gelu_native_layer_norm_29")
    buf105 = reinterpret_tensor(buf101, (324, 512), (512, 1), 0); del buf101  # reuse
    buf106 = buf97; del buf97  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_30")
    cpp_fused__to_copy_add_mul_view_30(buf103, constant170, _frozen_param410, _frozen_param412, buf95, buf106)
    print("done cpp_fused__to_copy_add_mul_view_30")
    buf107 = torch.ops.mkldnn._convolution_pointwise(buf106, _frozen_param636, _frozen_param413, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf107, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf108 = buf99; del buf99  # reuse
    buf109 = buf98; del buf98  # reuse
    buf111 = reinterpret_tensor(buf106, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf106  # reuse
    print("enter cpp_fused_native_layer_norm_31")
    cpp_fused_native_layer_norm_31(buf107, _frozen_param33, _frozen_param34, buf108, buf109, buf111)
    print("done cpp_fused_native_layer_norm_31")
    del buf107
    buf113 = buf103; del buf103  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_32")
    cpp_fused_gelu_native_layer_norm_32(buf111, constant178, _frozen_param415, buf113)
    print("done cpp_fused_gelu_native_layer_norm_32")
    buf116 = empty_strided_cpu((1, 512, 18, 18), (165888, 1, 9216, 512), torch.float32)
    buf117 = reinterpret_tensor(buf111, (1, 512, 18, 18), (165888, 1, 9216, 512), 0); del buf111  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_33")
    cpp_fused__to_copy_add_mul_view_33(buf113, constant179, _frozen_param417, _frozen_param419, buf105, _frozen_param412, buf95, buf117, buf116)
    print("done cpp_fused__to_copy_add_mul_view_33")
    del buf105
    buf118 = torch.ops.mkldnn._convolution_pointwise(buf117, _frozen_param639, _frozen_param420, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf118, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf119 = buf109; del buf109  # reuse
    buf120 = buf108; del buf108  # reuse
    buf122 = reinterpret_tensor(buf117, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf117  # reuse
    print("enter cpp_fused_native_layer_norm_34")
    cpp_fused_native_layer_norm_34(buf118, _frozen_param36, _frozen_param37, buf119, buf120, buf122)
    print("done cpp_fused_native_layer_norm_34")
    buf124 = buf113; del buf113  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_35")
    cpp_fused_gelu_native_layer_norm_35(buf122, constant187, _frozen_param422, buf124)
    print("done cpp_fused_gelu_native_layer_norm_35")
    buf126 = reinterpret_tensor(buf122, (324, 512), (512, 1), 0); del buf122  # reuse
    buf127 = buf118; del buf118  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_36")
    cpp_fused__to_copy_add_mul_view_36(buf124, constant188, _frozen_param424, _frozen_param426, buf116, buf127)
    print("done cpp_fused__to_copy_add_mul_view_36")
    buf128 = torch.ops.mkldnn._convolution_pointwise(buf127, _frozen_param642, _frozen_param427, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf128, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf129 = buf120; del buf120  # reuse
    buf130 = buf119; del buf119  # reuse
    buf132 = reinterpret_tensor(buf127, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf127  # reuse
    print("enter cpp_fused_native_layer_norm_37")
    cpp_fused_native_layer_norm_37(buf128, _frozen_param39, _frozen_param40, buf129, buf130, buf132)
    print("done cpp_fused_native_layer_norm_37")
    del buf128
    buf134 = buf124; del buf124  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_38")
    cpp_fused_gelu_native_layer_norm_38(buf132, constant196, _frozen_param429, buf134)
    print("done cpp_fused_gelu_native_layer_norm_38")
    buf137 = buf95; del buf95  # reuse
    buf138 = reinterpret_tensor(buf132, (1, 512, 18, 18), (165888, 1, 9216, 512), 0); del buf132  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_39")
    cpp_fused__to_copy_add_mul_view_39(buf134, constant197, _frozen_param431, _frozen_param433, buf126, _frozen_param426, buf116, buf138, buf137)
    print("done cpp_fused__to_copy_add_mul_view_39")
    del buf126
    buf139 = torch.ops.mkldnn._convolution_pointwise(buf138, _frozen_param645, _frozen_param434, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf139, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf140 = buf130; del buf130  # reuse
    buf141 = buf129; del buf129  # reuse
    buf143 = reinterpret_tensor(buf138, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf138  # reuse
    print("enter cpp_fused_native_layer_norm_40")
    cpp_fused_native_layer_norm_40(buf139, _frozen_param42, _frozen_param43, buf140, buf141, buf143)
    print("done cpp_fused_native_layer_norm_40")
    buf145 = buf134; del buf134  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_41")
    cpp_fused_gelu_native_layer_norm_41(buf143, constant205, _frozen_param436, buf145)
    print("done cpp_fused_gelu_native_layer_norm_41")
    buf147 = reinterpret_tensor(buf143, (324, 512), (512, 1), 0); del buf143  # reuse
    buf148 = buf139; del buf139  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_42")
    cpp_fused__to_copy_add_mul_view_42(buf145, constant206, _frozen_param438, _frozen_param440, buf137, buf148)
    print("done cpp_fused__to_copy_add_mul_view_42")
    buf149 = torch.ops.mkldnn._convolution_pointwise(buf148, _frozen_param648, _frozen_param441, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf149, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf150 = buf141; del buf141  # reuse
    buf151 = buf140; del buf140  # reuse
    buf153 = reinterpret_tensor(buf148, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf148  # reuse
    print("enter cpp_fused_native_layer_norm_43")
    cpp_fused_native_layer_norm_43(buf149, _frozen_param45, _frozen_param46, buf150, buf151, buf153)
    print("done cpp_fused_native_layer_norm_43")
    del buf149
    buf155 = buf145; del buf145  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_44")
    cpp_fused_gelu_native_layer_norm_44(buf153, constant214, _frozen_param443, buf155)
    print("done cpp_fused_gelu_native_layer_norm_44")
    buf158 = buf116; del buf116  # reuse
    buf159 = reinterpret_tensor(buf153, (1, 512, 18, 18), (165888, 1, 9216, 512), 0); del buf153  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_45")
    cpp_fused__to_copy_add_mul_view_45(buf155, constant215, _frozen_param445, _frozen_param447, buf147, _frozen_param440, buf137, buf159, buf158)
    print("done cpp_fused__to_copy_add_mul_view_45")
    del buf147
    buf160 = torch.ops.mkldnn._convolution_pointwise(buf159, _frozen_param651, _frozen_param448, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf160, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf161 = buf151; del buf151  # reuse
    buf162 = buf150; del buf150  # reuse
    buf164 = reinterpret_tensor(buf159, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf159  # reuse
    print("enter cpp_fused_native_layer_norm_46")
    cpp_fused_native_layer_norm_46(buf160, _frozen_param48, _frozen_param49, buf161, buf162, buf164)
    print("done cpp_fused_native_layer_norm_46")
    buf166 = buf155; del buf155  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_47")
    cpp_fused_gelu_native_layer_norm_47(buf164, constant223, _frozen_param450, buf166)
    print("done cpp_fused_gelu_native_layer_norm_47")
    buf168 = reinterpret_tensor(buf164, (324, 512), (512, 1), 0); del buf164  # reuse
    buf169 = buf160; del buf160  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_48")
    cpp_fused__to_copy_add_mul_view_48(buf166, constant224, _frozen_param452, _frozen_param454, buf158, buf169)
    print("done cpp_fused__to_copy_add_mul_view_48")
    buf170 = torch.ops.mkldnn._convolution_pointwise(buf169, _frozen_param654, _frozen_param455, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf170, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf171 = buf162; del buf162  # reuse
    buf172 = buf161; del buf161  # reuse
    buf174 = reinterpret_tensor(buf169, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf169  # reuse
    print("enter cpp_fused_native_layer_norm_49")
    cpp_fused_native_layer_norm_49(buf170, _frozen_param51, _frozen_param52, buf171, buf172, buf174)
    print("done cpp_fused_native_layer_norm_49")
    del buf170
    buf176 = buf166; del buf166  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_50")
    cpp_fused_gelu_native_layer_norm_50(buf174, constant232, _frozen_param457, buf176)
    print("done cpp_fused_gelu_native_layer_norm_50")
    buf179 = buf137; del buf137  # reuse
    buf180 = reinterpret_tensor(buf174, (1, 512, 18, 18), (165888, 1, 9216, 512), 0); del buf174  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_51")
    cpp_fused__to_copy_add_mul_view_51(buf176, constant233, _frozen_param459, _frozen_param461, buf168, _frozen_param454, buf158, buf180, buf179)
    print("done cpp_fused__to_copy_add_mul_view_51")
    del buf168
    buf181 = torch.ops.mkldnn._convolution_pointwise(buf180, _frozen_param657, _frozen_param462, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf181, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf182 = buf172; del buf172  # reuse
    buf183 = buf171; del buf171  # reuse
    buf185 = reinterpret_tensor(buf180, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf180  # reuse
    print("enter cpp_fused_native_layer_norm_52")
    cpp_fused_native_layer_norm_52(buf181, _frozen_param54, _frozen_param55, buf182, buf183, buf185)
    print("done cpp_fused_native_layer_norm_52")
    buf187 = buf176; del buf176  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_53")
    cpp_fused_gelu_native_layer_norm_53(buf185, constant241, _frozen_param464, buf187)
    print("done cpp_fused_gelu_native_layer_norm_53")
    buf189 = reinterpret_tensor(buf185, (324, 512), (512, 1), 0); del buf185  # reuse
    buf190 = buf181; del buf181  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_54")
    cpp_fused__to_copy_add_mul_view_54(buf187, constant242, _frozen_param466, _frozen_param468, buf179, buf190)
    print("done cpp_fused__to_copy_add_mul_view_54")
    buf191 = torch.ops.mkldnn._convolution_pointwise(buf190, _frozen_param660, _frozen_param469, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf191, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf192 = buf183; del buf183  # reuse
    buf193 = buf182; del buf182  # reuse
    buf195 = reinterpret_tensor(buf190, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf190  # reuse
    print("enter cpp_fused_native_layer_norm_55")
    cpp_fused_native_layer_norm_55(buf191, _frozen_param57, _frozen_param58, buf192, buf193, buf195)
    print("done cpp_fused_native_layer_norm_55")
    del buf191
    buf197 = buf187; del buf187  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_56")
    cpp_fused_gelu_native_layer_norm_56(buf195, constant250, _frozen_param471, buf197)
    print("done cpp_fused_gelu_native_layer_norm_56")
    buf200 = buf158; del buf158  # reuse
    buf201 = reinterpret_tensor(buf195, (1, 512, 18, 18), (165888, 1, 9216, 512), 0); del buf195  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_57")
    cpp_fused__to_copy_add_mul_view_57(buf197, constant251, _frozen_param473, _frozen_param475, buf189, _frozen_param468, buf179, buf201, buf200)
    print("done cpp_fused__to_copy_add_mul_view_57")
    del buf189
    buf202 = torch.ops.mkldnn._convolution_pointwise(buf201, _frozen_param663, _frozen_param476, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf202, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf203 = buf193; del buf193  # reuse
    buf204 = buf192; del buf192  # reuse
    buf206 = reinterpret_tensor(buf201, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf201  # reuse
    print("enter cpp_fused_native_layer_norm_58")
    cpp_fused_native_layer_norm_58(buf202, _frozen_param60, _frozen_param61, buf203, buf204, buf206)
    print("done cpp_fused_native_layer_norm_58")
    buf208 = buf197; del buf197  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_59")
    cpp_fused_gelu_native_layer_norm_59(buf206, constant259, _frozen_param478, buf208)
    print("done cpp_fused_gelu_native_layer_norm_59")
    buf210 = reinterpret_tensor(buf206, (324, 512), (512, 1), 0); del buf206  # reuse
    buf211 = buf202; del buf202  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_60")
    cpp_fused__to_copy_add_mul_view_60(buf208, constant260, _frozen_param480, _frozen_param482, buf200, buf211)
    print("done cpp_fused__to_copy_add_mul_view_60")
    buf212 = torch.ops.mkldnn._convolution_pointwise(buf211, _frozen_param666, _frozen_param483, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf212, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf213 = buf204; del buf204  # reuse
    buf214 = buf203; del buf203  # reuse
    buf216 = reinterpret_tensor(buf211, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf211  # reuse
    print("enter cpp_fused_native_layer_norm_61")
    cpp_fused_native_layer_norm_61(buf212, _frozen_param63, _frozen_param64, buf213, buf214, buf216)
    print("done cpp_fused_native_layer_norm_61")
    del buf212
    buf218 = buf208; del buf208  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_62")
    cpp_fused_gelu_native_layer_norm_62(buf216, constant268, _frozen_param485, buf218)
    print("done cpp_fused_gelu_native_layer_norm_62")
    buf221 = buf179; del buf179  # reuse
    buf222 = reinterpret_tensor(buf216, (1, 512, 18, 18), (165888, 1, 9216, 512), 0); del buf216  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_63")
    cpp_fused__to_copy_add_mul_view_63(buf218, constant269, _frozen_param487, _frozen_param489, buf210, _frozen_param482, buf200, buf222, buf221)
    print("done cpp_fused__to_copy_add_mul_view_63")
    del buf210
    buf223 = torch.ops.mkldnn._convolution_pointwise(buf222, _frozen_param669, _frozen_param490, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf223, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf224 = buf214; del buf214  # reuse
    buf225 = buf213; del buf213  # reuse
    buf227 = reinterpret_tensor(buf222, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf222  # reuse
    print("enter cpp_fused_native_layer_norm_64")
    cpp_fused_native_layer_norm_64(buf223, _frozen_param66, _frozen_param67, buf224, buf225, buf227)
    print("done cpp_fused_native_layer_norm_64")
    buf229 = buf218; del buf218  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_65")
    cpp_fused_gelu_native_layer_norm_65(buf227, constant277, _frozen_param492, buf229)
    print("done cpp_fused_gelu_native_layer_norm_65")
    buf231 = reinterpret_tensor(buf227, (324, 512), (512, 1), 0); del buf227  # reuse
    buf232 = buf223; del buf223  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_66")
    cpp_fused__to_copy_add_mul_view_66(buf229, constant278, _frozen_param494, _frozen_param496, buf221, buf232)
    print("done cpp_fused__to_copy_add_mul_view_66")
    buf233 = torch.ops.mkldnn._convolution_pointwise(buf232, _frozen_param672, _frozen_param497, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf233, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf234 = buf225; del buf225  # reuse
    buf235 = buf224; del buf224  # reuse
    buf237 = reinterpret_tensor(buf232, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf232  # reuse
    print("enter cpp_fused_native_layer_norm_67")
    cpp_fused_native_layer_norm_67(buf233, _frozen_param69, _frozen_param70, buf234, buf235, buf237)
    print("done cpp_fused_native_layer_norm_67")
    del buf233
    buf239 = buf229; del buf229  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_68")
    cpp_fused_gelu_native_layer_norm_68(buf237, constant286, _frozen_param499, buf239)
    print("done cpp_fused_gelu_native_layer_norm_68")
    buf242 = buf200; del buf200  # reuse
    buf243 = reinterpret_tensor(buf237, (1, 512, 18, 18), (165888, 1, 9216, 512), 0); del buf237  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_69")
    cpp_fused__to_copy_add_mul_view_69(buf239, constant287, _frozen_param501, _frozen_param503, buf231, _frozen_param496, buf221, buf243, buf242)
    print("done cpp_fused__to_copy_add_mul_view_69")
    del buf231
    buf244 = torch.ops.mkldnn._convolution_pointwise(buf243, _frozen_param675, _frozen_param504, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf244, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf245 = buf235; del buf235  # reuse
    buf246 = buf234; del buf234  # reuse
    buf248 = reinterpret_tensor(buf243, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf243  # reuse
    print("enter cpp_fused_native_layer_norm_70")
    cpp_fused_native_layer_norm_70(buf244, _frozen_param72, _frozen_param73, buf245, buf246, buf248)
    print("done cpp_fused_native_layer_norm_70")
    buf250 = buf239; del buf239  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_71")
    cpp_fused_gelu_native_layer_norm_71(buf248, constant295, _frozen_param506, buf250)
    print("done cpp_fused_gelu_native_layer_norm_71")
    buf252 = reinterpret_tensor(buf248, (324, 512), (512, 1), 0); del buf248  # reuse
    buf253 = buf244; del buf244  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_72")
    cpp_fused__to_copy_add_mul_view_72(buf250, constant296, _frozen_param508, _frozen_param510, buf242, buf253)
    print("done cpp_fused__to_copy_add_mul_view_72")
    buf254 = torch.ops.mkldnn._convolution_pointwise(buf253, _frozen_param678, _frozen_param511, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf254, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf255 = buf246; del buf246  # reuse
    buf256 = buf245; del buf245  # reuse
    buf258 = reinterpret_tensor(buf253, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf253  # reuse
    print("enter cpp_fused_native_layer_norm_73")
    cpp_fused_native_layer_norm_73(buf254, _frozen_param75, _frozen_param76, buf255, buf256, buf258)
    print("done cpp_fused_native_layer_norm_73")
    del buf254
    buf260 = buf250; del buf250  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_74")
    cpp_fused_gelu_native_layer_norm_74(buf258, constant304, _frozen_param513, buf260)
    print("done cpp_fused_gelu_native_layer_norm_74")
    buf263 = buf221; del buf221  # reuse
    buf264 = reinterpret_tensor(buf258, (1, 512, 18, 18), (165888, 1, 9216, 512), 0); del buf258  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_75")
    cpp_fused__to_copy_add_mul_view_75(buf260, constant305, _frozen_param515, _frozen_param517, buf252, _frozen_param510, buf242, buf264, buf263)
    print("done cpp_fused__to_copy_add_mul_view_75")
    del buf252
    buf265 = torch.ops.mkldnn._convolution_pointwise(buf264, _frozen_param681, _frozen_param518, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf265, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf266 = buf256; del buf256  # reuse
    buf267 = buf255; del buf255  # reuse
    buf269 = reinterpret_tensor(buf264, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf264  # reuse
    print("enter cpp_fused_native_layer_norm_76")
    cpp_fused_native_layer_norm_76(buf265, _frozen_param78, _frozen_param79, buf266, buf267, buf269)
    print("done cpp_fused_native_layer_norm_76")
    buf271 = buf260; del buf260  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_77")
    cpp_fused_gelu_native_layer_norm_77(buf269, constant313, _frozen_param520, buf271)
    print("done cpp_fused_gelu_native_layer_norm_77")
    buf273 = reinterpret_tensor(buf269, (324, 512), (512, 1), 0); del buf269  # reuse
    buf274 = buf265; del buf265  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_78")
    cpp_fused__to_copy_add_mul_view_78(buf271, constant314, _frozen_param522, _frozen_param524, buf263, buf274)
    print("done cpp_fused__to_copy_add_mul_view_78")
    buf275 = torch.ops.mkldnn._convolution_pointwise(buf274, _frozen_param684, _frozen_param525, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf275, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf276 = buf267; del buf267  # reuse
    buf277 = buf266; del buf266  # reuse
    buf279 = reinterpret_tensor(buf274, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf274  # reuse
    print("enter cpp_fused_native_layer_norm_79")
    cpp_fused_native_layer_norm_79(buf275, _frozen_param81, _frozen_param82, buf276, buf277, buf279)
    print("done cpp_fused_native_layer_norm_79")
    del buf275
    buf281 = buf271; del buf271  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_80")
    cpp_fused_gelu_native_layer_norm_80(buf279, constant322, _frozen_param527, buf281)
    print("done cpp_fused_gelu_native_layer_norm_80")
    buf284 = buf242; del buf242  # reuse
    buf285 = reinterpret_tensor(buf279, (1, 512, 18, 18), (165888, 1, 9216, 512), 0); del buf279  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_81")
    cpp_fused__to_copy_add_mul_view_81(buf281, constant323, _frozen_param529, _frozen_param531, buf273, _frozen_param524, buf263, buf285, buf284)
    print("done cpp_fused__to_copy_add_mul_view_81")
    del buf273
    buf286 = torch.ops.mkldnn._convolution_pointwise(buf285, _frozen_param687, _frozen_param532, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf286, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf287 = buf277; del buf277  # reuse
    buf288 = buf276; del buf276  # reuse
    buf290 = reinterpret_tensor(buf285, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf285  # reuse
    print("enter cpp_fused_native_layer_norm_82")
    cpp_fused_native_layer_norm_82(buf286, _frozen_param84, _frozen_param85, buf287, buf288, buf290)
    print("done cpp_fused_native_layer_norm_82")
    buf292 = buf281; del buf281  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_83")
    cpp_fused_gelu_native_layer_norm_83(buf290, constant331, _frozen_param534, buf292)
    print("done cpp_fused_gelu_native_layer_norm_83")
    buf294 = reinterpret_tensor(buf290, (324, 512), (512, 1), 0); del buf290  # reuse
    buf295 = buf286; del buf286  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_84")
    cpp_fused__to_copy_add_mul_view_84(buf292, constant332, _frozen_param536, _frozen_param538, buf284, buf295)
    print("done cpp_fused__to_copy_add_mul_view_84")
    buf296 = torch.ops.mkldnn._convolution_pointwise(buf295, _frozen_param690, _frozen_param539, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf296, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf297 = buf288; del buf288  # reuse
    buf298 = buf287; del buf287  # reuse
    buf300 = reinterpret_tensor(buf295, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf295  # reuse
    print("enter cpp_fused_native_layer_norm_85")
    cpp_fused_native_layer_norm_85(buf296, _frozen_param87, _frozen_param88, buf297, buf298, buf300)
    print("done cpp_fused_native_layer_norm_85")
    del buf296
    buf302 = buf292; del buf292  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_86")
    cpp_fused_gelu_native_layer_norm_86(buf300, constant340, _frozen_param541, buf302)
    print("done cpp_fused_gelu_native_layer_norm_86")
    buf305 = buf263; del buf263  # reuse
    buf306 = reinterpret_tensor(buf300, (1, 512, 18, 18), (165888, 1, 9216, 512), 0); del buf300  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_87")
    cpp_fused__to_copy_add_mul_view_87(buf302, constant341, _frozen_param543, _frozen_param545, buf294, _frozen_param538, buf284, buf306, buf305)
    print("done cpp_fused__to_copy_add_mul_view_87")
    del buf294
    buf307 = torch.ops.mkldnn._convolution_pointwise(buf306, _frozen_param693, _frozen_param546, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf307, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf308 = buf298; del buf298  # reuse
    buf309 = buf297; del buf297  # reuse
    buf311 = reinterpret_tensor(buf306, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf306  # reuse
    print("enter cpp_fused_native_layer_norm_88")
    cpp_fused_native_layer_norm_88(buf307, _frozen_param90, _frozen_param91, buf308, buf309, buf311)
    print("done cpp_fused_native_layer_norm_88")
    buf313 = buf302; del buf302  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_89")
    cpp_fused_gelu_native_layer_norm_89(buf311, constant349, _frozen_param548, buf313)
    print("done cpp_fused_gelu_native_layer_norm_89")
    buf315 = reinterpret_tensor(buf311, (324, 512), (512, 1), 0); del buf311  # reuse
    buf316 = buf307; del buf307  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_90")
    cpp_fused__to_copy_add_mul_view_90(buf313, constant350, _frozen_param550, _frozen_param552, buf305, buf316)
    print("done cpp_fused__to_copy_add_mul_view_90")
    buf317 = torch.ops.mkldnn._convolution_pointwise(buf316, _frozen_param696, _frozen_param553, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf317, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf318 = buf309; del buf309  # reuse
    buf319 = buf308; del buf308  # reuse
    buf321 = reinterpret_tensor(buf316, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf316  # reuse
    print("enter cpp_fused_native_layer_norm_91")
    cpp_fused_native_layer_norm_91(buf317, _frozen_param93, _frozen_param94, buf318, buf319, buf321)
    print("done cpp_fused_native_layer_norm_91")
    del buf317
    buf323 = buf313; del buf313  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_92")
    cpp_fused_gelu_native_layer_norm_92(buf321, constant358, _frozen_param555, buf323)
    print("done cpp_fused_gelu_native_layer_norm_92")
    buf326 = buf284; del buf284  # reuse
    buf327 = reinterpret_tensor(buf321, (1, 512, 18, 18), (165888, 1, 9216, 512), 0); del buf321  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_93")
    cpp_fused__to_copy_add_mul_view_93(buf323, constant359, _frozen_param557, _frozen_param559, buf315, _frozen_param552, buf305, buf327, buf326)
    print("done cpp_fused__to_copy_add_mul_view_93")
    del buf315
    buf328 = torch.ops.mkldnn._convolution_pointwise(buf327, _frozen_param699, _frozen_param560, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf328, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf329 = buf319; del buf319  # reuse
    buf330 = buf318; del buf318  # reuse
    buf332 = reinterpret_tensor(buf327, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf327  # reuse
    print("enter cpp_fused_native_layer_norm_94")
    cpp_fused_native_layer_norm_94(buf328, _frozen_param96, _frozen_param97, buf329, buf330, buf332)
    print("done cpp_fused_native_layer_norm_94")
    buf334 = buf323; del buf323  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_95")
    cpp_fused_gelu_native_layer_norm_95(buf332, constant367, _frozen_param562, buf334)
    print("done cpp_fused_gelu_native_layer_norm_95")
    buf336 = reinterpret_tensor(buf332, (324, 512), (512, 1), 0); del buf332  # reuse
    buf337 = buf328; del buf328  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_96")
    cpp_fused__to_copy_add_mul_view_96(buf334, constant368, _frozen_param564, _frozen_param566, buf326, buf337)
    print("done cpp_fused__to_copy_add_mul_view_96")
    buf338 = torch.ops.mkldnn._convolution_pointwise(buf337, _frozen_param702, _frozen_param567, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf338, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf339 = buf330; del buf330  # reuse
    buf340 = buf329; del buf329  # reuse
    buf342 = reinterpret_tensor(buf337, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf337  # reuse
    print("enter cpp_fused_native_layer_norm_97")
    cpp_fused_native_layer_norm_97(buf338, _frozen_param99, _frozen_param100, buf339, buf340, buf342)
    print("done cpp_fused_native_layer_norm_97")
    del buf338
    buf344 = buf334; del buf334  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_98")
    cpp_fused_gelu_native_layer_norm_98(buf342, constant376, _frozen_param569, buf344)
    print("done cpp_fused_gelu_native_layer_norm_98")
    buf347 = buf305; del buf305  # reuse
    buf348 = reinterpret_tensor(buf342, (1, 512, 18, 18), (165888, 1, 9216, 512), 0); del buf342  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_99")
    cpp_fused__to_copy_add_mul_view_99(buf344, constant377, _frozen_param571, _frozen_param573, buf336, _frozen_param566, buf326, buf348, buf347)
    print("done cpp_fused__to_copy_add_mul_view_99")
    del buf326
    del buf336
    buf349 = torch.ops.mkldnn._convolution_pointwise(buf348, _frozen_param705, _frozen_param574, [3, 3], [1, 1], [1, 1], 512, 'none', [None], '')
    assert_size_stride(buf349, (1, 512, 18, 18), (165888, 1, 9216, 512))
    buf350 = buf340; del buf340  # reuse
    buf351 = buf339; del buf339  # reuse
    buf353 = reinterpret_tensor(buf348, (1, 18, 18, 512), (165888, 9216, 512, 1), 0); del buf348  # reuse
    print("enter cpp_fused_native_layer_norm_100")
    cpp_fused_native_layer_norm_100(buf349, _frozen_param102, _frozen_param103, buf350, buf351, buf353)
    print("done cpp_fused_native_layer_norm_100")
    buf355 = buf344; del buf344  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_101")
    cpp_fused_gelu_native_layer_norm_101(buf353, constant383, _frozen_param576, buf355)
    print("done cpp_fused_gelu_native_layer_norm_101")
    buf357 = reinterpret_tensor(buf353, (324, 512), (512, 1), 0); del buf353  # reuse
    print("enter cpp_fused_view_102")
    cpp_fused_view_102(buf355, constant384, _frozen_param578, buf357)
    print("done cpp_fused_view_102")
    del buf355
    buf358 = buf351; del buf351  # reuse
    buf359 = buf350; del buf350  # reuse
    buf361 = buf349; del buf349  # reuse
    print("enter cpp_fused__to_copy_native_layer_norm_103")
    cpp_fused__to_copy_native_layer_norm_103(buf357, _frozen_param580, buf347, _frozen_param105, _frozen_param106, buf358, buf359, buf361)
    print("done cpp_fused__to_copy_native_layer_norm_103")
    del buf347
    del buf357
    del buf358
    del buf359
    buf362 = torch.ops.mkldnn._convolution_pointwise(buf361, _frozen_param708, _frozen_param581, [0, 0], [2, 2], [1, 1], 1, 'none', [None], '')
    assert_size_stride(buf362, (1, 1024, 9, 9), (82944, 1, 9216, 1024))
    del buf361
    buf363 = torch.ops.mkldnn._convolution_pointwise(buf362, _frozen_param709, _frozen_param583, [3, 3], [1, 1], [1, 1], 1024, 'none', [None], '')
    assert_size_stride(buf363, (1, 1024, 9, 9), (82944, 1, 9216, 1024))
    buf364 = empty_strided_cpu((1, 9, 9, 1), (81, 9, 1, 81), torch.float32)
    buf365 = empty_strided_cpu((1, 9, 9, 1), (81, 9, 1, 81), torch.float32)
    buf367 = empty_strided_cpu((1, 9, 9, 1024), (82944, 9216, 1024, 1), torch.bfloat16)
    print("enter cpp_fused_native_layer_norm_104")
    cpp_fused_native_layer_norm_104(buf363, _frozen_param107, _frozen_param108, buf364, buf365, buf367)
    print("done cpp_fused_native_layer_norm_104")
    buf369 = reinterpret_tensor(buf74, (81, 4096), (4096, 1), 0); del buf74  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_105")
    cpp_fused_gelu_native_layer_norm_105(buf367, constant396, _frozen_param585, buf369)
    print("done cpp_fused_gelu_native_layer_norm_105")
    buf371 = reinterpret_tensor(buf367, (81, 1024), (1024, 1), 0); del buf367  # reuse
    buf372 = buf363; del buf363  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_106")
    cpp_fused__to_copy_add_mul_view_106(buf369, constant397, _frozen_param587, _frozen_param589, buf362, buf372)
    print("done cpp_fused__to_copy_add_mul_view_106")
    buf373 = torch.ops.mkldnn._convolution_pointwise(buf372, _frozen_param712, _frozen_param590, [3, 3], [1, 1], [1, 1], 1024, 'none', [None], '')
    assert_size_stride(buf373, (1, 1024, 9, 9), (82944, 1, 9216, 1024))
    buf374 = buf365; del buf365  # reuse
    buf375 = buf364; del buf364  # reuse
    buf377 = reinterpret_tensor(buf372, (1, 9, 9, 1024), (82944, 9216, 1024, 1), 0); del buf372  # reuse
    print("enter cpp_fused_native_layer_norm_107")
    cpp_fused_native_layer_norm_107(buf373, _frozen_param110, _frozen_param111, buf374, buf375, buf377)
    print("done cpp_fused_native_layer_norm_107")
    del buf373
    buf379 = buf369; del buf369  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_108")
    cpp_fused_gelu_native_layer_norm_108(buf377, constant405, _frozen_param592, buf379)
    print("done cpp_fused_gelu_native_layer_norm_108")
    buf382 = empty_strided_cpu((1, 1024, 9, 9), (82944, 1, 9216, 1024), torch.float32)
    buf383 = reinterpret_tensor(buf377, (1, 1024, 9, 9), (82944, 1, 9216, 1024), 0); del buf377  # reuse
    print("enter cpp_fused__to_copy_add_mul_view_109")
    cpp_fused__to_copy_add_mul_view_109(buf379, constant406, _frozen_param594, _frozen_param596, buf371, _frozen_param589, buf362, buf383, buf382)
    print("done cpp_fused__to_copy_add_mul_view_109")
    del buf362
    del buf371
    buf384 = torch.ops.mkldnn._convolution_pointwise(buf383, _frozen_param715, _frozen_param597, [3, 3], [1, 1], [1, 1], 1024, 'none', [None], '')
    assert_size_stride(buf384, (1, 1024, 9, 9), (82944, 1, 9216, 1024))
    buf385 = buf375; del buf375  # reuse
    buf386 = buf374; del buf374  # reuse
    buf388 = reinterpret_tensor(buf383, (1, 9, 9, 1024), (82944, 9216, 1024, 1), 0); del buf383  # reuse
    print("enter cpp_fused_native_layer_norm_110")
    cpp_fused_native_layer_norm_110(buf384, _frozen_param113, _frozen_param114, buf385, buf386, buf388)
    print("done cpp_fused_native_layer_norm_110")
    del buf384
    del buf385
    del buf386
    buf390 = buf379; del buf379  # reuse
    print("enter cpp_fused_gelu_native_layer_norm_111")
    cpp_fused_gelu_native_layer_norm_111(buf388, constant412, _frozen_param599, buf390)
    print("done cpp_fused_gelu_native_layer_norm_111")
    buf392 = reinterpret_tensor(buf388, (81, 1024), (1024, 1), 0); del buf388  # reuse
    print("enter cpp_fused_view_112")
    cpp_fused_view_112(buf390, constant413, _frozen_param601, buf392)
    print("done cpp_fused_view_112")
    del buf390
    buf393 = empty_strided_cpu((1, 1024, 1, 1), (1024, 1, 1024, 1024), torch.float32)
    buf394 = reinterpret_tensor(buf393, (1, 1024, 1, 1), (1024, 1, 1, 1), 0); del buf393  # reuse
    buf395 = empty_strided_cpu((1, 1, 1, 1), (1, 1, 1, 1), torch.float32)
    buf396 = empty_strided_cpu((1, 1, 1, 1), (1, 1, 1, 1), torch.float32)
    buf398 = empty_strided_cpu((1, 1024), (1024, 1), torch.bfloat16)
    print("enter cpp_fused__to_copy_add_mean_mul_native_layer_norm_113")
    cpp_fused__to_copy_add_mean_mul_native_layer_norm_113(buf394, buf392, _frozen_param603, buf382, _frozen_param116, _frozen_param117, buf395, buf396, buf398)
    print("done cpp_fused__to_copy_add_mean_mul_native_layer_norm_113")
    del buf382
    del buf392
    del buf394
    del buf395
    del buf396
    buf399 = empty_strided_cpu((1, 1000), (1000, 1), torch.bfloat16)
    print("enter cpp_fused__to_copy_addmm_114")
    cpp_fused__to_copy_addmm_114(buf398, constant416, _frozen_param604, buf399)
    print("done cpp_fused__to_copy_addmm_114")
    return (buf399, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    global _frozen_param0
    _frozen_param0 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param1
    _frozen_param1 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param2
    _frozen_param2 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param3
    _frozen_param3 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param5
    _frozen_param5 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param6
    _frozen_param6 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param8
    _frozen_param8 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param9
    _frozen_param9 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param11
    _frozen_param11 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param12
    _frozen_param12 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param13
    _frozen_param13 = rand_strided((256, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param14
    _frozen_param14 = rand_strided((256, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param16
    _frozen_param16 = rand_strided((256, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param17
    _frozen_param17 = rand_strided((256, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param19
    _frozen_param19 = rand_strided((256, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param20
    _frozen_param20 = rand_strided((256, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param22
    _frozen_param22 = rand_strided((256, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param23
    _frozen_param23 = rand_strided((256, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param24
    _frozen_param24 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param25
    _frozen_param25 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param27
    _frozen_param27 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param28
    _frozen_param28 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param30
    _frozen_param30 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param31
    _frozen_param31 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param33
    _frozen_param33 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param34
    _frozen_param34 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param36
    _frozen_param36 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param37
    _frozen_param37 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param39
    _frozen_param39 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param40
    _frozen_param40 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param42
    _frozen_param42 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param43
    _frozen_param43 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param45
    _frozen_param45 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param46
    _frozen_param46 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param48
    _frozen_param48 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param49
    _frozen_param49 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param51
    _frozen_param51 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param52
    _frozen_param52 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param54
    _frozen_param54 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param55
    _frozen_param55 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param57
    _frozen_param57 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param58
    _frozen_param58 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param60
    _frozen_param60 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param61
    _frozen_param61 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param63
    _frozen_param63 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param64
    _frozen_param64 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param66
    _frozen_param66 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param67
    _frozen_param67 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param69
    _frozen_param69 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param70
    _frozen_param70 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param72
    _frozen_param72 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param73
    _frozen_param73 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param75
    _frozen_param75 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param76
    _frozen_param76 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param78
    _frozen_param78 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param79
    _frozen_param79 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param81
    _frozen_param81 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param82
    _frozen_param82 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param84
    _frozen_param84 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param85
    _frozen_param85 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param87
    _frozen_param87 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param88
    _frozen_param88 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param90
    _frozen_param90 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param91
    _frozen_param91 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param93
    _frozen_param93 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param94
    _frozen_param94 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param96
    _frozen_param96 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param97
    _frozen_param97 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param99
    _frozen_param99 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param100
    _frozen_param100 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param102
    _frozen_param102 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param103
    _frozen_param103 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param105
    _frozen_param105 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param106
    _frozen_param106 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param107
    _frozen_param107 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param108
    _frozen_param108 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param110
    _frozen_param110 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param111
    _frozen_param111 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param113
    _frozen_param113 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param114
    _frozen_param114 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param116
    _frozen_param116 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param117
    _frozen_param117 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.float32)
    global _frozen_param344
    _frozen_param344 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param606
    _frozen_param606 = rand_strided((128, 3, 4, 4), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param346
    _frozen_param346 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param607
    _frozen_param607 = rand_strided((128, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param348
    _frozen_param348 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param608
    _frozen_param608 = rand_strided((512, 128), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param350
    _frozen_param350 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param609
    _frozen_param609 = rand_strided((128, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param352
    _frozen_param352 = rand_strided((1, 128, 1, 1), (128, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param353
    _frozen_param353 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param610
    _frozen_param610 = rand_strided((128, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant93
    constant93 = rand_strided((16, 128, 32), (4096, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant94
    constant94 = rand_strided((4, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param355
    _frozen_param355 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param611
    _frozen_param611 = rand_strided((512, 128), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param357
    _frozen_param357 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param612
    _frozen_param612 = rand_strided((128, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param359
    _frozen_param359 = rand_strided((1, 128, 1, 1), (128, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param360
    _frozen_param360 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param613
    _frozen_param613 = rand_strided((128, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant102
    constant102 = rand_strided((16, 128, 32), (4096, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant103
    constant103 = rand_strided((4, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param362
    _frozen_param362 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param614
    _frozen_param614 = rand_strided((512, 128), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param364
    _frozen_param364 = rand_strided((128, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param615
    _frozen_param615 = rand_strided((128, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param366
    _frozen_param366 = rand_strided((1, 128, 1, 1), (128, 1, 1, 1), device='cpu', dtype=torch.float32)
    global constant109
    constant109 = rand_strided((16, 128, 32), (4096, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant110
    constant110 = rand_strided((4, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param367
    _frozen_param367 = rand_strided((256, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param616
    _frozen_param616 = rand_strided((256, 128, 2, 2), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param369
    _frozen_param369 = rand_strided((256, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param617
    _frozen_param617 = rand_strided((256, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param371
    _frozen_param371 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param618
    _frozen_param618 = rand_strided((1024, 256), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param373
    _frozen_param373 = rand_strided((256, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param619
    _frozen_param619 = rand_strided((256, 1024), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param375
    _frozen_param375 = rand_strided((1, 256, 1, 1), (256, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param376
    _frozen_param376 = rand_strided((256, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param620
    _frozen_param620 = rand_strided((256, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant122
    constant122 = rand_strided((64, 256, 16), (4096, 16, 1), device='cpu', dtype=torch.bfloat16)
    global constant123
    constant123 = rand_strided((16, 1024, 16), (16384, 16, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param378
    _frozen_param378 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param621
    _frozen_param621 = rand_strided((1024, 256), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param380
    _frozen_param380 = rand_strided((256, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param622
    _frozen_param622 = rand_strided((256, 1024), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param382
    _frozen_param382 = rand_strided((1, 256, 1, 1), (256, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param383
    _frozen_param383 = rand_strided((256, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param623
    _frozen_param623 = rand_strided((256, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant131
    constant131 = rand_strided((64, 256, 16), (4096, 16, 1), device='cpu', dtype=torch.bfloat16)
    global constant132
    constant132 = rand_strided((16, 1024, 16), (16384, 16, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param385
    _frozen_param385 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param624
    _frozen_param624 = rand_strided((1024, 256), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param387
    _frozen_param387 = rand_strided((256, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param625
    _frozen_param625 = rand_strided((256, 1024), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param389
    _frozen_param389 = rand_strided((1, 256, 1, 1), (256, 1, 1, 1), device='cpu', dtype=torch.float32)
    global constant138
    constant138 = rand_strided((64, 256, 16), (4096, 16, 1), device='cpu', dtype=torch.bfloat16)
    global constant139
    constant139 = rand_strided((16, 1024, 16), (16384, 16, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param390
    _frozen_param390 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param626
    _frozen_param626 = rand_strided((512, 256, 2, 2), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param392
    _frozen_param392 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param627
    _frozen_param627 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param394
    _frozen_param394 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param628
    _frozen_param628 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param396
    _frozen_param396 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param629
    _frozen_param629 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param398
    _frozen_param398 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param399
    _frozen_param399 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param630
    _frozen_param630 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant151
    constant151 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant152
    constant152 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param401
    _frozen_param401 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param631
    _frozen_param631 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param403
    _frozen_param403 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param632
    _frozen_param632 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param405
    _frozen_param405 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param406
    _frozen_param406 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param633
    _frozen_param633 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant160
    constant160 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant161
    constant161 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param408
    _frozen_param408 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param634
    _frozen_param634 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param410
    _frozen_param410 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param635
    _frozen_param635 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param412
    _frozen_param412 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param413
    _frozen_param413 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param636
    _frozen_param636 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant169
    constant169 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant170
    constant170 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param415
    _frozen_param415 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param637
    _frozen_param637 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param417
    _frozen_param417 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param638
    _frozen_param638 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param419
    _frozen_param419 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param420
    _frozen_param420 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param639
    _frozen_param639 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant178
    constant178 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant179
    constant179 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param422
    _frozen_param422 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param640
    _frozen_param640 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param424
    _frozen_param424 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param641
    _frozen_param641 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param426
    _frozen_param426 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param427
    _frozen_param427 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param642
    _frozen_param642 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant187
    constant187 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant188
    constant188 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param429
    _frozen_param429 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param643
    _frozen_param643 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param431
    _frozen_param431 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param644
    _frozen_param644 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param433
    _frozen_param433 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param434
    _frozen_param434 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param645
    _frozen_param645 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant196
    constant196 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant197
    constant197 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param436
    _frozen_param436 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param646
    _frozen_param646 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param438
    _frozen_param438 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param647
    _frozen_param647 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param440
    _frozen_param440 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param441
    _frozen_param441 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param648
    _frozen_param648 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant205
    constant205 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant206
    constant206 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param443
    _frozen_param443 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param649
    _frozen_param649 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param445
    _frozen_param445 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param650
    _frozen_param650 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param447
    _frozen_param447 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param448
    _frozen_param448 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param651
    _frozen_param651 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant214
    constant214 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant215
    constant215 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param450
    _frozen_param450 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param652
    _frozen_param652 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param452
    _frozen_param452 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param653
    _frozen_param653 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param454
    _frozen_param454 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param455
    _frozen_param455 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param654
    _frozen_param654 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant223
    constant223 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant224
    constant224 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param457
    _frozen_param457 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param655
    _frozen_param655 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param459
    _frozen_param459 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param656
    _frozen_param656 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param461
    _frozen_param461 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param462
    _frozen_param462 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param657
    _frozen_param657 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant232
    constant232 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant233
    constant233 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param464
    _frozen_param464 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param658
    _frozen_param658 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param466
    _frozen_param466 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param659
    _frozen_param659 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param468
    _frozen_param468 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param469
    _frozen_param469 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param660
    _frozen_param660 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant241
    constant241 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant242
    constant242 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param471
    _frozen_param471 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param661
    _frozen_param661 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param473
    _frozen_param473 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param662
    _frozen_param662 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param475
    _frozen_param475 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param476
    _frozen_param476 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param663
    _frozen_param663 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant250
    constant250 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant251
    constant251 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param478
    _frozen_param478 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param664
    _frozen_param664 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param480
    _frozen_param480 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param665
    _frozen_param665 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param482
    _frozen_param482 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param483
    _frozen_param483 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param666
    _frozen_param666 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant259
    constant259 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant260
    constant260 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param485
    _frozen_param485 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param667
    _frozen_param667 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param487
    _frozen_param487 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param668
    _frozen_param668 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param489
    _frozen_param489 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param490
    _frozen_param490 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param669
    _frozen_param669 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant268
    constant268 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant269
    constant269 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param492
    _frozen_param492 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param670
    _frozen_param670 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param494
    _frozen_param494 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param671
    _frozen_param671 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param496
    _frozen_param496 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param497
    _frozen_param497 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param672
    _frozen_param672 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant277
    constant277 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant278
    constant278 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param499
    _frozen_param499 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param673
    _frozen_param673 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param501
    _frozen_param501 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param674
    _frozen_param674 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param503
    _frozen_param503 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param504
    _frozen_param504 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param675
    _frozen_param675 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant286
    constant286 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant287
    constant287 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param506
    _frozen_param506 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param676
    _frozen_param676 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param508
    _frozen_param508 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param677
    _frozen_param677 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param510
    _frozen_param510 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param511
    _frozen_param511 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param678
    _frozen_param678 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant295
    constant295 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant296
    constant296 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param513
    _frozen_param513 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param679
    _frozen_param679 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param515
    _frozen_param515 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param680
    _frozen_param680 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param517
    _frozen_param517 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param518
    _frozen_param518 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param681
    _frozen_param681 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant304
    constant304 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant305
    constant305 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param520
    _frozen_param520 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param682
    _frozen_param682 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param522
    _frozen_param522 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param683
    _frozen_param683 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param524
    _frozen_param524 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param525
    _frozen_param525 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param684
    _frozen_param684 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant313
    constant313 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant314
    constant314 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param527
    _frozen_param527 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param685
    _frozen_param685 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param529
    _frozen_param529 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param686
    _frozen_param686 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param531
    _frozen_param531 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param532
    _frozen_param532 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param687
    _frozen_param687 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant322
    constant322 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant323
    constant323 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param534
    _frozen_param534 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param688
    _frozen_param688 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param536
    _frozen_param536 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param689
    _frozen_param689 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param538
    _frozen_param538 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param539
    _frozen_param539 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param690
    _frozen_param690 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant331
    constant331 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant332
    constant332 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param541
    _frozen_param541 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param691
    _frozen_param691 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param543
    _frozen_param543 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param692
    _frozen_param692 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param545
    _frozen_param545 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param546
    _frozen_param546 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param693
    _frozen_param693 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant340
    constant340 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant341
    constant341 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param548
    _frozen_param548 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param694
    _frozen_param694 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param550
    _frozen_param550 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param695
    _frozen_param695 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param552
    _frozen_param552 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param553
    _frozen_param553 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param696
    _frozen_param696 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant349
    constant349 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant350
    constant350 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param555
    _frozen_param555 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param697
    _frozen_param697 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param557
    _frozen_param557 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param698
    _frozen_param698 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param559
    _frozen_param559 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param560
    _frozen_param560 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param699
    _frozen_param699 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant358
    constant358 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant359
    constant359 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param562
    _frozen_param562 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param700
    _frozen_param700 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param564
    _frozen_param564 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param701
    _frozen_param701 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param566
    _frozen_param566 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param567
    _frozen_param567 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param702
    _frozen_param702 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant367
    constant367 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant368
    constant368 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param569
    _frozen_param569 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param703
    _frozen_param703 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param571
    _frozen_param571 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param704
    _frozen_param704 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param573
    _frozen_param573 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param574
    _frozen_param574 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param705
    _frozen_param705 = rand_strided((512, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant376
    constant376 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant377
    constant377 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param576
    _frozen_param576 = rand_strided((2048, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param706
    _frozen_param706 = rand_strided((2048, 512), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param578
    _frozen_param578 = rand_strided((512, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param707
    _frozen_param707 = rand_strided((512, 2048), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param580
    _frozen_param580 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cpu', dtype=torch.float32)
    global constant383
    constant383 = rand_strided((64, 512, 32), (16384, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant384
    constant384 = rand_strided((16, 2048, 32), (65536, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param581
    _frozen_param581 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param708
    _frozen_param708 = rand_strided((1024, 512, 2, 2), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param583
    _frozen_param583 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param709
    _frozen_param709 = rand_strided((1024, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param585
    _frozen_param585 = rand_strided((4096, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param710
    _frozen_param710 = rand_strided((4096, 1024), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param587
    _frozen_param587 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param711
    _frozen_param711 = rand_strided((1024, 4096), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param589
    _frozen_param589 = rand_strided((1, 1024, 1, 1), (1024, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param590
    _frozen_param590 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param712
    _frozen_param712 = rand_strided((1024, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant396
    constant396 = rand_strided((128, 1024, 32), (32768, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant397
    constant397 = rand_strided((32, 4096, 32), (131072, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param592
    _frozen_param592 = rand_strided((4096, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param713
    _frozen_param713 = rand_strided((4096, 1024), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param594
    _frozen_param594 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param714
    _frozen_param714 = rand_strided((1024, 4096), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param596
    _frozen_param596 = rand_strided((1, 1024, 1, 1), (1024, 1, 1, 1), device='cpu', dtype=torch.float32)
    global _frozen_param597
    _frozen_param597 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param715
    _frozen_param715 = rand_strided((1024, 1, 7, 7), (1, 0, 0, 0), device='cpu', dtype=torch.bfloat16)
    global constant405
    constant405 = rand_strided((128, 1024, 32), (32768, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant406
    constant406 = rand_strided((32, 4096, 32), (131072, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param599
    _frozen_param599 = rand_strided((4096, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param716
    _frozen_param716 = rand_strided((4096, 1024), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param601
    _frozen_param601 = rand_strided((1024, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param717
    _frozen_param717 = rand_strided((1024, 4096), (1, 0), device='cpu', dtype=torch.bfloat16)
    global _frozen_param603
    _frozen_param603 = rand_strided((1, 1024, 1, 1), (1024, 1, 1, 1), device='cpu', dtype=torch.float32)
    global constant412
    constant412 = rand_strided((128, 1024, 32), (32768, 32, 1), device='cpu', dtype=torch.bfloat16)
    global constant413
    constant413 = rand_strided((32, 4096, 32), (131072, 32, 1), device='cpu', dtype=torch.bfloat16)
    global _frozen_param604
    _frozen_param604 = rand_strided((1000, ), (1, ), device='cpu', dtype=torch.bfloat16)
    global _frozen_param718
    _frozen_param718 = rand_strided((1000, 1024), (1, 0), device='cpu', dtype=torch.bfloat16)
    global constant416
    constant416 = rand_strided((1000, 1024, 1), (1024, 1, 1), device='cpu', dtype=torch.bfloat16)
    arg344_1 = rand_strided((1, 3, 288, 288), (248832, 82944, 288, 1), device='cpu', dtype=torch.float32)
    fn = lambda: call([arg344_1])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('convnext_base', benchmark_compiled_module)
